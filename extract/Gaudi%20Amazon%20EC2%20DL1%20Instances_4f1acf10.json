{"origin_pdf_path": "https://cdrdv2-public.intel.com/784833/Gaudi%20Amazon%20EC2%20DL1%20Instances.pdf", "text_in_pdf": "FeatureDescriptionOAMsupport·OAMpoweredby54V,12Vand3.3V·DualB2Bconnectorsxl6PCleGen5hostinterfaceperOAMs·8X16PCleGen5connectorsBaseboardtoHlB(Host InterfaceBoard)Interface·Power:12V_Standby,54V·Side band signals: I2C,Reset,reference clocks,JTAG,UART,SGMll,USB·EightAmphenolconnectors:2x160P(10131762-301LF)+6x112P(10137002-101LF)·PerOAM:24x200GbE(48112GhzPAM4SerDesLinks)splitinto:Networking:Card to Card& Scale-out·2lx200GbEforOAM-to-OAMconnections·3x200GbEforscale-out·TotalBaseboard Scale-out:·8x3x200GbE=4.8TbEconnectedto6OSFP800portsPCBdimension·585mmx417mmx4.6mm\n\nTable 3. HLB-325 Features.  \n\n  \nHL-338 PCIe Add-In Card.  \n\nHL-338 PCIe Add-In Card  \n\nThe Intel Gaudi 3 AI accelerator PCIe add-in card is offered to system designers in accordance with PCIe CEM Spec. Revision 5.1 form and supports up to 600W TDP Power with passive cooling.  \n\nInterfaceDescriptionHost Linkxl6PCleGen5NetworkingScale-upCard-to-Card throughTopBoardScale-outThroughHost-NlConlyJTAGIn-field CPLDprogrammingandlow-level ASIC debugI2CSlave/SMBUSBMCcontrolandmonitoringinterface\n\nTable 4. HL-338 PCIe Key Interfaces.  \n\nHLTB-304 x4 Top Board  \n\nThe HLTB-304 board allows connectivity of 4 HL-338 cards, 6x 200 GbE links from each HL-338 card to each of the other 3 HL-338 cards, 18 links of 200 GbE total per card.  \n\n  \nFig 4. HLTB-304 Block Diagram.  \n\nIntel Gaudi 3 AI Accelerator Architecture  \n\nParallel Execution of the Heterogenous Engines  \n\nIntel Gaudi architecture was designed to allow activating all engines in parallel.   \nThis means that MME, TPC and NIC can all work at the same time.  \n\nThe two main use cases for running different engines in parallel are: 1.\t \u0007No dependency between the input and output of type of engine. In this case no special software intervention is needed. The Graph Compiler can simply trigger each engine to execute, providing the full input and output tensor sizes. 2.\t\u0007There is dependency between operations running on different engines: the output of one engine is used as the input of another engine.  \n\nThe first case is simple and allows MME, TPC and NIC to be scheduled to run in parallel. When one engine has completed its executing operation, the engine can be scheduled to start working on the next operation (immediately upon readiness of its inputs).  \n\nThe second case is more complex as it requires finer-grained scheduling, in addition to work size management that is done by the Intel Gaudi software. In this case, the dependent engines are scheduled to execute in a pipelined manner with a producer-consumer relation. The engine scheduling and entire orchestration is done by the Graph Compiler. A more detailed explanation on how several software layers are combined to work together to achieve efficient engine scheduling and execution is presented in the following section.  \n\n  \nFig 5. Intel® Gaudi® 3 AI Accelerator block diagram.  \n\nEvery chip component is explained in detail in the subsequent chapters.  \n\nIntel Gaudi 3 AI accelerator comprises of four key functions:  \n\n  \n\nCompute Engines  \n\n•\t 8 Matrix Multiplication Engines (MMEs) •\t 64 Tensor Processor Cores (TPCs)  \n\nMedia Engines  \n\n•\t 14 Media Decoder Engines (DECs) •\t 4 Rotator Engines (ROT)  \n\nMemory  \n\n•\t 96 MB of L2 Cache •\t 128 GB of 8 HBM2e Stacks  \n\nNetworking  \n\n•\t PCIe Gen5 X16 port for communicating with Host •\t 24 Network ports and the accompanied RDMA Engine •\t Scheduling and Synchronization Unit  \n\nPhysical Partioning  \n\nIntel Gaudi 3 AI accelerator compute engines are split into four clusters. Each cluster is referred to as a DCORE (Deep Learning Core) and contains:  \n\n•\t 2 Matrix Multiplication Engines (MMEs) •\t 16 Tensor Processor Cores (TPCs) •\t 24 MB of L2 Cache  \n\nFigure 6 reviews Intel Gaudi 3 AI accelerator architectural elements with DCORE partition, Media sub-system, Network sub-system and the connection with Host.  \n\nHOSTFRAMEWORKINTEGRATIONLAYERCUSTOMER KERNELINTEL KERNEL LIBRARYGRAPHCOMPILERLIBRARYCOMMUNICATIONLIBRARYUSERMODEDRIVERKERNELKERNELMODEDRIVER  \n\nIntel® Gaudi® 3  \n\nPCIe Gen5 X16  \n\n  \nFig 6. Intel® Gaudi® 3 architecture with DCORE point-of-view and supporting software layers.  \n\nHost Interface  \n\nIntel Gaudi 3 AI Accelerator PCIe Card  \n\nIntel Gaudi 3 AI accelerator is equipped with a state-of-the-art PCI Express Gen 5 x16 lane interface, a significant upgrade from the Gen 4 PCIe found in the prior generation accelerator. This advanced interface offers an impressive total bandwidth of 128 GB/sec, with 64 GB/sec available in each direction. This is a substantial improvement over the 64 GB/sec total bandwidth (32 GB/sec in each direction) provided by the Gen 4 PCIe.  \n\nThe PCIe Gen 5 interface allows Intel Gaudi 3 AI accelerator to seamlessly connect with the most powerful CPUs, external NICs, and SSDs available on the market. This ensures optimal performance and efficiency, making it a leading choice for high-performance computing solutions.  \n\nIntel Gaudi 3 AI Accelerator Control Path  \n\n  \nFig 7. Control Path Block Diagram.  \n\nTo manage the parallel and efficient execution of various engines, the Intel Gaudi 3 AI accelerator incorporates a programmable Control Path entity. This entity is designed for high throughput and low latency. Figure 7 provides the primary components of this functionality.  \n\nThe Control Path of Intel Gaudi 3 AI accelerator comprises the following elements:  \n\n•\t \u0007Submission Queues (SQs): These are issued by the runtime system.   \n•\t \u0007Completion Queues (CQs): These are used for job completion reporting.   \n•\t \u0007Programmable Scheduling Mechanism: This mechanism is utilized for task scheduling.   \n•\t \u0007Programmable Hardware Synchronization Mechanism: This is referred to as ‘Sync Manager (SM)’ in the diagram and is used for hardware synchronization.   \n•\t \u0007Programmable Interrupt Service Mechanism: This mechanism, referred to as ‘Interrupt Manager (INTR)’ in the diagram, enables the passing of asynchronous events to Habana Drivers.  \n\nEach of these components plays a crucial role in ensuring the smooth and efficient operation of Intel Gaudi 3 AI accelerator engines.  \n\nFor controlling parallel and efficient executions of the various engines, the Intel Gaudi 3 AI accelerator includes a programmable low-latency, high throughput Control Path entity.\n\nTechnical Paper  \n\nGenerative AI Compute AI for Enterprise  \n\nIntel® Gaudi® 3 AI Accelerator  \n\nThis technical paper introduces the next-generation AI accelerator from Intel, the Intel® Gaudi® 3 AI accelerator.  \n\nJuly 2025 – V1 Rev. 3  \n\nIntroduction  \n\nTable of Contents  \n\n1. Overview 3   \n2. HW System 4   \n3.  Architecture 7   \n4.  Host Interface 10   \n5. Compute 11   \n6. Software Suite 19   \n7. Networking 21   \n8.  Putting It All Together 25   \n9.  Performance Improvements 30  \n\nDeep Learning and Artificial Intelligence workloads continue to demand higher performance and lower power consumption. This technical paper introduces the next generation AI accelerator from Intel: the Intel® Gaudi® 3 AI accelerator. The new accelerator features the 5th generation of heterogenous AI acceleration architecture. The Intel Gaudi 3 AI accelerator was designed to provide state-ofthe-art datacenter performance for all AI workloads, from generative applications such as large language models (LLMs) and diffusion models (image generation such as Stable Diffusion) to standard object recognition, classification, and voice dubbing.  \n\nThe Intel® Gaudi® 2 AI accelerator, introduced in 2022, is supported by the Intel® Gaudi® software suite, which integrates the PyTorch framework. With the Intel Gaudi 3 AI accelerator we provide the next level of AI performance and power efficiency. Advancing from the Intel Gaudi 2 AI accelerator 7nm process, the Intel Gaudi 3 AI accelerator is manufactured in TSMC 5nm process, which provides improved area density and power efficiency.  \n\nIntel Gaudi 3 AI accelerator continues to push the boundaries of what is possible in performance and power efficiency. Built on the Intel Gaudi 2 AI accelerator architecture, Intel Gaudi 3 AI accelerator provides significant boosts in compute, memory bandwidth, and architectural efficiency.  \n\nThe Intel Gaudi 3 AI accelerator features two compute dies, which together contain 8 MME engines, 64 TPC engines and 24x 200 Gbps RDMA NIC ports. In addition, the total of 8 HBM2e chips comprise a 128 GB unified High Bandwidth Memory (HBM).  \n\nThe Intel Gaudi 3 AI accelerator excels at training and inference with 1.8 PFlops of FP8 and BF16 compute, 128 GB of HBM2e memory capacity, and 3.7 TB/s of HBM bandwidth.  \n\nAs deep learning models continue to increase in size and complexity, the demand for efficient, high-performance matrix multiplication engines is set to rise. The MMEs in solutions like Intel Gaudi 3 AI accelerator are therefore of critical importance to the ongoing advancement of deep learning technologies.  \n\nThe latest public model performance benchmarks are published at Model Performance for Intel® Gaudi® 3 AI Accelerators.  \n\nIntel Gaudi 3 AI Accelerator Overview  \n\nAI applications increasingly demand faster and more energy-efficient hardware solutions and the Intel Gaudi 3 AI accelerator was designed to answer the demand. With more than $2\\times$ FP8 GEMM FLOPs and more than $4x$ BF16 GEMM FLOPs compared to the Intel Gaudi 2 AI accelerator, Intel Gaudi 3 AI accelerator continues to provide state-of-the-art AI training performance. With $7.5\\times$ faster HBM bandwidth and $1.33\\times$ larger HBM capacity, the Intel Gaudi 3 AI accelerator provides an order-of-magnitude improvement in large language model inference performance compared to the Intel Gaudi 2 AI accelerator.  \n\nThe Intel Gaudi 3 AI accelerator (Figure 1) features two identical compute dies, connected through a high-bandwidth, low-latency interconnect over an interposer bridge. The die-to-die connection is transparent to the software, providing performance and behavior equivalent to that of a large unified single die.  \n\nThe Intel Gaudi 3 AI accelerator compute architecture is heterogeneous and includes two main compute engines – a Matrix Multiplication Engine (MME) and a fully programmable Tensor Processor Core (TPC) cluster. The MME is responsible for doing all operations that can be lowered to Matrix Multiplication, like fully connected layers, convolutions and batched-GEMMs. The TPC, a Very Long Instruction Word (VLIW) Single-Instruction Multiple-Data (SIMD) processor tailor-made for deep learning applications, is used to accelerate all non-GEMM operations.  \n\nIntel® Gaudi® Accelerator Product Line  \n\nIntel Gaudi 2 AI Accelerator to Intel Gaudi 3 AI Accelerator Feature Comparison.  \n\n  \nFig 1. Intel® Gaudi® 3 OAM Module.  \n\nFeature/ProductIntel@Gaudi@2AlAcceleratorIntelGaudi3AlAcceleratorBF16 MME TFLOPS4321678FP8MMETFLOPS8651678BF16VectorTFLOPS1128,7MME Units28TPC Units2464HBM Capacity96 GB128GBHBM Bandwidth2.46 TB/s3.7TB/sOn-die SRAM Capacity48MB96 MBOn-die SRAM Bandwidth (read/write)6.4/6.4 TB/s12.8/6.4 TB/sNetworking (bidirectional)600 GB/s1200 GB/sHost lnterfacePCle Gen4 X16PCle Gen5 X16Host Interface Peak BW64 GB/s(32 GB/sper direction)128 GB/s(64 GB/s per direction)Media Decoders814\n\nTable 1. Intel® Gaudi® 2 and Intel® Gaudi® 3 AI Accelerators.  \n\nHW System  \n\n  \nHL-325L OCP Accelerator Module  \n\nHL-325L OCP Accelerator Module  \n\nThe Intel Gaudi 3 AI accelerator OCP Accelerator Module (OAM) Card is offered to system designers in standard OCP OAM 2.0 Mezzanine card form and supports up to 900W Total Device Power (TDP) with passive cooling and up to 900W TDP with liquid cooling.  \n\nInterfaceDescriptionHost Linkxl6PCleGen5Networking: Card-to-Card&Scale-out48x1l2Gb/sPAM4SerDesLinksJTAGIn-field CPLD programming and low-level ASIC debugUARTLowleveldebug&BMCaccessI2CMasterOn/Off-boardPeripheralsI2C Slave/SMBUSBMCcontrolandmonitoringinterface\n\nTable 2. HL-325L OCP Accelerator Module Key Interfaces.  \n\nHLB-325L Universal Baseboard  \n\n  \nHLB-325L Universal Baseboard  \n\nThe HLB-325 Universal Baseboard is another product inspired by Open Compute Project (OCP) and offered for simplifying system design with the Intel Gaudi 3 AI accelerator. The HLB-325 supports eight Intel Gaudi 3 AI accelerator cards that are passively interconnected on its PCB in a non-blocking, all-to-all configuration, using 21 NICs from each card $\\left.3\\times200\\right.$ GbE ports to every other of the 7 cards), as well as routing the 3 remaining 200 GbE NICs from every Intel Gaudi 3 AI accelerator card ( $3\\times8=24;$ ) to the six on-board OSFP800 connectors for scaling-out.  \n\nThe baseboard has standard interface/connectors to the HIB (Host Interface Board), which allows the system designer customization to design to specific needs and the flexibility to build systems of choice with a different ratio of CPUs to accelerators for different varieties of topologies and applications.  \n\nBlock Diagram and Main Components  \n\n•\t HLB-325 has the following main components: •\t 8 X dual B2B connectors for the HL-325 Mezzanine boards •\t High speed connectors for x16 PCIe interconnect to HIB •\t 2 Complex Programmable Logic Devices •\t Power and reset control •\t JTAG distribution to the mezzanines  \n\n•\t LED indications   \n• $6\\times$ OSFP connectors $6\\!\\times\\!8000$ using 112G PAM 8 SerDes)   \n• $3\\times$ PHY retimers   \n•\t 8x PCIe retimers   \n•\t USB connectors for Debug  \n\n  \n\nFig 2. Key components of HLB-325.", "files_in_pdf": [{"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/dof83g.jpg", "size": 201253}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/aedmla.jpg", "size": 45657}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/t9xwi7.jpg", "size": 300266}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/v3w9o0.jpg", "size": 73530}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/m5hn1d.jpg", "size": 88417}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/084l9g.jpg", "size": 87489}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/ze2tbs.jpg", "size": 49637}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/1e4znz.jpg", "size": 713335}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065678/images/tk0td0.jpg", "size": 22375}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/04k43s.jpg", "size": 84172}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/emruyp.jpg", "size": 51548}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/9x8syl.jpg", "size": 49224}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/0kz4z7.jpg", "size": 171644}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/88qeoz.jpg", "size": 678582}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/kj4yk1.jpg", "size": 341871}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/whwkmy.jpg", "size": 160009}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065681/images/afyh1f.jpg", "size": 251460}]}