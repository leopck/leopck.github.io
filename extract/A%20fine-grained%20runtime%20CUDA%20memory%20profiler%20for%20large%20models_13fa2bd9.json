{"origin_pdf_path": "https://courses.cs.washington.edu/courses/cse503/25wi/final-reports/A%20fine-grained%20runtime%20CUDA%20memory%20profiler%20for%20large%20models.pdf", "text_in_pdf": "A fine-grained runtime CUDA memory profiler for large models\nBo Qiang, Jasper Bucher, Odin Zhang, Yanjing Li\nPaul G. Allen School of Computer Science & Engineering\nSeattle, Washington, USA\n{bqiang,jbutch,odinz,yanjing}@cs.washington.edu\nKeywords\nDynamic Testing, Memory Profiling, Machine Learning, Graphics\nProcessing Unit\n1 Introduction\nMachine learning models have the potential to transform scientific\nendeavors by uncovering insights from extensive datasets that were\npreviously inaccessible. [ 1] According to the scaling law of machine\nlearning [ 2], larger models training on larger datasets will always\ngain much better performance.\nIn large-scale projects, computational resources, especially GPU\nmemory are typically pushed to their limits. However, out-of-memory\n(OOM) errors can still arise due to various factors, including GPU\nmemory leaks, variations in floating-point precision, differences\nin forward-pass behavior with respect to tokenization, or dynami-\ncally growing input sizes during autoregressive inference. These\nerrors are particularly challenging to diagnose because the exact\nline at which a program fails is not necessarily the primary contrib-\nutor to excessive memory consumption. Throughout this paper, all\nsubsequent references to ‘OOM’ will specifically pertain to GPU\nout-of-memory situations. As a result, identifying problematic in-\nputs during both training and inference is often a labor-intensive\nprocess, typically involving exhaustive ablation studies or manu-\nally excluding anomalous samples one by one. These approaches\nare neither scalable nor effective in helping ML practitioners gain\ndeeper insights into their model architectures.\nEfficiently profiling GPU memory usage at a fine-grained level\nand attributing it to specific code blocks or neural network lay-\ners is therefore crucial for optimizing deep learning workflows.\nHowever, existing tools either offer CUDA-level logging that lacks\ndetailed, user-friendly insights into memory consumption for spe-\ncific Python code segments and variables, or function on PyTorch\ntensors, making it difficult to analyze model parameters. None of\nthe previous works are widely used by ML developers for debug-\nging out-of-memory errors and optimizing resource allocation in\nlarge-scale models. We introduce FineProfiler , a Python based\ntool that generates fine-grained GPU memory usage profiles for\nPyTorch-based models, offering precise tracking of memory alloca-\ntion and usage across different operations. In this paper, we develop\na lightweight CUDA memory profiler for tracking the consumption\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nACM 2025, Seattle, WA\n©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXXof PyTorch-based modules. Our approach dynamically tests the\nmemory trace of large models and we demonstrate its applicability\nto architectures of varying levels of complexity. We perform ex-\nperiments and analyses to demonstrate both the efficiency of our\nsystem and the user-friendly interface designed for developers.\nIn order to further evaluate FineProfiler , we will present a\nrepresentative case study focusing on one of the leading foundation\nmodels in the field of ‘AI for Science’, AlphaFold 3. The occurrence\nof out-of-memory (OOM) errors in this model is influenced by\nseveral factors related to the hierarchical structured inputs. No-\ntably, when processing inputs that include non-standard residue\ntypes, the dimensionality of the input vectors can increase sig-\nnificantly. Additionally, we have observed memory leaks when\ncertain modules are invoked exclusively for specific input data. Our\napproach clarifies which architectural components can be down-\nsized or pruned altogether, based on an assessed trade-off between\nmemory usage and performance. This profiler helps identify that\na significant amount of memory is consumed by the trunk of the\nnetwork. By reducing the size of this part of the architecture, we\nfind reduced error rates on spurious samples as well as improved\nparameter efficiency. We provide our open-source implementation\nof the profiler at github.com/YanjingLiLi/Torch_mempro.\n2 Related work\nTo profile CUDA memory usage in machine learning models, de-\nvelopers have created multiple tools. Below we summarize existing\nsoftware capable of tracing memory consumption.\nNVIDIA Nsight Systems. [3] This is a performance analysis tool\nthat is seamlessly integrated into the CUDA developer tool suite.\nNsight Systems provides developers with time and memory con-\nsumption for every CUDA-level function call, e.g., convolution net-\nwork forwards. A summary of I/O, CPU, GPU memory, and GPU\nutilization is presented in a visualization panel. However, most\nML researchers are now working with ML frameworks written\nin Python, where multiple neural network layers utilize the same\nCUDA API. Therefore, error tracing is hard for Nsight Systems, be-\ncause we cannot interpret the location of bugs from a CUDA-level\nerror message which could be raised from multiple places in the\nneural network. In contrast, our method solves this by building\nthe profiler purely on Python and integrating it into a popular ML\nframework, Pytorch.\nPyTorch’s Built-in Profiler. [5] PyTorch is currently the most pop-\nular machine learning framework, according to a survey conducted\nby Stack Overflow[ 4]. A profiler operates in the background to track\nmemory usage by calling the function torch.profiler.profile() .\nCompared to Nsight Systems, this profiler can save the memory\noccupation time curve as a snapshot, allowing for visualization\nof memory usage based on specific memory addresses. However,\n\nACM 2025, June 03–05, 2018, Seattle, WA Bo Qiang, Jasper Bucher, Odin Zhang, Yanjing Li\nestablishing connections between variables that encompass mul-\ntiple vectors and gradients distributed across various GPUs poses\na significant challenge. In contrast to this profiler, our method\ntracks variable memory by segmenting the code into hierarchi-\ncal graphs that represent the architecture of the neural network.\nThis approach can facilitate direct analysis for machine learning\nresearchers. Methodologically, our method can be viewed as a ma-\nchine learning objective layer built upon the built-in profiler in\nPyTorch.\nThird-party package pytorch_memlab. [6] This is a GitHub repos-\nitory with over 1k stars, developed by Kaiyu Shi from Shanghai\nJiaotong University. It provides line-by-line analysis for the pytorch\nnn.Module class, which are blocks of neural networks including\ntheir parameters and other run-time variables. This profiler mea-\nsures variable memory usage by logging the data type and shape\nof every intermediate tensor and its gradients. However, two sig-\nnificant challenges arise. First, calculating the memory usage of\ntorch.tensor only provides a lower bound for CUDA memory us-\nage, as it overlooks critical components such as optimizer states\nand model parameters. Additionally, because this profiler is run on\nCPUs, it requires frequently transferring tensors back and forth\nbetween the CPU and GPU, which negatively impacts efficiency. To\novercome this challenge, instead of manually calculating every ten-\nsor shape, we will keep variables on the GPU and log the memory\nallocation using torch-based API.\nWeights and Biases (WandB). [7] WandB is a popular, lightweight\nlogging tool for AI developers, primarily designed for the conve-\nnient tracking of training metrics. It is well-known for its ease of\nimplementation and its user-friendly web-based API. The frame-\nwork includes various GPU tracing utilities that monitor aspects\nsuch as process time spent accessing memory, as well as GPU al-\nlocation and utilization. While WandB can be useful for detecting\npotential memory leaks, its API does not provide the capability\nto diagnose specific neural modules that may be contributing to\nexcessive memory consumption.\nIn conclusion, although various tools have been developed, there\nis currently no profiler that can be directly applied to large ML\nmodels without extensive log analysis.\n3 Methodology\n3.1 Overview of FineProfiler\nIn this section, we introduce an overview of our fine-grained CUDA\nmemory profiler ( FineProfiler ). Currently, neural networks writ-\nten in Python are coded as follows:\u0007 \u0004\n1 class SimpleNN (nn.Module ) :\n2 def init (self ) :\n3 super (SimpleNN ,self ) .init ( )\n4 self .fc1 =nn.Linear ( 2 , 1 )\n5\n6 def forward (self ,x) :\n7 x=torch .sigmoid (self .fc1 (x) )\n8 return x\u0006 \u0005\nMachine learning researchers will define more complex model\narchitectures within the nn.Module . Our framework primarily con-\nsists of two components: a json file generator that log memory\nusage and a user-friendly visualizer that outputs HTML files. When\nusers intend to track memory usage for the forward path of themodel, they only need to wrap the forward call of the model with\na Python decorator memory_tracker . In Python, a decorator is a\nfunction that modifies the behavior of another function or method\nwithout changing its code. It is commonly used for logging, profil-\ning, authentication, and performance monitoring. In our context, a\ndecorator is designed to automatically track CUDA memory usage\nduring model execution in PyTorch. It wraps a function (typically\nthe model’s forward pass for inference mode and backward pass\nfor train mode) and logs GPU memory allocation and deallocation\nat both the code line level and variable level.\n\u0007 \u0004\n1 Apply memory tracing on model inference\n2 @memory_tracker wrap with the decorator\n3 def inference_model (x) :\n4 moving model into GPU\n5 model =SimpleNN ( ) .to(\" cuda \" )\n6 output =model (x)\n7 return output\n8\n9 Apply memory tracing on model training\n10 @memory_tracker wrap with the decorator\n11 def train_model (output ,label ,optimizer ) :\n12 moving model into GPU\n13 model =SimpleNN ( ) .to(\" cuda \" )\n14 output =model (x)\n15 loss =loss_func (output ,label )\n16 loss .backward ( )\n17 optimizer .step ( )\u0006 \u0005\nUsers simply need to apply the decorator to their model and\ntrain it iteratively, with each iteration comprising a forward and\nbackward pass. A raw JSON file will be generated, capturing de-\ntailed CUDA memory logs for every line of code. A parser then\nprocesses this JSON file, filtering out low-level details unrelated\nto debugging. The refined JSON is subsequently mapped to the\nmodel’s hierarchical structure, constructed using our novel model\narchitecture builder. Ultimately, a structured JSON output file is\nproduced, detailing memory usage per line of code and per vari-\nable. Additionally, an HTML visualization is generated, offering an\nintuitive representation of memory consumption across different\ncode segments. The workflow is visualized in Figure 1.\nOur main contributions are: (1) Providing an unbiased CUDA\nmemory profiler gives full analysis from both line-of-code level and\nvariable level, which the former is not supported by any previous\nworks; (2) We enable memory tracking for both the forward pass\nand gradient backpropagation, while the latter is not supported by\nMemLab. (3) We present memory profiling results in both JSON\nand an interactive HTML page.\n3.2 Design and Implementation\n3.2.1 Per-variable memory track. The memory of variables is tracked\nby iterating through all named parameters in a PyTorch model for\neach line:\n\u0007 \u0004\n1 mem_bytes =param .element_size ( )∗param .nelement ( )\n2 print (f\"{ name }: { mem_bytes / 10242:.3 f} MB\" )\u0006 \u0005\nThe memory usage of each parameter is computed by multiply-\ning its element size (the number of bytes each element occupies)\nwith the total number of elements (obtained using nelement() ).\nThis approach provides insights into the lower-bound memory\nconsumption of each tensor involved in the current line-of-code,\nwhich is crucial for optimizing memory usage, particularly when\ntraining large models on GPUs.\n\nA fine-grained runtime CUDA memory profiler for large models ACM 2025, June 03–05, 2018, Seattle, WA\nFigure 1: Proposed Workflow of FineProfiler: Gray blocks\nrepresent components executed or generated by our method,\nwhile the white block represents the user’s model. Users can\nwrap the memory tracker directly on their customized model.\nDuring execution, the memory tracker monitors memory us-\nage for each line of code and tensor operation, summarizing\nthe results in a JSON file and an HTML page as output.\n3.2.2 Per-line-of-code memory track.\nForward Pytorch decorator. The decorator used as a memory\ntracker is the core component of FineProfiler , represented in\nFigure 1. It is responsible for tracking CUDA memory consumption.\nUnlike existing tools such as PyTorch-MemLab, which focuses only\non variable usage, or Nsight System only on overall usage, the\nforward pytorch decorator logs memory allocation and deallocation\nfor every line of python code. This dual-level approach allows users\nto diagnose CUDA OOM errors more precisely by distinguishing\nwhether memory spikes are caused by large input data, memory-\nintensive operations, or inefficient tensor management.\nThe Python decorator wraps the model’s forward pass and cap-\ntures memory usage while injecting dynamic hooks into the exe-\ncution flow. The decorator intercepts function calls, allowing it to\ntrack memory usage without modifying the original code. Dynamic\nhooks are injected both at the Python execution level and at the\nPyTorch tensor level. At the Python execution level, our decora-\ntor hooks into each executed line of code using sys.settrace() ,\nenabling it to track per-line memory usage. At the PyTorch ten-\nsor level, it hooks into PyTorch’s autograd system by leveraging\nsys.settrace() , which monitors tensor operations within the\nneural network layers.\nTo capture memory usage at the Python execution level, our dec-\norator dynamically injects hooks using sys.settrace() , allowing\nit to intercept execution at each line and extract file names and line\nnumbers from frame.f_code.co_filename andframe.f_lineno .\nGPU memory usage is recorded before and after execution using\ntorch.cuda.memory_allocated() and\ntorch.cuda. memory_reserved() . This enables FineProfiler to\nlog per-line memory changes, detect sudden spikes, and identify\ninefficient memory usage patterns that may indicate memory leaks.\nThese delta memory usages are linked with specific lines of code\nand variables. By capturing execution at the Python level, users can\npinpoint the exact line of code responsible for excessive memory\nconsumption.\nFor variable-level tracking, the system captures memory alloca-\ntion and deallocation events associated with tensors. Each tensor\nis uniquely identified using id(tensor), and its memory footprint is\ncomputed through attributes such as tensor.element_size() andtensor.nelement() . To monitor tensor transformations and track\nmemory usage across different operations, the system employs\nPyTorch hooks, including register_forward_hook , to attribute\nmemory usage to specific layers in a neural network.\nThe decorator plays a key role in this process by automatically\nwrapping user-defined models and injecting the necessary instru-\nmentation code. Instead of defining new custom hooks, the dec-\norator captures function calls, performs additional tracking logic,\nand then forwards execution to the original module. This approach\nallows the system to dynamically attach hooks at runtime with-\nout modifying the underlying model code. The collected data is\nstored in structured logs, enabling an in-depth analysis of how each\nvariable contributes to overall memory consumption.\nBackward Pytorch decorator. In machine learning, neural net-\nworks are usually trained by a forward pass to get the loss and\nthen backpropagate the gradient through model layers to update\nall parameters. Therefore, tracking the CUDA memory usage for\nbackpropagation is also important to detect errors in code.\nIn our implementation, we monitor CUDA memory usage by\nassessing memory consumption before and after executing the\nmodule, paralleling the method used during the forward pass. How-\never, unlike the forward decorator, it is impossible to analyze the\nbackward pass line by line due to its operation on the underlying\ncomputational graph instead of explicitly defining all execution.\nTo overcome this, we employ the register_full_backward_hook\nfor each submodule of the complete network, allowing us to capture\nand log memory usage across all modules. This approach enables\nus to generate comprehensive logs for each submodule and neural\nlayer, facilitating an in-depth analysis of memory consumption\nduring the backward pass.\n3.2.3 Outputs processing.\nLog parser. Both the forward decorator and the backward decora-\ntor run recursively to capture the fine-grained details of the neural\nnetworks. This implies that for a nn.Linear() layer, the logging\noperations are decomposed into weight matrix multiplication and\nbias calculation, resulting in redundancy for debugging purposes.\nTo address this issue, we implement a log parser that processes\nthe JSON file by removing all low-level call logs. A Python script au-\ntomatically identifies directory paths containing third-party PyTorch-\nlevel packages and removes any corresponding logs from the raw\nJSON output while preserving the correct hierarchical structure.\nAfter this pruning process, the refined JSON file is passed to the\nbuilder for further processing.\nModel architecture builder. The key challenge in using other\nCUDA memory tracking tools is how to interpret the logs that\ncould be useful for real-life machine learning debugging. Nowa-\ndays, neural networks are getting bigger and bigger, both on the\nscale of the number of parameters and the complexity of these\ndesigned architectures. Rather than analyzing code line by line,\nPython programs are structured as a hierarchical tree within the\nminds of machine learning engineers.\nIn order to make the logs interpretable, we implement a forward\nand backward decorator, as well as a model architecture builder\nthat parses any PyTorch-based code into a tree-based structure.\nThis structure recursively collects all computations in the forward\n\nACM 2025, June 03–05, 2018, Seattle, WA Bo Qiang, Jasper Bucher, Odin Zhang, Yanjing Li\npass of every nn.Module . Each node in the tree represents a specific\ncomputation, and we enhance this representation by adding the\ncorresponding file name and line numbers as metadata for each\nchild node. This comprehensive logging mechanism facilitates bet-\nter debugging and understanding of the model’s architecture and\nits execution flow during training and inference.\n3.2.4 Visualization. The visualization component aims to offer an\nefficient, fine-grained, and user-readable solution for deep learn-\ning users working with large-scale models. By integrating detailed\nmemory tracking at both the code and variable levels, detailed out-\nput generation, and multi-GPU awareness, this tool will address\ncritical challenges in memory optimization and help developers\nmore effectively diagnose and mitigate out-of-memory errors. Our\ntool will generate structured output in JSON format, capturing\nmemory usage statistics for each code block and variable. To be\nspecific, JSON output takes the hierarchical form of the neural net-\nwork where each key represents a chunk of code that is sequentially\nexecuted during the forward pass. In the JSON output for the chunk\nof code, memory usage, and variables are logged. This JSON out-\nput will enable easy integration into existing debugging pipelines,\nallowing developers to systematically analyze memory trends and\nidentify bottlenecks.\nAdditionally, an HTML page will be generated where each vari-\nable / layer is represented as a button. When the user clicks on a\nbutton, it will display the memory usage information for each line\nthat involves the selected item.\n4 Experiment\nIn the experimental evaluation section, we first conducted a fun-\ndamental functional test on a toy model to determine whether our\napproach could deliver line-by-line CUDA memory logging and\norganize this data module by module. Next, we performed experi-\nments to assess the performance of our system, FineProfiler , and\ncompared it with other well-known tools. Finally, we applied our\nmethod to real-world debugging scenarios in AlphaFold 3.\n4.1 Functional test\n4.1.1 Line-Specific Detection. We first examine the profiler’s ca-\npacity for line-specific memory attribution. An MLP with multiple\nlayers is constructed, and its hidden dimensions are progressively\nincreased. During training, the profiler logs memory allocation,\nlinking each allocation to the exact line of code responsible (e.g.,\nweight initialization or forward-pass operations). Through this ex-\nperiment, we verify whether the profiler pinpoints the specific lines\ncausing memory surges, enabling developers to optimize particular\ncode segments more efficiently.\nRefined Line-specific tracing\n{\n\"file\": \"model/MLP.py\",\n\"line\": 25,\n\"function\": \"forward\",\n\"delta_allocated_bytes\": 4000256,\n\"delta_reserved_bytes\": 0,\n\"delta_allocated_gb\": 0.0037255287170410156,\n\"delta_reserved_gb\": 0.0\n}...\n4.1.2 Module-Specific Detection. Next, we investigate module-specific\nmemory tracking using the same MLP setup. Instead of attributing\nallocations to individual lines, the profiler aggregates memory usage\nat the module level, providing a consolidated view of each layer’s\nresource consumption. This module-level analysis highlights which\nparts of the network (e.g., certain dense layers) demand the most\nGPU memory. By identifying these high-consumption modules,\ndevelopers can strategically apply optimization techniques (e.g.,\nreduced hidden dimension size or selective gradient checkpointing)\nwhere they will have the greatest impact.\nThis module-level logging is enabled not only for forward calls of\nthe neural network but also for tracking CUDA memory consump-\ntion during the backpropagation of gradients, which is essential for\nparameter optimization. To our knowledge, FineProfiler is the\nfirst tool that offers fine-grained logging for model training.\n{\n\"name\": \"Main_model\",\n\"children\": [\n{\n\"name\": \"layer1\",\n\"children\": [\n{\n\"name\": \"linear\",\n\"children\": [],\n\"delta_allocated_gb\": 0.01,\n\"delta_reserved_gb\": 0.02\n},\n{\n\"name\": \"relu\",\n\"children\": [],\n\"delta_allocated_gb\": 0.01,\n\"delta_reserved_gb\": 0.0\n}\n]\n},\n{\n\"name\": \"layer2\",\n\"children\": [\n{\n\"name\": \"linear\",\n\"children\": [],\n\"delta_allocated_gb\": 0.01,\n\"delta_reserved_gb\": 0.02\n},\n{\n\"name\": \"relu\",\n\"children\": [],\n\"delta_allocated_gb\": 0.0,\n\"delta_reserved_gb\": 0.0\n}\n]\n},\n{\n\"name\": \"fc\",\n\"children\": [],\n\nA fine-grained runtime CUDA memory profiler for large models ACM 2025, June 03–05, 2018, Seattle, WA\n\"delta_allocated_gb\": 0.0,\n\"delta_reserved_gb\": 0.0\n}\n]\n}\n4.1.3 Actionable Suggestions. The practical advantage of our pro-\nfiler is its ability to generate actionable suggestions. Specifically,\nwe leverage the profiler logs to compute the peak GPU memory\nusage during the entire backward pass of the network, then convert\nthat value in relation to the current batch size. By dividing the total\navailable GPU memory by the per-sample memory footprint, we\nobtain an estimated maximum feasible batch size .\n>> python s c r i p t / mlp_experiment . py\nMaximum b a t c h s i z e : 207240\n>> python s c r i p t / t r a n s f o r m e r _ e x p e r i m e n t . py\nMaximum b a t c h s i z e : 467\nAs shown above, running our profiler on a simple MLP archi-\ntecture yields a maximum batch size of 207,240 , while a more\ncomplex Transformer model caps at a feasible batch size of 467.\nThis estimation addresses a common challenge in deep learning\ndevelopment: practitioners typically discover the maximum batch\nsize through a trial-and-error process (e.g., testing batch sizes 2, 4,\n8, etc.) until an out-of-memory (OOM) error arises. Our approach,\nhowever, streamlines the process by systematically analyzing the\nmemory profiling logs to pinpoint exactly how much memory is\nconsumed at peak allocation. Not only does this save considerable\ntime, but it also ensures that GPU resources are utilized as effi-\nciently as possible—maximizing batch sizes without risking OOM\nerrors.\nFigure 2: UI of Pytorch built-in profiler\n4.2 UI Analysis\nIn this section, we provide an analysis of the current popular CUDA\nmemory tools and discuss the advantages of our visualization web-\npage.\nAs shown in Figure 3, the built-in PyTorch profiler provides a\nplot of CUDA memory usage over time. Memory allocations are\nlogged and color-coded according to the corresponding CUDA calls.\nThis feature is particularly useful for identifying memory leaks and\ninvestigating which iterations cause abnormal memory behavior.\nHowever, this visualization is hard to interpret for Python debug-\nging and model architecture analysis, especially for large models\nor huge batch sizes that are non-trivial to fit into a single GPU\nfor one forward pass. Nsight system provides more fine-grained\nFigure 3: UI of NVIDIA Nsight system\nFigure 4: UI of our FineProfiler\nlow-level logging as in Figure 3. Besides CUDA memory logging, it\nalso gives information about CPU usage and CPU-GPU information\nexchanges.\nA visualization of the demo is presented in Figure 4. In addition\nto logging basic information about peak memory usage during the\nentire training iteration and inference pipeline, the webpage fea-\ntures a detailed line-by-line delta memory visualization. To identify\nabnormal memory behavior or bottlenecks, users can first exam-\nine peak CUDA memory by reviewing the JSON output from our\nmemory summary, which is organized by module. They can then\nclick on the corresponding code lines on the webpage to examine\nthe tensor memory size and additional memory allocations at each\ncomputational step.\n\nACM 2025, June 03–05, 2018, Seattle, WA Bo Qiang, Jasper Bucher, Odin Zhang, Yanjing Li\nFigure 5: Runtime of different cuda profiler for MLP / Transformer experiment\n4.3 Experiment on runtime\nWhile all profilers can offer valuable insights to aid ML developers\nin debugging their models, these methods may also introduce addi-\ntional runtime overhead that could adversely affect their practical\napplications. Therefore, in this section, we benchmark the run-\nning time for well-known CUDA memory profilers. Experiments\nare conducted on MLPs of different numbers of layers and vanilla\ntransformers of different sizes of hidden dimensions. Runtimes are\nevaluated for both inference and training processes. During infer-\nence, the automatic gradient calculation is disabled for PyTorch\nmodels. For each training iteration, randomly generated data is in-\nput for the forward pass to compute the loss, followed by one step\nof backpropagation and optimization. We run 100 iterations for all\nMLP experiments and 10 iterations for all Transformer experiments.\nThe results are illustrated in Figure. 5. ’control’ stands for the blank\nexperiments without any profiling.\nIn inference mode, both MemLab and the built-in PyTorch pro-\nfiler introduce significantly more overhead in running time com-\npared to the Nsight Systems and our method. However, this over-\nhead does not scale with the number of layers or the size of the\nhidden dimensions. For larger models, the additional running time\nincurred by the built-in PyTorch profiler and the blank experiment\nremains consistent. We conclude that this overhead is primarily\ndue to the process of saving snapshots at the beginning and end\nof the program rather than from injecting additional logging into\nthe model itself. However, MemLab still significantly slows downmodel inference. On the other hand, our method incurs only a mar-\nginal additional runtime compared to the native NVIDIA profiler,\ndemonstrating its efficiency.\nFor model training, it is clear that our approach, denoted as\nFineProfiler , may introduce some latency in the backpropagation\ngradient computation due to the recursive logging mechanism. The\nupper-right figure illustrates that the running time scales linearly\nwith the number of layers in the neural network. However, as\ndemonstrated in our Transformer experiments, the running time\nof our method does not exhibit scaling with the size of the hidden\ndimensions. These findings indicate that while our method may\nhinder training efficiency—especially when developers partition\nneural networks into multiple nested modules—it remains unique in\nits capability to offer fine-grained logging during training. This level\nof detailed logging is crucial for optimizing model performance and\ndebugging, setting our method apart from others in the field.\n4.4 Peak memory detection\nIt is essential for the profiler to accurately identify memory bottle-\nnecks in the code. Consequently, we analyze the output logs from\nMemlab and the built-in profiler of PyTorch, comparing their peak\nmemory usage with that of our method. We run all methods on a\n12-layer transformer encoder-decoder network and the results are\nshown in Figure. 6. It is obvious that FineProfiler and the built-in\nprofiler of PyTorch give similar results while Memlab underesti-\nmate the memory consumption of the transformer model. This\nis because Memlab only tracks the memory for training through\n\nA fine-grained runtime CUDA memory profiler for large models ACM 2025, June 03–05, 2018, Seattle, WA\nFigure 6: Peak memory profiled by different methods for a\n12-layer transformer\ntensor sizes and tensor gradients. The gradient stored in the param-\neters and optimizers takes around 0.05GB in the experiment which\nonly FineProfiler and the built-in profiler of PyTorch are able to\ndetect.\n4.5 Application to development of the\nAlphaFold3 architecture\nIn this evaluation, we aim to demonstrate that our profiler can be\napplied to large, complex models, and serve to aid developers on\ncomplex real-world tasks. In particular, we are interested in an ma-\nchine learning architecture design problem related to AlphaFold3.\nAlphaFold3 is a 300M parameter model trained to predict the struc-\nture of a protein given its amino acid sequence. The architecture of\nAF3 is complex, but can be viewed as containing two major compo-\nnents: (i) the trunk which processes the amino acid sequence, and\n(ii) the diffusion module which generates the full structure given the\noutputs of the trunk. The trunk serves to steer the final generation,\nsimilar to how a prompt to a large language model guides how it\ngenerates the answer.\nFor our application, we want to use a model that has already\nlearned some things and apply it to a new job: creating proteins from\nscratch, without needing any initial sequence input or guidance.\nNormally, this involves the trunk, which helps in generating initial\nguesses. However, for this task, the trunk isn’t needed, and it’s\nactually holding back the model’s performance because it’s taking\nup resources that could be used more effectively.\nWe aim to use FineProfiler to understand the effect of removing\nthe trunk. And to use our program to understand how relinquished\nresources can be reallocated to accelerate training and thereby\nboost performance.\nUsing the FineProfiler , we can provide a rough measure of\narchitectural complexity. Upon removal of the trunk component,\nwe found a nearly ∼4-fold decrease in the number of submodules\ncalled on the forward pass (3393 in the base model versus 900 in\nthe variant without the trunk–the ’A14’ model). This significant\nreduction in the architectural complexity upon removing the trunk\nsuggests the model would be substantially simpler and thereforefaster. Interestingly, however, we found that our initial implemen-\ntation did not significantly accelerate training.\nWe investigate in Figure 7 the causes of this effect by profiling\nthe memory consumption of both models during the backward-\npass. We see that, where a significant proportion of memory was\nallocated to the trunk (top of Figure 7), there is now still a large\nactivation outside of the diffusion module.\nTrunk D.M. \nD.M. Replaced trunk \nFigure 7: Memory profiles for AF3 architecture (with trunk,\ntop) and ’A14’ (AF3 without the trunk, bottom).\nTo investigate the memory effects of the replaced trunk modules,\nwe carry out an ablation of the memory consumption associated\nwith the replaced trunk components in Figure 8. We see that during\nthe backward pass, the memory consumption of the A14 (initial\nimplementation) is comparable to that of the original AF3, providing\na potential explanation as to why the training time is approximately\nthe same after removing the trunk. Finally, we see that the forward\npass is largely consumed by the trunk components, suggesting an\nalternative module is likely most appropriate for the A14 variant\nof AF3.\nWe found after training the new variant with the removed ad-\nditional components, that training time was reduced by 35%. This\nefficiency increase is attributable to the decreased memory alloca-\ntion during the backward pass, and was previously undetectable\nusing only the forward pass (see figure 8). Other conventional\nmethods of memory tracing must rely on the forward pass only\nand therefore could not be used to guide development in the same\nmanner.\nOverall, this shows our profiler can be used to aid in real-world\nproblems. We note there are additional effects other than the ones\ndescribed here which likely increase memory consumption of the\nablated model associated with.\n\nACM 2025, June 03–05, 2018, Seattle, WA Bo Qiang, Jasper Bucher, Odin Zhang, Yanjing Li\nFigure 8: Memory consumption by model variants.\n5 Discussion\nThe increasing emphasis on inference-time scalability has high-\nlighted the importance of ensuring model robustness not only dur-\ning training but also throughout the inference process. In this work,\nwe introduced FineProfiler, a lightweight, Python-based CUDA\nmemory profiler for large-scale PyTorch models. Unlike existing\nprofilers that operate at the CUDA or framework level, FineProfiler\noffers fine-grained insights at both the code-line and variable level\nusing Python decorators and dynamic tracing mechanisms. Our\ntool not only facilitates accurate memory attribution during both\nthe forward and backward passes, but also produces structured,\ninterpretable outputs in both JSON and HTML formats. Through\ncomprehensive experiments and real-world applications such as\nAlphaFold3, we demonstrated that FineProfiler can effectively de-\ntect memory bottlenecks, guide architectural decisions, and suggest\noptimization strategies—ultimately improving model reliability and\nresource efficiency. On this task, we showed that our framework of-\nfers insights by monitoring memory consumption of the backward\npass which are not available from the existing literature. By offering\nan intuitive debugging interface and maintaining minimal runtime\noverhead, FineProfiler addresses a critical gap in the deep learning\ndevelopment ecosystem and is a promising tool for optimizing deep\nlearning workflows at scale.\nAcknowledgments\nWe thank the members of the Institute for Protein Design; Rohith\nKrishna, Nathaniel Corley and Woody Ahern for their insightful\nthoughts during the planning of this project.\nReferences\n[1]Michael I Jordan and Tom M Mitchell. 2015. Machine learning: Trends, perspectives,\nand prospects. Science 349, 6245 (2015), 255–260.\n[2]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n[3]NVIDIA Corporation. [n. d.]. NVIDIA Nsight Systems. https://developer.nvidia.\ncom/nsight-systems. Accessed: 2025-02-05.\n[4]Stack Overflow. 2024. Stack Overflow Developer Survey 2024: Technology. https:\n//survey.stackoverflow.co/2024/technology Accessed: 2024-10-02.\n[5]PyTorch. 2023. PyTorch Profiler Recipe. https://pytorch.org/tutorials/recipes/\nrecipes/profiler_recipe.html. Accessed: 2025-02-05.\n[6]Stonesjtu. 2025. pytorch_memlab. https://github.com/Stonesjtu/pytorch_memlab\nAccessed: [Insert Date Here].\n[7]Weights and Biases. 2025. Weights and Biases. https://wandb.ai/site/. Accessed:\n2023-10-05.A Methods\n(Appendix)\nReceived 24 January 2024", "files_in_pdf": []}