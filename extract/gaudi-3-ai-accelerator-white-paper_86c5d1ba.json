{"origin_pdf_path": "https://cdrdv2-public.intel.com/817486/gaudi-3-ai-accelerator-white-paper.pdf", "text_in_pdf": "Matrix Multiplication Engine (MME)  \n\nExecutes all matrix multiplication operations  \n\nConfigurable, not programmable   \nEach MME is a large output stationary systolic array:   \n256x256 MAC structure w/ FP32 accumulators   \n64k MACs/cycle for BF16 and FP8   \nInternal pipeline to maximize compute throughput:   \nInput read, compute & output write all execute in parallel   \nIntegrated transpose engines for zero-overhead input transpose   \nAccumulated result is converted to any precision before write out   \nInternal buffers for input reuse – replacing L1\\$   \nIntegrated Address Generation Unit (AGU):   \nAddress calculation within 5-D data tensor   \nOOB read padding and write prevention  \n\n  \nMME Block Diagram  \n\nTensor Processor Core (TPC)  \n\nTotal of 64 TPC units across the chip   \nExecutes non-Matmul operations   \nProgrammable: C enhanced with TPC intrinsics   \nVLIW with 4 separate pipeline slots:   \nVector: 256B-wide SIMD   \nScalar   \n■ Load: Vector or scalar   \nStore: Vector or scalar   \nLoad and Store slots integrate Address Generation Unit   \nCalculates the memory address within a 5-D data tensor   \nSupports main 1/2/4-Byte datatypes: FP and integer   \n12KB vector register file   \n80KB of vector local memory (VLM)   \nLatency hiding mechanism  \n\n  \nTPC Block Diagram  \n\nMemory Sub-System  \n\nUnified Memory space of L2 / L3/ HBM  \n\nNear Memory Compute:  \n\nAdd / Sub Max / Min  \n\nUsage of Memory Context ID (MCID) to tag cache lines with shared algorithmic usage  \n\nCache Directives:   \nNo-\\$, L2\\$, L3\\$, L2\\$+L3\\$   \nDiscard: Invalidate all same-MCID CLs   \nDegrade: Reset same-MCID CLs hit count  \n\n  \nMemory Sub-System Logical View  \n\nControl Path and Runtime Driver  \n\nSeparate NOC fabric for control path messages  \n\nSync Manager handles on-die control logic:   \nDispatches work to the designated units Waits on counted events, and triggers start-ofjob to the units   \nRuntime Driver :   \nSets engine work dependencies by configuring the Sync Manager   \nSubmits jobs through Submission Queues   \nAccepts completion events through Completion Queues   \nInterrupt Manager enables passing events   \nasynchronously to the driver  \n\n  \nControl Path and Interaction with the Host  \n\nIntel® Gaudi® Software Suite  \n\nIntegrates the main frameworks used today  \n\nSupports FP16/BF16   FP8 quantization  \n\nMain proprietary SW layers:  \n\nGraph Compiler: Determines all engine dependency and scheduling logic Matrix operations: Configuring the MME TPC kernels: All non-Matrix operations Collective Communication Library (HCCL)  \n\nSeveral sources for TPC Kernels:  \n\nHabana’s optimized TPC kernel library   \nCustom user kernels   \nMLIR-based fused kernels: generated during graph   \ncompilation  \n\nLayered View of Intel® Gaudi® Software Suite\n\nAugust 2024  \n\nIntel Gaudi 3 AI Accelerator: Architected for Gen AI Training and Inference  \n\nRoman Kaplan, Ph.D. Principal AI Performance Architect  \n\nIntel Corporation August 2024  \n\nGaudi Product Generations  \n\n  \n\nProduct ParameterGaudiGaudi 2Gaudi 3TDP (OAM)400W600W900W (Air) / 1200W (Liquid)Peak Compute (BF16)60 TFLOPS432TFLOPs1835 TFLOPsHBM Capacity32 GB96 GB128 GBPeakHBMBW900 GB/s2.46 TB/s3.67 TB/sPeakPCleBW (bi-directional)64 GB/s64 GB/s128 GB/sEmbeddedNiCBW (bi-directional)2 Tb/s4.8 Tb/s9.6 Tb/s  \n\nIntel® Gaudi® 3 AI Accelerator OAM  \n\nOAM: Open Compute Platform Acceleration Module  \n\n2 compute dies connected over an interposer bridge  \n\n8 HBM2e stacks   \nUp to 900W with air cooling   \nUp to 1200W with liquid cooling   \nPCIe Gen5 x16   \n24x 200GbE RoCE via 48 112G PAM4   \nSerdes  \n\n  \nIntel Gaudi 3 AI Accelerator OAM  \n\nIntel® Gaudi® 3 Spec and Block Diagram  \n\nFeature/ProductIntel? Gaudi 3 AcceleratorBF16 Matrix TFLOPs1835FP8 Matrix TFLOPs1835BF16 Vector TFLOPs28.7MME Units8TPC Units64HBM Capacity128 GBHBM Bandwidth3.67 TB/sOn-die SRAM Capacity96 MBOn-die SRAM Bandwidth (L2 Cache)12.8 TB/sNetworking1200 GB/s bidirectionalHost InterfacePCle Gen5 xl6Host Interface Peak BW128 GB/s bidirectionalMedia EngineRotator + 14 Decoders (HEVC, H.264, JPEG, VP9)  \n\nIntel Gaudi 3 Al Accelerator Block DiagramDie 116GB HBM2e12 x 200Gbps,RDMA NICMedia Engine16GB HBM2e2xMME2xMME16GB HBM2e16xTPCCache 24MBCache 24MB16xTPC16GB HBM2e 16GB HBM2e16GB HBM2eCache Cache 16xTPC 16xTPC 24MB16GB HBM2e2x MME24MB 2xMME16GB HBM2eMedia EnginePCle Gen5 Die 012 x 200Gbps,RDMA NIC  \n\nArchitecture in Depth  \n\nUniform memory mapping of HBM by MMU  \n\nCompute is clustered: 4 Deep Learning Cores (DCORE) Each DCORE: 2xMME, 16xTPC, 24MB cache  \n\nL2 and L3 data caches: L2: Allocated only in DCORE cache L3: Uniformly distributed across all DCORE caches  \n\nMedia accelerators: Decoder and Rotator  \n\nNW Sub-system containing:  \n\n  \nIntel Gaudi 3 AI Accelerator Block Diagram   \nDie 0", "files_in_pdf": [{"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065675/images/ekgysp.jpg", "size": 158268}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065675/images/bmae38.jpg", "size": 20848}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065675/images/engwyx.jpg", "size": 213464}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065675/images/vwvm4w.jpg", "size": 109827}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065675/images/3zkwde.jpg", "size": 86548}, {"path": ".pdf_temp/viewrange_chunk_2_6_10_1762065675/images/9trflq.jpg", "size": 188405}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065676/images/x5a5xi.jpg", "size": 393181}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065676/images/gsq7ai.jpg", "size": 353544}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065676/images/bxanj4.jpg", "size": 310687}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065676/images/tmofmm.jpg", "size": 275341}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065676/images/xbgk2w.jpg", "size": 209214}, {"path": ".pdf_temp/viewrange_chunk_1_1_5_1762065676/images/n9dl5t.jpg", "size": 194927}]}