
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Search - Fridays with Faraday</title>
  <meta name="description" content="Search all technical posts and experiments">
  <link rel="stylesheet" href="css/style.css">
  <link rel="alternate" type="application/rss+xml" title="Fridays with Faraday" href="rss.xml">
</head>
<body>
  <div class="background"></div>
  <div class="grid-overlay"></div>

  
<nav>
  <div class="container">
    <a href="/" class="logo">
      <span class="logo-f">f</span><span class="logo-slash">/</span><span class="logo-f">f</span>
    </a>
    <ul class="nav-menu">
      <li><a href="/#work">Work</a></li>
      <li><a href="/experiments.html" class="">Experiments</a></li>
      <li><a href="/search.html">Search</a></li>
      <li><a href="mailto:your.email@example.com">Contact</a></li>
    </ul>
    <button class="nav-toggle" aria-label="Toggle menu">
      <span></span>
      <span></span>
      <span></span>
    </button>
  </div>
</nav>

  <section style="padding-top: 8rem;">
    <div class="container">
      <div class="section-header">
        <h2>Search Posts</h2>
        <p>Find technical posts across all categories</p>
      </div>
      
      <div class="search-container">
        <input 
          type="text" 
          id="search-input" 
          placeholder="Search for posts, techniques, or topics..." 
          class="search-input"
        />
      </div>
      
      <div id="search-results" class="search-results"></div>
    </div>
  </section>

  
<footer>
  <div class="container">
    <div class="footer-content">
      <div class="footer-logo">
        <span class="logo-f">f</span><span class="logo-slash">/</span><span class="logo-f">f</span>
      </div>
      <p>Fridays with Faraday - Working with microcontrollers and embedded systems</p>
      <div class="footer-links">
        <a href="/rss.xml">RSS Feed</a>
        <a href="https://github.com/yourusername/yourusername.github.io">GitHub</a>
      </div>
    </div>
  </div>
</footer>

  <script src="js/main.js"></script>
  <script>

window.SEARCH_DATA = [{"title":"Minimal Bare Metal Bootloader","description":"**ARM Cortex-M4** • **Bootloader** • **Assembly**","category":"experiments","tags":[],"url":"/experiments/bootloader.html","content":"# Minimal Bare Metal Bootloader\n\n**ARM Cortex-M4** • **Bootloader** • **Assembly**\n\nWriting a minimal bootloader from scratch without vendor HAL libraries. Just startup assembly, a linker script, and some C code. The goal was to understand what actually happens before main() runs.\n\n## Results\n\n| Metric | Value |\n|--------|-------|\n| Binary Size | **2KB** |\n| Boot Time | **50ms** |\n| Flash Reserved | **8KB** |\n| Success Rate | **100%** |\n\n## Why Write a Bootloader?\n\nI wanted to implement field fi"},{"title":"Getting ESP32 to 12µA Sleep Current","description":"**Tags:** ESP32 • Low Power • Deep Sleep","category":"experiments","tags":[],"url":"/experiments/esp32-low-power.html","content":"# Getting ESP32 to 12µA Sleep Current\n\n**Tags:** ESP32 • Low Power • Deep Sleep\n\nOut of the box, ESP32 deep sleep pulls around 150µA. This documents how I got it down to 12µA by tweaking power domains, disabling peripherals, and configuring GPIO properly.\n\n## Results\n\n| Metric | Value |\n|--------|--------|\n| **Sleep Current** | 12µA |\n| **Wake Time** | 800ms |\n| **Power Reduction** | 92% |\n| **CR2032 Battery Life** | 3 Years |\n\n## The Problem\n\nI needed to run a sensor node on a coin cell for yea"},{"title":"High-Speed ADC with DMA","description":"**STM32F4** **DMA** **ADC**","category":"experiments","tags":[],"url":"/experiments/stm32-dma.html","content":"# High-Speed ADC with DMA\n\n**STM32F4** **DMA** **ADC**\n\nSetting up continuous 2 MSPS ADC sampling on STM32F407 using circular DMA buffers. The goal was to capture high-speed analog signals while keeping the CPU free for processing.\n\n---\n\n## Results\n\n| Metric | Value |\n|--------|-------|\n| **Samples/Second** | 2M |\n| **CPU Usage** | 6% |\n| **Buffer Size** | 4KB |\n| **Samples Lost** | 0 |\n\n---\n\n## The Goal\n\nI needed to continuously sample an analog signal at 2 MHz for a digital oscilloscope projec"},{"title":"ESP32 High-Speed ADC Performance: DMA and Interrupt Analysis","description":"High-speed analog-to-digital conversion on microcontrollers often becomes CPU-bound long before hitting the advertised sampling rates. The ESP32 integrates two successive approximation register (SAR) ","category":"experiments","tags":[],"url":"/experiments/esp32-adc-performance.html","content":"# ESP32 High-Speed ADC Performance: DMA and Interrupt Analysis\n\n## Executive Summary: The ADC Bottleneck and DMA Solution\n\nHigh-speed analog-to-digital conversion on microcontrollers often becomes CPU-bound long before hitting the advertised sampling rates. The ESP32 integrates two successive approximation register (SAR) ADCs with up to 18 channels, capable of 12-bit resolution and theoretical sampling frequencies up to 83.3 kHz in digital controller mode. However, without direct memory access ("},{"title":"ESP32 Power Management Trade-offs: Register-Level Investigation","description":"Power management on ESP32 involves complex trade-offs between voltage regulation efficiency, clock configuration optimization, power domain control, and application performance requirements. While Esp","category":"experiments","tags":[],"url":"/experiments/esp32-power-management.html","content":"# ESP32 Power Management Trade-offs: Register-Level Investigation\n\n## Executive Summary: The Power-Performance-Complexity Triangle\n\nPower management on ESP32 involves complex trade-offs between voltage regulation efficiency, clock configuration optimization, power domain control, and application performance requirements. While Espressif provides comprehensive APIs for power management, achieving optimal power consumption requires deep understanding of the hardware's voltage regulator architectur"},{"title":"Achieving Sub‑1µA Sleep Currents on ESP32: A Register‑Level, Memory‑ and Timing‑Aware Methodology","description":"Ultra‑low power systems demand a disciplined understanding of silicon behavior, memory placement, and clock/power domains. On the ESP32, sleep current is shaped by Dynamic Frequency Scaling (DFS), aut","category":"experiments","tags":[],"url":"/experiments/esp32-ultra-low-power.html","content":"# Achieving Sub‑1µA Sleep Currents on ESP32: A Register‑Level, Memory‑ and Timing‑Aware Methodology\n\n## Executive Summary: Why Sub‑1µA Matters and What It Takes on ESP32\n\nUltra‑low power systems demand a disciplined understanding of silicon behavior, memory placement, and clock/power domains. On the ESP32, sleep current is shaped by Dynamic Frequency Scaling (DFS), auto light‑sleep, deep‑sleep policy, and how GPIO, RTC, and flash are configured. While Espressif’s low‑power guide provides indicat"},{"title":"ESP32 Real-Time WiFi Performance: MAC Layer Analysis","description":"Achieving reliable real-time WiFi performance on ESP32 presents unique challenges due to the complex interactions between the IEEE 802.11 MAC layer, firmware drivers, and application timing constraint","category":"experiments","tags":[],"url":"/experiments/esp32-wifi-performance.html","content":"# ESP32 Real-Time WiFi Performance: MAC Layer Analysis\n\n## Executive Summary: Real-Time WiFi Challenges on ESP32\n\nAchieving reliable real-time WiFi performance on ESP32 presents unique challenges due to the complex interactions between the IEEE 802.11 MAC layer, firmware drivers, and application timing constraints. While ESP32 supports WiFi standards from 802.11b to 802.11n with advanced features like HT40, QoS, and AMPDU aggregation, the wireless medium introduces inherent latency and variabili"},{"title":"Gaudi2 Memory Subsystem Analysis and Optimization: Deep Technical Guide","description":"In AI accelerator design, the memory subsystem determines whether theoretical compute performance translates into real-world performance. Gaudi2's memory architecture represents a radical departure fr","category":"experiments","tags":[],"url":"/experiments/gaudi-memory-subsystem.html","content":"# Gaudi2 Memory Subsystem Analysis and Optimization: Deep Technical Guide\n\n## Executive Summary: Memory as the Performance Bottleneck\n\nIn AI accelerator design, the memory subsystem determines whether theoretical compute performance translates into real-world performance. Gaudi2's memory architecture represents a radical departure from traditional GPU design philosophies, favoring predictable, high-bandwidth memory access over general-purpose caching schemes.\n\nOur deep analysis reveals that Gaud"},{"title":"Mixed-Precision Arithmetic Performance on Gaudi2: FP16/BF16 Implementation Analysis","description":"Mixed-precision arithmetic represents one of the most significant advances in deep learning acceleration, reducing computational requirements and memory bandwidth while maintaining model accuracy. Gau","category":"experiments","tags":[],"url":"/experiments/gaudi-mixed-precision.html","content":"# Mixed-Precision Arithmetic Performance on Gaudi2: FP16/BF16 Implementation Analysis\n\n## Executive Summary: Precision vs Performance Trade-offs\n\nMixed-precision arithmetic represents one of the most significant advances in deep learning acceleration, reducing computational requirements and memory bandwidth while maintaining model accuracy. Gaudi2's implementation of FP16, BF16, and FP8 arithmetic showcases sophisticated hardware acceleration that fundamentally changes the performance characteri"},{"title":"Gaudi2 vs NVIDIA H100: A Deep Technical Performance Analysis","description":"When Intel released the Gaudi2 accelerator, the market's immediate question was simple: how does it stack up against NVIDIA's H100? After extensive testing and analysis, the answer is nuanced but defi","category":"experiments","tags":[],"url":"/experiments/gaudi-vs-h100.html","content":"# Gaudi2 vs NVIDIA H100: A Deep Technical Performance Analysis\n\n## Executive Summary: The Numbers Don't Lie\n\nWhen Intel released the Gaudi2 accelerator, the market's immediate question was simple: how does it stack up against NVIDIA's H100? After extensive testing and analysis, the answer is nuanced but definitive. According to our comprehensive benchmarking, the performance relationship between Gaudi2 and H100 varies significantly by workload and application scenario.\n\nThe raw performance gap i"},{"title":"Gaudi2 Architecture Deep Dive for AI Workloads","description":"Intel’s Gaudi2 is a second-generation AI training accelerator built around a deliberate separation of concerns: a configurable Matrix Multiplication Engine (MME) optimized for GEMMs and convolutions, ","category":"experiments","tags":[],"url":"/experiments/gaudi2-architecture.html","content":"# Gaudi2 Architecture Deep Dive for AI Workloads\n\n## Executive Summary: What Gaudi2 Is and Why It Matters\n\nIntel’s Gaudi2 is a second-generation AI training accelerator built around a deliberate separation of concerns: a configurable Matrix Multiplication Engine (MME) optimized for GEMMs and convolutions, and a programmable Tensor Processor Core (TPC) for everything else. The result is a system architected for throughput, scalability, and predictable behavior under production workloads, rather t"},{"title":"DirectX Video API Performance: Driver Internals Analysis","description":"Modern video decode pipelines push complex coordination requirements across user-mode APIs, driver layers, and GPU command submission paths. DirectX Video Acceleration 2.0 (DXVA2) formalized a clear s","category":"experiments","tags":[],"url":"/experiments/dxva-performance.html","content":"# DirectX Video API Performance: Driver Internals Analysis\n\n## Executive Summary\n\nModern video decode pipelines push complex coordination requirements across user-mode APIs, driver layers, and GPU command submission paths. DirectX Video Acceleration 2.0 (DXVA2) formalized a clear split between the host decoder (software layer) and the accelerator (driver/GPU), enabling efficient offload of decode work through a well-defined device driver interface (DDI). At the driver level on Linux, the i915 ke"},{"title":"GPU Acceleration Pipeline Analysis with Level Zero","description":"Intel's Level Zero API represents the lowest-level interface between applications and Intel GPU hardware, providing direct access to compute and acceleration capabilities. This report provides an in-d","category":"experiments","tags":[],"url":"/experiments/level-zero-analysis.html","content":"# GPU Acceleration Pipeline Analysis with Level Zero\n\n## Executive Summary\n\nIntel's Level Zero API represents the lowest-level interface between applications and Intel GPU hardware, providing direct access to compute and acceleration capabilities. This report provides an in-depth technical analysis of the Level Zero driver architecture, GPU acceleration pipeline, command submission mechanisms, and performance optimization strategies at the driver and system level. Drawing from Intel's official d"},{"title":"Multi-threading Performance with VAAPI","description":"Video processing workloads represent one of the most computationally demanding scenarios in modern graphics systems, requiring sophisticated multi-threading strategies to achieve optimal performance. ","category":"experiments","tags":[],"url":"/experiments/vaapi-multithreading.html","content":"# Multi-threading Performance with VAAPI\n\n## Executive Summary\n\nVideo processing workloads represent one of the most computationally demanding scenarios in modern graphics systems, requiring sophisticated multi-threading strategies to achieve optimal performance. The Video Acceleration API (VAAPI) provides a robust framework for hardware-accelerated video processing, but its performance characteristics are fundamentally shaped by thread synchronization mechanisms, buffer management strategies, a"},{"title":"Cache Hierarchy Optimization in Attention Mechanisms","description":"This deep technical analysis examines cache hierarchy optimization in attention mechanisms for transformer models, focusing on CPU cache behavior, memory access patterns, and cache miss analysis. Thro","category":"experiments","tags":[],"url":"/experiments/llm-cache-hierarchy.html","content":"# Cache Hierarchy Optimization in Attention Mechanisms\n\n## Executive Summary\n\nThis deep technical analysis examines cache hierarchy optimization in attention mechanisms for transformer models, focusing on CPU cache behavior, memory access patterns, and cache miss analysis. Through detailed profiling using perf, cache simulation tools, and memory access analysis, we reveal how attention mechanisms interact with modern cache hierarchies and provide practical optimization strategies. Key findings i"},{"title":"CPU vs GPU Inference: A System Call Analysis","description":"This deep technical analysis examines system-level behavior during CPU vs GPU inference for large language models, focusing on process scheduling, system calls, and hardware interrupt patterns. Throug","category":"experiments","tags":[],"url":"/experiments/llm-cpu-gpu-system-calls.html","content":"# CPU vs GPU Inference: A System Call Analysis\n\n## Executive Summary\n\nThis deep technical analysis examines system-level behavior during CPU vs GPU inference for large language models, focusing on process scheduling, system calls, and hardware interrupt patterns. Through extensive profiling using strace, perf, and advanced debugging tools, we reveal fundamental differences in system behavior that explain performance characteristics. Key findings include optimal CPU thread counts (4-5 threads), s"},{"title":"Tracing GPU Memory Bandwidth in Transformer Models","description":"Transformer inference at scale is dominated by memory traffic, not floating-point arithmetic. Across a broad set of modern models and batch sizes, decode-phase attention kernels exhibit arithmetic int","category":"experiments","tags":[],"url":"/experiments/llm-gpu-memory-bandwidth.html","content":"# Tracing GPU Memory Bandwidth in Transformer Models\n\n## Executive Summary\n\nTransformer inference at scale is dominated by memory traffic, not floating-point arithmetic. Across a broad set of modern models and batch sizes, decode-phase attention kernels exhibit arithmetic intensities clustered around 0.5–1.0 operations per byte— squarely in the memory-bound regime. Profiling evidence from large-batch runs shows DRAM read utilization frequently hitting 60–80% while compute warps in flight remain "},{"title":"Hardware-Accelerated Matrix Multiplication Deep Dive","description":"This deep technical analysis examines hardware-accelerated matrix multiplication in CUDA kernels, providing a comprehensive reverse engineering study of CUDA kernels, PTX assembly analysis, and perfor","category":"experiments","tags":[],"url":"/experiments/llm-matrix-multiplication.html","content":"# Hardware-Accelerated Matrix Multiplication Deep Dive\n\n## Executive Summary\n\nThis deep technical analysis examines hardware-accelerated matrix multiplication in CUDA kernels, providing a comprehensive reverse engineering study of CUDA kernels, PTX assembly analysis, and performance counter analysis. Through systematic optimization of Single-Precision General Matrix Multiplication (SGEMM) kernels, we demonstrate progression from a naive implementation achieving 309 GFLOPs (1.3% of peak) to an op"},{"title":"Memory Bandwidth Bottlenecks in Large Language Models","description":"This deep technical analysis examines memory bandwidth bottlenecks in large language model inference, using strace, perf, and advanced memory profiling techniques to identify and resolve performance l","category":"experiments","tags":[],"url":"/experiments/llm-memory-bottlenecks.html","content":"# Memory Bandwidth Bottlenecks in Large Language Models\n\n## Executive Summary\n\nThis deep technical analysis examines memory bandwidth bottlenecks in large language model inference, using strace, perf, and advanced memory profiling techniques to identify and resolve performance limitations. Recent research reveals that large-batch LLM inference remains memory-bound, with DRAM bandwidth saturation occurring at ~76% read utilization even with optimized attention kernels like FlashAttention. Our com"},{"title":"Batch Processing Performance Analysis in vLLM","description":"Batch processing in vLLM represents the architectural foundation that enables high-throughput language model inference through dynamic batching, intelligent scheduling, and continuous memory managemen","category":"experiments","tags":[],"url":"/experiments/vllm-batch-processing.html","content":"# Batch Processing Performance Analysis in vLLM\n\n## Executive Summary: Batching as the Throughput Foundation\n\nBatch processing in vLLM represents the architectural foundation that enables high-throughput language model inference through dynamic batching, intelligent scheduling, and continuous memory management. Unlike static batching systems that require fixed batch sizes and sacrifice either latency or throughput, vLLM's continuous batching architecture adapts to incoming requests in real-time,"},{"title":"vLLM Internals: Tracing vLLM's KV Cache Management","description":"The key-value (KV) cache is the memory substrate that sustains autoregressive decoding in large language models. As tokens are generated, each layer’s attention mechanism reads previously computed key","category":"experiments","tags":[],"url":"/experiments/vllm-kv-cache.html","content":"# vLLM Internals: Tracing vLLM's KV Cache Management\n\n## Executive Summary: Why KV Cache is Central to vLLM\n\nThe key-value (KV) cache is the memory substrate that sustains autoregressive decoding in large language models. As tokens are generated, each layer’s attention mechanism reads previously computed keys and values, and appends new keys and values for the next step. If these tensors cannot be retained in fast memory, performance collapses under the cost of recomputation or transfer. vLLM’s "},{"title":"Memory Pool Optimization in vLLM","description":"Memory management in vLLM represents the fundamental infrastructure that enables high-throughput language model inference. Unlike traditional inference engines that rely on static memory allocation, v","category":"experiments","tags":[],"url":"/experiments/vllm-memory-pool.html","content":"# Memory Pool Optimization in vLLM\n\n## Executive Summary: Memory as the Performance Foundation\n\nMemory management in vLLM represents the fundamental infrastructure that enables high-throughput language model inference. Unlike traditional inference engines that rely on static memory allocation, vLLM employs a sophisticated hybrid memory architecture that dynamically manages GPU memory through PagedAttention, implementing operating system-inspired virtual memory concepts to handle the extreme memo"},{"title":"Performance Profiling of vLLM Token Generation Pipeline","description":"The token generation pipeline in vLLM is the critical path where autoregressive decoding transforms from initial prompt processing into iterative token-by-token generation. While modern GPUs excel at ","category":"experiments","tags":[],"url":"/experiments/vllm-token-generation.html","content":"# Performance Profiling of vLLM Token Generation Pipeline\n\n## Executive Summary: Pipeline Performance as Bottleneck\n\nThe token generation pipeline in vLLM is the critical path where autoregressive decoding transforms from initial prompt processing into iterative token-by-token generation. While modern GPUs excel at tensor operations, vLLM's v0.6.0 performance analysis revealed that CPU overhead—particularly in scheduling, API server operations, and output processing—was constraining GPU utilizat"},{"title":"\"ESP32 Advanced Power Management: Ultra-Low Power Techniques\"","description":"\"Deep dive into ESP32 power management techniques including dynamic voltage scaling, power domain control, and assembly optimization for maximum battery life.\"","category":"\"esp32\"","tags":["power-management","ultra-low-power","ESP32","battery","optimization"],"url":"/\"esp32\"/esp32-advanced-power-management.html","content":"# ESP32 Advanced Power Management: Ultra-Low Power Techniques\n\n## Executive Summary: Pushing ESP32 to Its Limits\n\nThe ESP32 microcontroller offers remarkable power efficiency capabilities, but achieving true ultra-low power operation requires understanding advanced techniques that go beyond basic sleep modes. This analysis examines dynamic voltage scaling, power domain granular control, clock optimization, and assembly-level optimizations that can reduce power consumption by up to 80% compared t"}];

function searchPosts(query) {
  const results = [];
  const searchTerm = query.toLowerCase();
  
  window.SEARCH_DATA.forEach(post => {
    const searchableText = [
      post.title,
      post.description,
      post.category,
      post.tags.join(' '),
      post.content
    ].join(' ').toLowerCase();
    
    if (searchableText.includes(searchTerm)) {
      results.push({
        title: post.title,
        description: post.description,
        category: post.category,
        url: post.url,
        relevance: searchableText.split(searchTerm).length - 1
      });
    }
  });
  
  return results.sort((a, b) => b.relevance - a.relevance);
}

function displaySearchResults(results) {
  const resultsContainer = document.getElementById('search-results');
  
  if (results.length === 0) {
    resultsContainer.innerHTML = '<p>No results found.</p>';
    return;
  }
  
  resultsContainer.innerHTML = results.map(result => `
    <div class="search-result">
      <h3><a href="${result.url}">${result.title}</a></h3>
      <p>${result.description}</p>
      <span class="search-category">${result.category}</span>
    </div>
  `).join('');
}

// Auto-search on page load if URL has hash
document.addEventListener('DOMContentLoaded', function() {
  const urlParams = new URLSearchParams(window.location.search);
  const query = urlParams.get('q');
  
  if (query) {
    document.getElementById('search-input').value = query;
    const results = searchPosts(query);
    displaySearchResults(results);
  }
  
  // Search on input
  document.getElementById('search-input').addEventListener('input', function(e) {
    const query = e.target.value;
    if (query.length > 2) {
      const results = searchPosts(query);
      displaySearchResults(results);
    } else {
      document.getElementById('search-results').innerHTML = '';
    }
  });
});

  </script>
</body>
</html>