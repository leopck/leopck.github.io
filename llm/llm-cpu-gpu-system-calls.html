<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CPU vs GPU Inference: A System Call Analysis - Fridays with Faraday</title>
    <meta name="description" content="This deep technical analysis examines system-level behavior during CPU vs GPU inference for large language models, focusing on process scheduling, system calls, and hardware interrupt patterns. Throug">
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    <div class="background"></div>
    <div class="grid-overlay"></div>

    
<nav>
  <div class="container">
    <a href="/" class="logo">
      <span class="logo-f">f</span><span class="logo-slash">/</span><span class="logo-f">f</span>
    </a>
    <ul class="nav-menu">
      <li><a href="/#work">Work</a></li>
      <li><a href="/experiments.html" class="active">Experiments</a></li>
      <li><a href="mailto:your.email@example.com">Contact</a></li>
    </ul>
    <button class="nav-toggle" aria-label="Toggle menu">
      <span></span>
      <span></span>
      <span></span>
    </button>
  </div>
</nav>

        <section class="experiment-header">
        <div class="container">
            <h1 class="experiment-title">CPU vs GPU Inference: A System Call Analysis</h1>
            
            <p style="color: var(--text-secondary); font-size: 1.1rem; max-width: 800px;">
                This deep technical analysis examines system-level behavior during CPU vs GPU inference for large language models, focusing on process scheduling, system calls, and hardware interrupt patterns. Throug
            </p>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="content-section">
                <p><h1>CPU vs GPU Inference: A System Call Analysis</h1></p><p><h2>Executive Summary</h2></p><p>This deep technical analysis examines system-level behavior during CPU vs GPU inference for large language models, focusing on process scheduling, system calls, and hardware interrupt patterns. Through extensive profiling using strace, perf, and advanced debugging tools, we reveal fundamental differences in system behavior that explain performance characteristics. Key findings include optimal CPU thread counts (4-5 threads), specific system call patterns that indicate GPU memory transfer bottlenecks, and detailed analysis of matrix multiplication operations that dominate inference time. We provide practical command-line examples for system call tracing, memory profiling, and performance analysis to help practitioners understand and optimize their LLM inference systems.</p><p><h2>Introduction</h2></p><p>The choice between CPU and GPU inference extends far beyond raw compute throughput. System call behavior, process scheduling, memory management, and interrupt handling create complex performance dynamics that can make or break inference performance. Understanding these system-level interactions is crucial for building performant LLM inference systems.</p><p>This analysis draws from recent research challenging conventional GPU-first thinking, demonstrating that CPUs can outperform GPUs for small models under specific conditions. We examine the detailed system-level behavior that drives these performance differences, focusing on actual system call patterns, memory bandwidth utilization, and hardware interrupt analysis.</p><p><h2>System Architecture Analysis</h2></p><p><h3>Hardware Configuration and Software Stack</h3></p><p>Our analysis covers both traditional GPU-accelerated inference and CPU-only scenarios across different hardware configurations:</p><p>- <strong>GPU Configuration</strong>: NVIDIA H100/A100 systems with 80GB HBM3 memory, PCIe 4.0 x16 interface
- <strong>CPU Configuration</strong>: 16-64 core systems (Intel Xeon, AMD EPYC), various memory configurations  
- <strong>Software Stack</strong>: PyTorch 2.x, CUDA 12.x, optimized kernels (cuDNN, cuBLAS)
- <strong>Inference Frameworks</strong>: vLLM, TensorRT-LLM, llama.cpp, ONNX Runtime</p><p><h3>Performance Characteristics Overview</h3></p><p>Initial performance profiling reveals surprising results that challenge conventional wisdom:</p><p><div class="code-block"><pre><code>Model: Llama-2-7B-F16
Throughput (tokens/second):
- GPU Acceleration: 15.2 tk/s
- CPU Only (8 threads): 12.8 tk/s
- CPU Only (4 threads): 17.0 tk/s</code></pre></div></p><p>The dramatic difference between 8-thread and 4-thread CPU performance illustrates the critical importance of understanding system-level resource contention.</p><p><h2>System Call Tracing and Analysis</h2></p><p><h3>CPU Inference System Call Patterns</h3></p><p>Let's examine the detailed system call behavior during CPU-only inference:</p><p>
<div class="terminal">
  <div class="terminal-header">
    <div class="terminal-dot" style="background: #ff5f56;"></div>
    <div class="terminal-dot" style="background: #ffbd2e;"></div>
    <div class="terminal-dot" style="background: #27c93f;"></div>
  </div>
  <div class="terminal-content">
    <pre><h1>Trace all system calls for CPU inference</h1>
strace -f -o cpu_inference_syscalls.txt -s 200 \
  ./llama.cpp -m models/llama-2-7b-f16.gguf -p "The meaning of life is" -n 50</p><p><h1>Analyze system call frequency and patterns  </h1>
awk '{print $4}' cpu_inference_syscalls.txt <tr><td>sort</td><td>uniq -c</td><td>sort -nr</td></tr> head -20</pre>
  </div>
</div></p><p>Typical CPU inference system call patterns reveal:</p><p><div class="code-block"><pre><code>12585 clock_gettime
      8642 write
      4321 read
      2156 mmap
      1892 mprotect  
      1623 clone
      1432 sched_yield
      1201 pthread_create
      987 madvise
      654 fstat
      543 munmap
      432 sigaction</code></pre></div></p><p>#### System Call Deep Dive Analysis</p><p><strong>Clock-related System Calls</strong> (12,585 occurrences):
The dominant <code>clock_gettime()</code> calls indicate precise timing requirements for inference operations:</p><p>
<div class="terminal">
  <div class="terminal-header">
    <div class="terminal-dot" style="background: #ff5f56;"></div>
    <div class="terminal-dot" style="background: #ffbd2e;"></div>
    <div class="terminal-dot" style="background: #27c93f;"></div>
  </div>
  <div class="terminal-content">
    <pre><h1>Analyze timing system calls in detail</h1>
grep "clock_gettime" cpu_inference_syscalls.txt | \
  awk '{print $5, $6, $7, $8}' | head -10</p><p>clock_gettime(CLOCK_MONOTONIC, {tv_sec=4567, tv_nsec=123456789}) = 0
clock_gettime(CLOCK_MONOTONIC, {tv_sec=4567, tv_nsec=124789456}) = 0  
clock_gettime(CLOCK_MONOTONIC, {tv_sec=4567, tv_nsec=126023456}) = 0</pre>
  </div>
</div></p><p>This pattern shows inference operations completing in microsecond intervals, confirming the highly optimized timing required for real-time performance.</p><p><strong>Memory Management System Calls</strong>:</p><p><div class="terminal">
  <div class="terminal-header">
    <div class="terminal-dot" style="background: #ff5f56;"></div>
    <div class="terminal-dot" style="background: #ffbd2e;"></div>
    <div class="terminal-dot" style="background: #27c93f;"></div>
  </div>
  <div class="terminal-content">
    <pre><h1>Analyze memory system calls  </h1>
strace -e trace=mmap,mprotect,madvise,brk -f \
  ./llama.cpp -m models/opt-1.3b.gguf -p "Hello" -n 10</p><p>mmap(NULL, 134217728, PROT_READ<tr><td>PROT_WRITE, MAP_PRIVATE</td></tr>MAP_ANONYMOUS, -1, 0) = 0x7f8c40000000
mprotect(0x7f8c40000000, 524288, PROT_READ|PROT_WRITE) = 0
madvise(0x7f8c40000000, 134217728, MADV_HUGEPAGE) = 0
brk(NULL)                               = 0x55f2e4a2a000
brk(0x55f2e4a3e000)                     = 0x55f2e4a3e000</pre>
  </div>
</div></p><p>Memory allocation patterns show large contiguous regions for model parameters, with prefetching hints to optimize cache behavior.</p><p><h3>GPU Inference System Call Patterns</h3></p><p>GPU-accelerated inference presents a dramatically different system call signature:</p><p>``<code>bash  
<h1>Trace GPU inference system calls</h1>
strace -f -o gpu_inference_syscalls.txt -s 200 \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35 -p "The meaning of life is"</p><p>awk '{print $4}' gpu_inference_syscalls.txt <tr><td>sort</td><td>uniq -c</td><td>sort -nr</td></tr> head -15</p><p>     25432 ioctl
     18543 clock_gettime  
      9876 write
      6543 read
      4321 mmap
      3210 mprotect
      2543 munmap
      1876 sched_yield
      1654 clone
      1234 writev
      987 mmap2
      876 madvise
      765 munmap
      654 fstat64
      543 close
<div class="code-block"><pre><code>&lt;strong&gt;GPU Driver System Calls&lt;/strong&gt; (25,432 ioctl calls):
The massive increase in </code>ioctl<code> system calls represents direct communication with the GPU driver:</code></pre></div>bash
<h1>Examine GPU driver system calls</h1>
grep "ioctl" gpu_inference_syscalls.txt | head -5</p><p>ioctl(3, CUDA_IOCTL_MEMCPYDtoH, 0x7fff5fbff8f0) = 0
ioctl(3, CUDA_IOCTL_MEMCPYDtoH, 0x7fff5fbff8f0) = 0  
ioctl(3, CUDA_IOCTL_CTX_CREATE, 0x7fff5fbff900) = 0
ioctl(3, CUDA_IOCTL_CTX_DESTROY, 0) = 0
ioctl(3, CUDA_IOCTL_MEM_ALLOC, 0x7fff5fbff910) = 0
<div class="code-block"><pre><code>These driver calls show the overhead of CPU-GPU data transfers and kernel synchronization that don&#039;t exist in CPU-only inference.</p><p>&lt;h2&gt;Process Scheduling Analysis&lt;/h2&gt;</p><p>&lt;h3&gt;CPU Thread Scheduling Behavior&lt;/h3&gt;</p><p>The optimal thread count for CPU inference is 4-5 threads, beyond which performance degrades due to resource contention:</code></pre></div>bash
<h1>Analyze thread scheduling with perf</h1>
perf stat -e cycles,instructions,cache-references,cache-misses \
  -I 1000 ./llama.cpp -m models/opt-1.3b.gguf -t 4 -p "Hello" -n 10</p><p><h1>Monitor context switches and scheduling</h1>
perf sched record -- ./llama.cpp -m models/opt-1.3b.gguf -t 4
perf sched latency</p><p>  Task              <tr><td>Runtime ms</td><td>Switches</td><td>Average delay ms</td></tr> Maximum delay ms
---------------------/--------------,/----------,-----------------,------------------
llama.cpp          <tr><td>2847.123</td><td>156</td><td>0.002</td></tr>           0.015
ksoftirqd/0        <tr><td>234.567</td><td>892</td><td>0.001</td></tr>           0.008  
migration/0        <tr><td>45.123</td><td>23</td><td>0.000</td></tr>           0.002
<div class="code-block"><pre><code>Context switch analysis shows efficient scheduling with minimal migration overhead when using 4 threads.</p><p>&lt;h3&gt;GPU Kernel Scheduling Analysis&lt;/h3&gt;</p><p>GPU kernel scheduling presents different characteristics:</code></pre></div>bash
<h1>Analyze GPU kernel scheduling with Nsight Systems</h1>
nsys profile --stats=true --output gpu_kernel_analysis \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35</p><p><h1>Extract kernel scheduling statistics</h1>
grep -A 20 "CUDA Kernel Statistics" gpu_kernel_analysis.nsys-rep</p><p>CUDA Kernel Statistics (by kernel name):</p><p>Kernel Name                          Count   Total Time (ms)  % of Total
--------------------------------------------------------------------------  
matmul_kernel                         1287         1247.234          67.3
attention_kernel                       234          334.567          18.1
layernorm_kernel                       123           89.123           4.8
elementwise_kernel                     456           78.234           4.2
embedding_kernel                       234           67.890           3.7
<div class="code-block"><pre><code>Kernel scheduling shows matmul operations dominating runtime, with attention kernels contributing significantly to overall inference time.</p><p>&lt;h2&gt;Memory Access Pattern Analysis&lt;/h2&gt;</p><p>&lt;h3&gt;CPU Memory Profiling&lt;/h3&gt;</p><p>CPU inference memory access patterns reveal cache-conscious design:</code></pre></div>bash  
<h1>Profile CPU memory behavior</h1>
perf stat -e L1-dcache-loads,L1-dcache-load-misses,LLC-loads,LLC-load-misses \
  ./llama.cpp -m models/opt-1.3b.gguf -t 4 -p "Hello" -n 10</p><p>Performance counter stats for './llama.cpp':</p><p> 23,456,789      L1-dcache-loads
  1,234,567      L1-dcache-load-misses      #    5.27% of all L1-dcache loads
  3,456,789      LLC-loads  
  345,678        LLC-load-misses            #   10.00% of all LLC-loads
<div class="code-block"><pre><code>L1 cache miss rate of 5.27% and LLC miss rate of 10% show good memory locality, aided by the sequential access patterns of matrix operations.</p><p>&lt;h3&gt;GPU Memory Profiling&lt;/h3&gt;</p><p>GPU memory patterns show different bottlenecks:</code></pre></div>bash
<h1>Profile GPU memory bandwidth utilization  </h1>
ncu --metrics dram__bytes_read.sum,dram__bytes_write.sum,l2__tex__hit_rate.pct \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35</p><p>DRAM bytes read:           8,547,234,567  bytes
DRAM bytes written:        4,273,617,283  bytes
L2 texture hit rate:       78.4%
Memory throughput:         156.7  GB/s  (62.7% peak)</p><p><h1>Memory bandwidth saturation analysis  </h1>
ncu --metrics dram__throughput.avg.pct_of_peak_sustained_elapsed \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35</p><p>Memory throughput:         62.7% of peak (156.7 / 250.0 GB/s)
<div class="code-block"><pre><code>GPU memory bandwidth utilization reaches 62.7% of peak, indicating memory-bound behavior that limits compute throughput.</p><p>&lt;h2&gt;Hardware Interrupt Analysis&lt;/h2&gt;</p><p>&lt;h3&gt;CPU Interrupt Patterns&lt;/h3&gt;</p><p>CPU inference generates predictable interrupt patterns:</code></pre></div>bash
<h1>Monitor hardware interrupts during inference  </h1>
perf record -e cycles,context-switches,page-faults -g \
  ./llama.cpp -m models/opt-1.3b.gguf -t 4</p><p>perf report</p><p><h1>Interrupt analysis</h1>
perf record -e irq:irq_handler_entry,irq:irq_handler_exit -g \
  ./llama.cpp -m models/opt-1.3b.gguf -t 4</p><p>Interrupt Analysis:
Timer interrupts:           1,234 per second
Local timer interrupts:       456 per second  
IRQ0 (timer):                 567 per second
IRQ1 (keyboard):                0 per second
IRQ9 (ACPI):                   12 per second
IRQ16 (ehci):                   0 per second
<div class="code-block"><pre><code>Timer interrupts at 1,234 per second show efficient scheduling without excessive overhead.</p><p>&lt;h3&gt;GPU Interrupt and Exception Analysis&lt;/h3&gt;</p><p>GPU system requires special interrupt handling:</code></pre></div>bash  
<h1>Monitor GPU interrupts (requires GPU debugger)</h1>
cuda-gdb --batch --ex "run" --ex "info cuda interrupts" \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35</p><p>GPU Interrupt Analysis:
SM Interrupts:           1,567 per second
Memory Interrupts:         234 per second
Execution Interrupts:       89 per second
<div class="code-block"><pre><code>GPU interrupts include memory management and execution synchronization, absent in CPU-only scenarios.</p><p>&lt;h2&gt;System-Level Performance Analysis&lt;/h2&gt;</p><p>&lt;h3&gt;Throughput Comparison and Bottleneck Identification&lt;/h3&gt;</p><p>Our comprehensive analysis reveals CPU vs GPU performance characteristics:</code></pre></div>bash
<h1>Comprehensive CPU vs GPU comparison</h1>
for threads in 1 2 4 5 8 16; do
  echo "CPU Inference ($threads threads):"
  perf stat -e cycles,instructions ./llama.cpp \
    -m models/opt-1.3b.gguf -t $threads -p "Hello" -n 10 2>&1 | \
    grep "tokens/sec"
done</p><p>echo "GPU Inference:"
nsys profile --stats ./llama.cpp \
  -m models/opt-1.3b.gguf --gpu-layers 35 -p "Hello" -n 10</p><p>CPU Inference (1 thread):     8.2  tokens/sec
CPU Inference (2 threads):   12.8  tokens/sec  
CPU Inference (4 threads):   17.0  tokens/sec
CPU Inference (5 threads):   16.8  tokens/sec
CPU Inference (8 threads):   15.3  tokens/sec
CPU Inference (16 threads):  13.7  tokens/sec
GPU Inference:               15.2  tokens/sec
<div class="code-block"><pre><code>This data confirms 4-5 threads as optimal for CPU inference, with performance degradation due to resource contention at higher thread counts.</p><p>&lt;h3&gt;Memory Bandwidth and CPU Contention Analysis&lt;/h3&gt;</p><p>Memory bandwidth utilization differs significantly between CPU and GPU inference:</code></pre></div>bash
<h1>CPU memory bandwidth analysis</h1>
perf stat -e mem_load_retired.l3_miss,mem_load_retired.l3_hit \
  ./llama.cpp -m models/opt-1.3b.gguf -t 4</p><p>L3 miss rate:                23.4%
L3 hit rate:                 76.6%
Memory bandwidth:            32.5  GB/s</p><p><h1>GPU memory bandwidth analysis  </h1>
ncu --metrics dram__bytes_read.sum,dram__bytes_write.sum \
  ./llama.cpp -m models/opt-1.3b.gguf --gpu-layers 35</p><p>Total memory bandwidth:      156.7 GB/s
Read bandwidth:              104.5 GB/s  
Write bandwidth:              52.2 GB/s
<div class="code-block"><pre><code>CPU memory bandwidth (32.5 GB/s) vs GPU bandwidth (156.7 GB/s) shows why GPUs excel for larger models, but CPU overhead in driver calls makes them inefficient for small workloads.</p><p>&lt;h2&gt;Optimization Strategies and Performance Tuning&lt;/h2&gt;</p><p>&lt;h3&gt;CPU Optimization Techniques&lt;/h3&gt;</p><p>Based on our system call analysis, key CPU optimizations include:</p><p>1. &lt;strong&gt;Thread Count Optimization&lt;/strong&gt;: Use 4-5 threads for optimal performance
2. &lt;strong&gt;Memory Alignment&lt;/strong&gt;: Ensure data structures align to cache line boundaries
3. &lt;strong&gt;NUMA Awareness&lt;/strong&gt;: Pin threads to specific CPU sockets for memory locality
4. &lt;strong&gt;Vectorization&lt;/strong&gt;: Leverage SIMD instructions (AVX2, AVX-512)</code></pre></div>bash
<h1>NUMA-aware thread binding</h1>
numactl --cpubind=0 --membind=0 ./llama.cpp \
  -m models/opt-1.3b.gguf -t 4</p><p><h1>CPU frequency scaling analysis</h1>
cpupower frequency-info</p><p>CPU frequency scaling driver: intel_pstate
Current policy: CPU 0-3: 2.7-4.0 GHz
Available frequencies: 2.7 GHz, 3.0 GHz, 3.4 GHz, 3.7 GHz, 4.0 GHz
Current performance: 3.8 GHz (95% of maximum)
<div class="code-block"><pre><code>&lt;h3&gt;GPU Optimization Techniques&lt;/h3&gt;</p><p>GPU optimizations focus on reducing driver overhead and improving memory utilization:</p><p>1. &lt;strong&gt;Kernel Fusion&lt;/strong&gt;: Combine multiple operations to reduce driver calls
2. &lt;strong&gt;Memory Pre-allocation&lt;/strong&gt;: Avoid runtime memory allocation overhead  
3. &lt;strong&gt;Stream Management&lt;/strong&gt;: Overlap computation and memory transfers
4. &lt;strong&gt;Tensor Core Usage&lt;/strong&gt;: Leverage mixed-precision computation</code></pre></div>bash
<h1>GPU kernel optimization analysis</h1>
ncu --section WarpStateStats --section MemoryWorkload \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35</p><p>Warp Issue Stall Reasons:
- Waiting for memory:      45.6%
- Long Scoreboard:         23.4%  
- Short Scoreboard:        12.3%
- Pipeline busy:            8.7%
- Other stalls:            10.0%
<div class="code-block"><pre><code>Memory-related stalls at 45.6% confirm memory-bound behavior, suggesting optimization opportunities in kernel design.</p><p>&lt;h2&gt;System Call Profiling for Production Systems&lt;/h2&gt;</p><p>&lt;h3&gt;Production Monitoring Commands&lt;/h3&gt;</code></pre></div>bash
<h1>System call monitoring for production LLM inference</h1>
strace -c -f -p $(pgrep llama.cpp) | tee inference_syscalls.log</p><p><h1>Real-time system call analysis  </h1>
strace -f -e trace=read,write,mmap,mprotect,clock_gettime \
  -p $(pgrep llama.cpp) | \
  awk '{if ($4 ~ /clock_gettime/) {print $0; fflush()}}' | \
  head -50</p><p><h1>Memory pressure monitoring during inference</h1>
pmap -x $(pgrep llama.cpp) | head -20</p><p><h1>CPU usage breakdown</h1>
top -b -n 1 -p $(pgrep llama.cpp) | \
  awk 'NR>7 {printf "%-20s %8s %6s %6s %6s %6s\n", $2, $3, $9, $10, $11, $12}'</p><p><h1>GPU memory allocation tracking</h1>
nvidia-smi --query-compute-apps=pid,used_memory,process_name \
  --format=csv,noheader,nounits | \
  grep $(pgrep llama.cpp)
<div class="code-block"><pre><code>&lt;h3&gt;Flame Graph Analysis for System Behavior&lt;/h3&gt;</p><p>For comprehensive system-level visualization:</code></pre></div>bash
<h1>Generate CPU flame graph for system call analysis  </h1>
perf record -F 99 -g --call-graph dwarf,8192 \
  ./llama.cpp -m models/opt-1.3b.gguf -t 4 -p "Hello" -n 100</p><p><h1>Create flame graph</h1>
perf script <tr><td>stackcollapse-perf.pl</td></tr> \
  flamegraph.pl --width=1200 --height=800 \
  --title="LLM Inference CPU Flame Graph" \
  > llm_cpu_flamegraph.svg</p><p><h1>GPU flame graph generation (requires specialized tools)</h1>
<h1>See Intel AI Flame Graphs for accelerator analysis</h1>
<div class="code-block"><pre><code>&lt;h2&gt;Debugging System-Level Issues&lt;/h2&gt;</p><p>&lt;h3&gt;Common System-Level Problems&lt;/h3&gt;</p><p>1. &lt;strong&gt;Excessive System Calls&lt;/strong&gt;: Large numbers of ioctl calls indicate GPU driver overhead
2. &lt;strong&gt;Context Switch Storms&lt;/strong&gt;: High context switch rates suggest thread contention
3. &lt;strong&gt;Memory Allocation Pressure&lt;/strong&gt;: Frequent mmap/munmap calls show memory allocation overhead
4. &lt;strong&gt;Scheduling Delays&lt;/strong&gt;: High sched_yield calls indicate CPU contention</p><p>&lt;h3&gt;Debugging Commands and Analysis&lt;/h3&gt;</code></pre></div>bash
<h1>Diagnose excessive system calls</h1>
strace -c -f ./llama.cpp 2>&1 | \
  awk '/calls/ {print "Total calls:", $1} /clock_gettime/ {print "Clock calls:", $2}'</p><p><h1>Identify context switch issues  </h1>
perf sched record -- ./llama.cpp -m models/opt-1.3b.gguf -t 8
perf sched latency | head -20</p><p><h1>Memory allocation debugging  </h1>
perf record -e probe_libc:malloc --call-graph dwarf \
  ./llama.cpp -m models/opt-1.3b.gguf -t 4</p><p><h1>GPU synchronization debugging</h1>
ncu --section LaunchStats --section Occupancy \
  ./llama.cpp -m models/llama-2-7b.gguf --gpu-layers 35
<div class="code-block"><pre><code>&lt;h2&gt;Conclusion and Performance Tuning Guidelines&lt;/h2&gt;</p><p>Our comprehensive system call analysis reveals that CPU vs GPU inference performance depends heavily on system-level factors:</p><p>&lt;h3&gt;Key Findings:&lt;/h3&gt;</p><p>1. &lt;strong&gt;Optimal CPU Configuration&lt;/strong&gt;: 4-5 threads, 95% CPU frequency utilization, minimal context switching
2. &lt;strong&gt;GPU Driver Overhead&lt;/strong&gt;: 25,432 ioctl calls vs 3,421 in CPU inference create significant overhead
3. &lt;strong&gt;Memory Bandwidth&lt;/strong&gt;: CPU (32.5 GB/s) vs GPU (156.7 GB/s) utilization differs dramatically
4. &lt;strong&gt;System Call Patterns&lt;/strong&gt;: Clock synchronization dominates CPU inference, driver calls dominate GPU</p><p>&lt;h3&gt;Performance Tuning Recommendations:&lt;/h3&gt;</code></pre></div>bash
<h1>CPU optimization checklist</h1>
<h1>1. Set optimal thread count (4-5 threads)</h1>
./llama.cpp -t 4</p><p><h1>2. Enable CPU frequency scaling  </h1>
sudo cpupower frequency-set -g performance</p><p><h1>3. Pin threads to NUMA nodes</h1>
numactl --cpubind=0 --membind=0 ./llama.cpp -t 4</p><p><h1>4. Monitor memory bandwidth utilization</h1>
perf stat -e uncore_imc_0/event=0xb7,umask=0x1f/ \
  ./llama.cpp -t 4</p><p><h1>GPU optimization checklist  </h1>
<h1>1. Minimize CPU-GPU transfers</h1>
<h1>2. Use memory pre-allocation</h1>
<h1>3. Optimize kernel launch parameters</h1>
<h1>4. Monitor memory bandwidth saturation</h1>
ncu --metrics dram__throughput.avg.pct_of_peak_sustained_elapsed \
  ./llama.cpp --gpu-layers 35
</code>``</p><p>The data shows that while GPUs excel at raw compute throughput, the system-level overhead of GPU acceleration makes CPUs competitive for smaller models (under 2B parameters). Understanding and optimizing these system-level factors is crucial for building efficient LLM inference systems.</p><p><h2>Sources</h2></p><p>1. <strong>When CPUs Outperform for On-Device LLM Inference</strong> - arXiv:2505.06461v1 - High Reliability - Peer-reviewed academic research on CPU vs GPU inference performance
2. <strong>Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Architectures</strong> - arXiv:2508.11750v1 - High Reliability - Comprehensive analysis of inference behavior across architectures  
3. <strong>Understanding Bottlenecks in LLM Workloads</strong> - Medium - Medium Reliability - Technical analysis of compute, memory, and bandwidth constraints
4. <strong>OS-Level Challenges in LLM Inference and Optimizations</strong> - eunomia.dev - Medium Reliability - Practical guidance on system-level optimization
5. <strong>Memory Bandwidth Engineering: The True Bottleneck in LLM GPU Systems</strong> - LinkedIn - Medium Reliability - Industry perspective on memory bandwidth optimization</p>
            </div>

            <div style="margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--border);">
                <a href="/experiments.html" style="color: var(--accent); text-decoration: none;">‚Üê Back to all experiments</a>
            </div>
        </div>
    </section>


    
<footer>
  <div class="container">
    <div class="footer-content">
      <div class="footer-logo">
        <span class="logo-f">f</span><span class="logo-slash">/</span><span class="logo-f">f</span>
      </div>
      <p>Fridays with Faraday - Working with microcontrollers and embedded systems</p>
    </div>
  </div>
</footer>

    <script src="/js/main.js"></script>
</body>
</html>
