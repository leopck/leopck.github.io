<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>vLLM Internals: Tracing vLLM's KV Cache Management - Fridays with Faraday</title>
    <meta name="description" content="The key-value (KV) cache is the memory substrate that sustains autoregressive decoding in large language models. As tokens are generated, each layer’s attention mechanism reads previously computed key">
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    <div class="background"></div>
    <div class="grid-overlay"></div>

    
<nav>
  <div class="container">
    <a href="/" class="logo">
      <span class="logo-f">f</span><span class="logo-slash">/</span><span class="logo-f">f</span>
    </a>
    <ul class="nav-menu">
      <li><a href="/#work">Work</a></li>
      <li><a href="/experiments.html" class="active">Experiments</a></li>
      <li><a href="mailto:your.email@example.com">Contact</a></li>
    </ul>
    <button class="nav-toggle" aria-label="Toggle menu">
      <span></span>
      <span></span>
      <span></span>
    </button>
  </div>
</nav>

        <section class="experiment-header">
        <div class="container">
            <h1 class="experiment-title">vLLM Internals: Tracing vLLM's KV Cache Management</h1>
            
            <p style="color: var(--text-secondary); font-size: 1.1rem; max-width: 800px;">
                The key-value (KV) cache is the memory substrate that sustains autoregressive decoding in large language models. As tokens are generated, each layer’s attention mechanism reads previously computed key
            </p>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="content-section">
                <p><h1>vLLM Internals: Tracing vLLM's KV Cache Management</h1></p><p><h2>Executive Summary: Why KV Cache is Central to vLLM</h2></p><p>The key-value (KV) cache is the memory substrate that sustains autoregressive decoding in large language models. As tokens are generated, each layer’s attention mechanism reads previously computed keys and values, and appends new keys and values for the next step. If these tensors cannot be retained in fast memory, performance collapses under the cost of recomputation or transfer. vLLM’s central insight is to manage KV memory like an operating system manages virtual memory: divide it into fixed-size blocks, map logical token positions to physical pages on demand, and reuse or evict blocks with awareness of sequence identity and prefix sharing. This is the essence of PagedAttention.[^1]</p><p>In vLLM v0.6.0, the project disclosed that the dominant bottleneck to throughput on modern GPUs was CPU overhead, not GPU compute. The engine separated the API process from the inference engine, introduced multi-step scheduling, and overlapped output processing—all to keep the GPU fed and busy.[^2] Against that backdrop, KV cache allocation, evictions, and reuse become first-order concerns for tail latency and throughput: misconfigured memory budgets or inefficient block management translate into preemption, recomputation, and GPU stalls.</p><p>This deep dive analyzes KV cache internals at the code level and correlates them with observability signals. We begin with source-level structures and flows, then expand to eviction modes, preemption mechanics, and practical tracing and profiling methodologies that reveal how the KV cache behaves in production. We conclude with action-oriented tuning guidance and common failure modes.</p><p><h3>Key Takeaways</h3></p><p>- PagedAttention treats KV cache as OS-like virtual memory: fixed-size blocks, logical-to-physical mapping, and non-contiguous storage to reduce fragmentation and enable sharing.[^1]
- vLLM v0.6.0 reduced CPU overhead by separating the API server and inference engine, adopting multi-step scheduling, and overlapping output processing; KV memory tuning amplifies these wins by reducing preemption and recomputation.[^2]
- The fastest way to see KV cache behavior is with low-overhead system tracing: brk/mmap/page-fault flame graphs for heap growth patterns, perf stat for counters, and developer-facing metrics in vLLM for preemption and utilization.[^3][^7]</p><p>---</p><p><h2>Architecture and Source Map: KV Cache in vLLM</h2></p><p>vLLM’s serving stack is built around an OpenAI-compatible server process and a separate engine process. The engine encapsulates an executor, worker(s), model runner, scheduler, and cache engine. Configuration is grouped into Model, Cache, Scheduler, Parallel, and Device configurations, all coordinated by a top-level VllmConfig. This modularity allows the KV cache policy to be tuned independently from parallelism and scheduling decisions.[^4][^5][^14]</p><p>PagedAttention organizes KV tensors per sequence group by layer and head, but stores them as fixed-size blocks on the device. Logical token indices map to blocks; blocks can be allocated on demand, shared when prefixes match, and evicted under pressure. This design removes the need for static contiguous allocations sized for max sequence length and mitigates fragmentation.[^1]</p><p>To frame responsibilities, Table 1 summarizes core components and their relationship to KV cache.</p><p>To illustrate the architecture surface area and its cache touchpoints, Table 1 outlines the components that shape allocation, access, and eviction.</p><p>Table 1. Components and responsibilities relevant to KV cache</p><p><tr><td>Component</td><td>Primary Responsibility</td><td>KV Cache Touchpoints</td></tr>
<tr><td>---------------------</td><td>------------------------------------------------------</td><td>---------------------------------------------------------------------------</td></tr>
<tr><td>API server (P0)</td><td>Request validation, tokenization, streaming outputs</td><td>No direct KV access; drives request flow that triggers cache allocation</td></tr>
<tr><td>Engine core (P1)</td><td>Orchestration, config wiring</td><td>Constructs Cache/Scheduler configs; receives metrics for preemption</td></tr>
<tr><td>Executor</td><td>Distributed execution selection</td><td>Chooses worker layout affecting per-GPU KV budgets</td></tr>
<tr><td>Worker</td><td>Device-resident model execution</td><td>Holds device KV memory; performs block ops under runner’s instructions</td></tr>
<tr><td>Model Runner</td><td>Model execution and batch preparation</td><td>Constructs attention inputs with block-based KV addressing</td></tr>
<tr><td>Scheduler</td><td>Batching and scheduling decisions</td><td>Decides when to preempt/recompute; sets prefill/decode budgets</td></tr>
<tr><td>Cache Engine</td><td>Block allocation/free and logical mapping</td><td>Owns block tables; evicts under pressure; supports prefix reuse</td></tr></p><p>Configuration drives KV behavior. The CacheConfig determines block size, utilization budget, and swap space; the SchedulerConfig defines batching budgets and preemption policy; the ParallelConfig decides how model weights are sharded, thereby changing the memory available for KV per GPU.[^4][^5]</p><p>Table 2 distills the most influential CacheConfig and SchedulerConfig fields.</p><p>Table 2. Key configuration fields influencing KV cache</p><p><tr><td>Config</td><td>Field</td><td>Effect on KV Cache</td></tr>
<tr><td>------------------</td><td>-------------------------------</td><td>-------------------------------------------------------------------------------------------------------------------</td></tr>
<tr><td>CacheConfig</td><td>block_size</td><td>Size of a KV block in tokens; coarser blocks reduce metadata overhead but increase internal fragmentation</td></tr>
<tr><td>CacheConfig</td><td>gpu_memory_utilization</td><td>Fraction of GPU memory reserved for KV; higher values reduce preemption at the risk of OOM</td></tr>
<tr><td>CacheConfig</td><td>swap_space</td><td>For SWAP preemption, CPU space to offload KV; trades recompute for transfer</td></tr>
<tr><td>CacheConfig</td><td>cache_dtype</td><td>Data type for KV storage (e.g., fp16); affects capacity and算术精度</td></tr>
<tr><td>CacheConfig</td><td>enable_prefix_caching</td><td>Enables block sharing for identical prompt prefixes across requests</td></tr>
<tr><td>SchedulerConfig</td><td>max_num_batched_tokens</td><td>Token budget per batch; too low increases scheduling overhead, too high penalizes decode ITL</td></tr>
<tr><td>SchedulerConfig</td><td>max_num_seqs</td><td>Caps concurrent sequences; tighter cap reduces KV pressure</td></tr>
<tr><td>SchedulerConfig</td><td>enable_chunked_prefill</td><td>Splits large prefill; prioritizes decode to preserve ITL</td></tr>
<tr><td>SchedulerConfig</td><td>preemption_mode</td><td>RECOMPUTE (default) recomputes KV under pressure; SWAP offloads KV (slower in most cases)</td></tr>
<tr><td>ParallelConfig</td><td>tensor/pipeline parallel size</td><td>Shards model weights; more shards free memory for KV but add synchronization</td></tr></p><p><h3>Block-Based KV Storage and Logical Addressing</h3></p><p>Under PagedAttention, each sequence is mapped to KV blocks of fixed token capacity. Logical addressing translates token positions to block IDs and offsets, allowing non-contiguous physical storage. Copy-on-write enables prefix sharing: if two requests share the same prompt prefix, they can reference the same blocks until a divergence point, saving memory and compute.[^1]</p><p>This approach mirrors virtual memory paging: sequences obtain blocks on demand, free them when finished, and suffer eviction only under pressure. The design reduces external fragmentation and enables aggressive reuse.</p><p><h3>Engine Initialization and Cache Setup</h3></p><p>At startup, the engine initializes device context, constructs the CacheConfig (including gpu_memory_utilization and block_size), and performs warm-up runs that may capture CUDA graphs for subsequent steps. KV capacity is derived from available device memory after reserving space for activations and model weights; the CacheEngine then maintains block tables and allocates/free blocks as the Scheduler requests them.[^4][^5][^8]</p><p>---</p><p><h2>Tracing KV Cache Allocation, Eviction, and Reuse</h2></p><p>Understanding allocation dynamics, eviction triggers, and reuse opportunities requires correlating code paths with OS-level signals. We recommend a three-pronged approach: brk/mmap/page-fault flame graphs for growth hotspots, perf stat for counter baselines, and vLLM metrics for preemption and utilization trends.[^3][^6][^7]</p><p>Table 3 summarizes how syscalls map to KV cache behaviors.</p><p>Table 3. Syscall-to-KV-cache behavior mapping</p><p><tr><td>Syscall</td><td>What it reveals about KV cache</td><td>Typical frequency under load</td><td>Low-overhead sampling command</td></tr>
<tr><td>---------</td><td>----------------------------------------------------------</td><td>---------------------------------------------------</td><td>------------------------------------------------------------------</td></tr>
<tr><td>brk</td><td>Heap expansion; growth from allocator activity</td><td>Infrequent (often <1000/s in production)</td><td>perf stat -e syscalls:sys_enter_brk -I 1000 -a</td></tr>
<tr><td>mmap</td><td>Large mappings; arena extensions or explicit mappings</td><td>Moderate; depends on allocator and usage</td><td>perf record -e syscalls:sys_enter_mmap -ag -- sleep 60</td></tr>
<tr><td>page-fault</td><td>Physical memory consumption when writes populate pages</td><td>Low; but informative for growth hotspots</td><td>perf record -e page-faults -ag -- sleep 60</td></tr>
<tr><td>munmap</td><td>Unmapping; confirms memory returned to OS</td><td>Variable; often sparse for long-lived caches</td><td>perf record -e syscalls:sys_enter_munmap -ag -- sleep 60</td></tr></p><p>To focus the investigation, Table 4 lists developer-facing metrics and their diagnostic value.</p><p>Table 4. vLLM metrics for KV cache (monitor via /metrics)</p><p><tr><td>Metric</td><td>Meaning</td><td>Diagnostic use-case</td></tr>
<tr><td>----------------------------------</td><td>-----------------------------------------------------</td><td>----------------------------------------------------------</td></tr>
<tr><td>vllm_kv_cache_blocks</td><td>Total KV cache blocks</td><td>Capacity planning and utilization</td></tr>
<tr><td>vllm_kv_cache_used_blocks</td><td>Used blocks</td><td>Utilization trend; preemption predictor</td></tr>
<tr><td>vllm_kv_cache_free_blocks</td><td>Free blocks</td><td>Headroom; risk of preemption</td></tr>
<tr><td>vllm_kv_cache_frag_ratio</td><td>Fragmentation ratio</td><td>Too many small free blocks may signal fragmentation</td></tr>
<tr><td>vllm_preemption_count</td><td>Preempted requests (by mode)</td><td>Tune utilization or scheduling budgets</td></tr>
<tr><td>vllm_swapped_kv_bytes</td><td>Bytes swapped (if SWAP)</td><td>Transfer vs recompute trade-offs</td></tr></p><p>Note: Names and availability vary by version and build; use the /metrics endpoint to enumerate available series.[^7]</p><p><h3>Memory Profiling Workflow for KV Growth</h3></p><p>We target three signals with increasing specificity:</p><p>- brk(): Lowest-overhead way to see heap expansion. Most allocators rarely shrink the break, so brk spikes strongly correlate with growth periods.
- mmap(): Allocators may use mmap for larger or isolated allocations; tracing helps identify arenas or large, persistent mappings.
- Page faults: While allocations commit virtual memory, writes to those pages cause minor faults; tracing them reveals the code paths that actually populate physical memory.</p><p>Collect stacks for each with perf and visualize as memory flame graphs. The workflow is standardized: record events with stacks, collapse to folded stacks, and render flame graphs using the memory color palette.[^3]</p><p><h3>Interpretation: From Syscall Patterns to Cache Policies</h3></p><p>Under memory pressure, the scheduler may preempt sequences. In vLLM V1, the default preemption mode is RECOMPUTE: the engine drops KV for the victim sequences and recomputes the cache when those sequences resume. SWAP is also available but generally slower due to transfer costs, except in cases such as beam search where recompute is inapplicable.[^5][^8] Observing a rise in preemption counts alongside limited free blocks indicates that KV budgets are too tight; the remedy is to raise gpu_memory_utilization, reduce concurrency (max_num_seqs), or increase parallelism degrees to free device memory for KV.</p><p>---</p><p><h2>Deep Source Dive: BlockManager and Eviction Mechanics</h2></p><p>Two types of allocation churn dominate KV behavior:</p><p>1) Prefill expansion: New prompts allocate blocks until their token length is cached.
2) Decode growth: Each step appends a small number of tokens; blocks grow incrementally and occasionally require new allocations.</p><p>BlockManager2 (or equivalent block-space manager) maintains per-sequence block tables, tracks referenced and allocated blocks, and enforces copy-on-write semantics for shared prefixes. Under pressure, it evicts blocks using a policy that balances fairness and capacity. vLLM’s scheduler cooperates with the cache engine by chunking prefill and prioritizing decode, limiting prefill’s ability to starve decode and keeping inter-token latency smooth.[^1][^8]</p><p>Table 5 summarizes preemption and eviction modes and their implications.</p><p>Table 5. Preemption and eviction modes</p><p><tr><td>Mode</td><td>Mechanism</td><td>Strengths</td><td>Costs / Trade-offs</td></tr>
<tr><td>--------------</td><td>-----------------------------------------------</td><td>----------------------------------------------</td><td>-------------------------------------------------------------</td></tr>
<tr><td>RECOMPUTE</td><td>Drop KV; recompute on resume</td><td>Low overhead; scalable</td><td>CPU/GPU time for recompute; potential TTFT/TPOT penalties</td></tr>
<tr><td>SWAP</td><td>Offload KV to CPU swap space</td><td>Preserves KV; avoids recompute</td><td>Transfer overhead often dominates; slower than recompute</td></tr>
<tr><td>Evict blocks</td><td>Reclaim least valuable blocks</td><td>Protects throughput under pressure</td><td>May increase fragmentation if policies mis-tuned</td></tr>
<tr><td>Chunked prefill</td><td>Split prefill into chunks; schedule decode first</td><td>Improves ITL; overlaps memory-bound decode and compute-bound prefill</td><td>Requires careful budget tuning (max_num_batched_tokens)</td></tr></p><p><h3>Code-Level Line-By-Line Outline</h3></p><p>A code-level trace through a request’s life reveals three hotspots:</p><p>- SchedulerOutput creation: The scheduler decides which sequences to run, how many prefill tokens to admit, and whether to preempt. Budgets for max_num_batched_tokens bound prefill; decode is prioritized to protect ITL.[^5][^8]
- Block allocation paths: BlockManager allocates or references blocks for the scheduled tokens. With prefix caching enabled, identical prefixes reference shared blocks until divergence, avoiding redundant allocation and computation.[^4][^8]
- Attention kernel preparation: The model runner composes inputs using block-based KV addressing and invokes attention kernels. Efficient block addressing reduces overhead in the hot path.[^1]</p><p>These hotspots are where CPU overhead manifests—Python control structures and object orchestration around scheduling, batch preparation, and kernel launch. Multi-step scheduling amortizes this overhead across several steps, which, combined with asynchronous output processing, significantly reduces GPU idle time.[^2]</p><p>---</p><p><h2>Performance Counters and CPU Profiling Correlations</h2></p><p>When GPUs are fast and models are optimized, CPUs become the bottleneck. The v0.6.0 analysis found large time shares consumed by the API server and scheduling, leaving the GPU underutilized on some workloads.[^2] On-CPU flame graphs derived from Linux perf samples reveal where those cycles go: often in scheduler loops, object manipulations, and data preparation. Off-CPU analysis shows synchronous output processing and blocking transfers that stall the pipeline.</p><p>Table 6 lists perf events that typically correlate with KV behavior.</p><p>Table 6. Key perf events and diagnostic targets</p><p><tr><td>Event</td><td>Why it matters for KV cache</td></tr>
<tr><td>-------------------------------</td><td>----------------------------------------------------------------------------------------------</td></tr>
<tr><td>cycles, instructions, IPC</td><td>Baseline CPU saturation and efficiency; validate multi-step scheduling savings</td></tr>
<tr><td>stalled-cycles-frontend/backend</td><td>Decode often memory-bound; backend stalls can indicate GPU headroom or CPU->GPU orchestration</td></tr>
<tr><td>L1-dcache-loads/misses</td><td>Hot loops in scheduling/prepare often show high loads; misses point to code/data layout issues</td></tr>
<tr><td>LLC-loads/misses</td><td>Larger working sets (e.g., block tables) may stress cache</td></tr>
<tr><td>page-faults</td><td>Correlates with memory growth; complements brk/mmap tracing</td></tr>
<tr><td>context-switches</td><td>High switch rates indicate contention or excessive preemption</td></tr>
<tr><td>task-clock</td><td>Time spent on-CPU; compare with GPU time shares from internal counters</td></tr></p><p><h3>Flame Graph Workflow</h3></p><p>A reproducible workflow:</p><p>- Record: perf record -F 99 -p <engine_pid> -g -- sleep 60
- Fold stacks: perf script | stackcollapse-perf.pl > out.folded
- Render: flamegraph.pl --title="vLLM on-CPU (99 Hz)" out.folded > cpu.svg</p><p>Repeat the process off-CPU (e.g., perf record -e sched:sched_switch -ag -- sleep 60) to visualize blocking paths. Filtering folded stacks by scheduler or output processing symbols isolates bottlenecks. Interpret widest boxes first—those are the hot code paths that most benefit from optimization or algorithmic changes.[^6][^11][^10]</p><p>---</p><p><h2>Memory Dump and GC Analysis: What to Look For</h2></p><p>Python memory dumps and garbage collection logs help differentiate transient churn from persistent bloat:</p><p>- KV cache lives in device memory, but Python-side structures—block tables, sequence metadata, scheduler queues—live on the heap. Look for growth trends in these objects across workload phases.
- Fragmentation signals include many small free blocks alongside allocation failures; block_size tuning and chunked prefill settings can mitigate this.
- Short-lived Python objects associated with request orchestration dominate allocation rates. vLLM’s object cache reduces repeated allocations and deallocations, easing GC pressure and improving end-to-end throughput.[^2]</p><p>Table 7 provides a practical checklist.</p><p>Table 7. Memory dump checklist for vLLM services</p><p><tr><td>Category</td><td>What to check</td><td>Tool / Signal</td></tr>
<tr><td>-------------------------</td><td>----------------------------------------------------</td><td>-------------------------------------------------------</td></tr>
<tr><td>Block tables</td><td>Growth rate vs. number of sequences</td><td>Heap snapshots; GC logs</td></tr>
<tr><td>Prefix sharing</td><td>Shared block references under load</td><td>Cache engine counters</td></tr>
<tr><td>Free lists</td><td>Many tiny free blocks; fragmentation ratio rising</td><td>vLLM metrics; block manager internals</td></tr>
<tr><td>Scheduler queues</td><td>Persistent queue growth under overload</td><td>Object counts; perf off-CPU</td></tr>
<tr><td>Object churn</td><td>High alloc/free rates in orchestration layers</td><td>Python tracers; object cache hit rate</td></tr></p><p>---</p><p><h2>Actionable Tuning Guidance</h2></p><p>Tuning the KV cache is the highest-leverage way to reduce preemption and keep decoders fed. Combine Cache and Scheduler settings with parallelism to maximize free blocks and reduce churn.</p><p>Table 8. KV tuning cheat sheet</p><p><tr><td>Symptom</td><td>Likely cause</td><td>Knob(s)</td><td>Expected effect</td></tr>
<tr><td>-------------------------------------------</td><td>---------------------------------------</td><td>------------------------------------------</td><td>------------------------------------------------------</td></tr>
<tr><td>Rising preemption counts</td><td>KV budget too tight</td><td>gpu_memory_utilization ↑</td><td>More free blocks; fewer preemptions</td></tr>
<tr><td>High TTFT at low QPS with multi-step</td><td>Scheduling stuck in long prefill runs</td><td>num-scheduler-steps ↓</td><td>Lower TTFT; may reduce throughput at high load</td></tr>
<tr><td>Decode ITL jitter</td><td>Prefill hogging budget</td><td>enable_chunked_prefill (on), max_num_batched_tokens ↓</td><td>Decode prioritized; smoother ITL</td></tr>
<tr><td>Frequent small allocations and churn</td><td>Block size too small</td><td>block_size ↑</td><td>Less metadata, fewer allocation calls</td></tr>
<tr><td>OOM during prefill</td><td>Insufficient free memory</td><td>max_num_seqs ↓; tensor/pipeline parallel ↑</td><td>Fewer concurrent sequences; more KV headroom</td></tr>
<tr><td>High KV fragmentation ratio</td><td>Block size mismatch to workload</td><td>block_size tuning; prefix caching on</td><td>Better block reuse; reduced external fragmentation</td></tr>
<tr><td>Transfer-dominated stalls</td><td>SWAP overuse</td><td>preemption_mode=RECOMPUTE</td><td>Avoid slow offload; recompute instead</td></tr></p><p><h3>Observability-Driven Tuning Loop</h3></p><p>Establish baselines with perf stat and vLLM’s /metrics. Then iterate:</p><p>1) Raise gpu_memory_utilization until free blocks remain stable under peak concurrency.  
2) Adjust max_num_batched_tokens and enable_chunked_prefill to protect ITL.  
3) If preemption persists, reduce concurrency (max_num_seqs) or increase parallelism degrees to carve out KV space.  
4) Validate changes with perf stat counters and flame graphs to confirm reduced CPU contention and smoother GPU utilization.[^5][^7][^10]</p><p>---</p><p><h2>Appendices</h2></p><p><h3>Glossary of KV-Cache-Related Terms</h3></p><p>- Block: A fixed-size segment of device memory used to store KV tokens for one or more sequences.  
- Page: Synonymous with block in the PagedAttention context; not to be confused with OS pages.  
- Prefill: The phase that processes prompt tokens and populates the KV cache.  
- Decode: The autoregressive phase that appends one or a few tokens per step and reads KV.  
- Preemption: Evicting or offloading a sequence’s KV to make room for others under pressure.  
- RECOMPUTE vs. SWAP: RECOMPUTE drops KV and recomputes later; SWAP offloads KV to CPU memory.  
- ITL vs. TPOT vs. TTFT: Inter-token latency (time between output tokens), time per output token, and time to first token.</p><p><h3>Reproducing the Tracing Workflows</h3></p><p>- CPU profiling: perf record -F 99 -p <PID> -g -- sleep 60; perf report; generate CPU flame graphs.  
- Syscall tracing: perf trace -p <PID>; or perf record -e syscalls:* -ag; then filter by brk/mmap.  
- Memory growth flame graphs: perf record -e syscalls:sys_enter_brk -ag; perf record -e page-faults -ag; collapse stacks and render with --color=mem.  
- vLLM metrics: Scrape /metrics; graph preemption counts and KV block usage to correlate with perf events and system load.[^10][^3][^7]</p><p><h3>Benchmark Notes and Reproducibility</h3></p><p>The v0.6.0 performance analysis used ShareGPT and synthetic prefill-/decode-heavy datasets and reported TTFT/TPOT and throughput for Llama 3 8B/70B on A100/H100. Key optimizations included process separation, multi-step scheduling, asynchronous output processing, and an object cache, which cumulatively reduced CPU overhead and improved GPU utilization.[^2] Reproduce with similar workloads and consistent QPS to isolate CPU vs. GPU bottlenecks.</p><p>---</p><p><h2>Information Gaps</h2></p><p>- Exact code paths and class names in the latest mainline can differ from v0.4.0-era descriptions; verification against the current repository is recommended.  
- Concrete strace/perf traces from representative vLLM runs are not included here; readers should capture and analyze their own workloads.  
- Developer-facing metrics may vary by version; confirm available series on the /metrics endpoint.  
- Precise preemption thresholds and heuristics can vary by release; use perf and metrics to characterize behavior in situ.</p><p>---</p><p><h2>References</h2></p><p>[^1]: Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., & Stoica, I. Efficient Memory Management for Large Language Model Serving with PagedAttention. arXiv:2309.06180. https://arxiv.org/pdf/2309.06180</p><p>[^2]: vLLM v0.6.0: 2.7x Throughput Improvement and 5x Latency Reduction. https://blog.vllm.ai/2024/09/05/perf-update.html</p><p>[^3]: Memory Leak (and Growth) Flame Graphs - Brendan Gregg. https://www.brendangregg.com/FlameGraphs/memoryflamegraphs.html</p><p>[^4]: Deep Dive into vLLM’s Architecture and Implementation (OpenAI-compatible). https://zerohertz.github.io/vllm-openai-1/</p><p>[^5]: Optimization and Tuning - vLLM. https://docs.vllm.ai/en/latest/configuration/optimization.html</p><p>[^6]: CPU Flame Graphs - Brendan Gregg. https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html</p><p>[^7]: vLLM Metrics and Observability (v0.9.0.1 docs). https://docs.vllm.ai/en/v0.9.0.1/design/v1/metrics.html</p><p>[^8]: Explaining the Source Code Behind the vLLM Fast Inference Engine. https://medium.com/@crclq2018/explaining-the-source-code-behind-the-vllm-fast-inference-engine-91429f54d1f7</p><p>[^9]: Getting Started with Flamegraphs - RHEL. https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-flamegraphs_monitoring-and-managing-system-status-and-performance</p><p>[^10]: Linux perf Examples - Brendan Gregg. https://www.brendangregg.com/perf.html</p><p>[^11]: FlameGraph - Stack trace visualizer. https://github.com/brendangregg/FlameGraph</p><p>[^12]: vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention. https://blog.vllm.ai/2023/06/20/vllm.html</p><p>[^13]: vllm-project/vllm - GitHub. https://github.com/vllm-project/vllm</p><p>[^14]: OpenAI-Compatible Server (v0.9.0.1). https://docs.vllm.ai/en/v0.9.0.1/serving/openai_compatible_server.html</p>
            </div>

            <div style="margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--border);">
                <a href="/experiments.html" style="color: var(--accent); text-decoration: none;">← Back to all experiments</a>
            </div>
        </div>
    </section>


    
<footer>
  <div class="container">
    <div class="footer-content">
      <div class="footer-logo">
        <span class="logo-f">f</span><span class="logo-slash">/</span><span class="logo-f">f</span>
      </div>
      <p>Fridays with Faraday - Working with microcontrollers and embedded systems</p>
    </div>
  </div>
</footer>

    <script src="/js/main.js"></script>
</body>
</html>
