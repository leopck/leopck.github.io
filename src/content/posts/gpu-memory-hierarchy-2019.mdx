---
title: "GPU Memory Hierarchy: Understanding Registers, Shared Memory, and Global Memory"
author: "stanley-phoong"
description: "Deep dive into GPU memory architecture, latency characteristics, and optimization strategies for CUDA kernels to maximize memory bandwidth utilization."
publishDate: 2019-04-08
category: gpu-programming
tags: [gpu, cuda, memory, performance, optimization]
difficulty: advanced
readingTime: 21
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

GPU memory hierarchy is fundamentally different from CPU memory. Understanding the latency and bandwidth characteristics of each level is essential for writing high-performance CUDA kernels.

## GPU Memory Hierarchy Overview

NVIDIA GPUs feature a multi-level memory hierarchy:

<Benchmark
  title="GPU Memory Hierarchy (NVIDIA V100)"
  columns={["Memory Type", "Size", "Latency", "Bandwidth", "Scope"]}
  rows={[
    { values: ["Registers", "~64 KB/SM", "1 cycle", "~10 TB/s", "Thread"], highlight: true },
    { values: ["Shared Memory", "96 KB/SM", "~20 cycles", "~3 TB/s", "Block"], highlight: true },
    { values: ["L1 Cache", "128 KB/SM", "~30 cycles", "~1.5 TB/s", "SM"], highlight: false },
    { values: ["L2 Cache", "6 MB", "~200 cycles", "~2.5 TB/s", "GPU"], highlight: false },
    { values: ["Global Memory", "16-32 GB", "~400 cycles", "~900 GB/s", "GPU"], highlight: false },
  ]}
/>

## Register Memory

Registers are the fastest memory, private to each thread:

```cuda
__global__ void register_example(float *input, float *output, int n) {
    // Variables in registers
    float a = input[threadIdx.x];
    float b = input[threadIdx.x + blockDim.x];
    float c = a * b;
    
    // Register pressure: too many registers causes occupancy issues
    float temp1, temp2, temp3;  // Each uses registers
    // ...
    
    output[threadIdx.x] = c;
}
```

Register usage affects occupancy:

<PerfChart
  title="Occupancy vs Register Usage per Thread"
  type="line"
  data={{
    labels: ["16", "32", "48", "64", "80", "96"],
    datasets: [{
      label: "Occupancy (%)",
      data: [100, 100, 75, 50, 33, 25],
      borderColor: "#3b82f6",
    }]
  }}
/>

<Callout type="tip" title="Register Optimization">
  Minimize register usage to maximize occupancy. Use shared memory for data shared across threads, not registers.
</Callout>

## Shared Memory

Shared memory is fast, on-chip memory shared by all threads in a block:

```cuda
__global__ void shared_memory_example(float *input, float *output, int n) {
    // Declare shared memory
    __shared__ float s_data[256];
    
    // Load from global to shared memory
    int tid = threadIdx.x;
    if (tid < 256) {
        s_data[tid] = input[blockIdx.x * 256 + tid];
    }
    __syncthreads();  // Synchronize threads
    
    // Process data from shared memory
    float result = s_data[tid] * 2.0f;
    
    // Write back to global memory
    if (tid < 256) {
        output[blockIdx.x * 256 + tid] = result;
    }
}
```

### Bank Conflicts

Shared memory is organized into 32 banks. Bank conflicts reduce bandwidth:

```cuda
// No bank conflicts: stride 1
__shared__ float data[1024];
float value = data[threadIdx.x];  // Each thread accesses different bank

// Bank conflicts: stride 32
float value = data[threadIdx.x * 32];  // All threads access same bank
```

<Benchmark
  title="Shared Memory Bandwidth: Bank Conflicts"
  columns={["Access Pattern", "Bandwidth", "Efficiency"]}
  rows={[
    { values: ["Stride 1 (no conflict)", "2.8 TB/s", "93%"], highlight: true },
    { values: ["Stride 2", "2.6 TB/s", "87%"], highlight: false },
    { values: ["Stride 4", "2.1 TB/s", "70%"], highlight: false },
    { values: ["Stride 32 (conflict)", "0.35 TB/s", "12%"], highlight: false },
  ]}
/>

## Global Memory

Global memory (DRAM) is slow but large. Access patterns matter:

```cuda
// Coalesced access: optimal
__global__ void coalesced_access(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float value = data[idx];  // Coalesced: 128 bytes per transaction
        // Process...
    }
}

// Non-coalesced access: poor
__global__ void non_coalesced_access(float *data, int n) {
    int idx = threadIdx.x * n + blockIdx.x;  // Strided access
    if (idx < n * blockDim.x) {
        float value = data[idx];  // Non-coalesced: many transactions
    }
}
```

### Memory Coalescing

Coalescing combines memory accesses from multiple threads:

<Benchmark
  title="Global Memory Bandwidth: Coalescing"
  columns={["Access Pattern", "Bandwidth", "Transactions"]}
  rows={[
    { values: ["Coalesced (128B)", "900 GB/s", "1 per warp"], highlight: true },
    { values: ["Strided 2", "450 GB/s", "2 per warp"], highlight: false },
    { values: ["Strided 4", "225 GB/s", "4 per warp"], highlight: false },
    { values: ["Random", "45 GB/s", "32 per warp"], highlight: false },
  ]}
/>

## Memory Access Patterns

### Pattern 1: Sequential Coalesced

```cuda
__global__ void pattern1_coalesced(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        output[idx] = input[idx] * 2.0f;  // Perfect coalescing
    }
}
```

**Bandwidth**: ~900 GB/s (theoretical maximum)

### Pattern 2: Strided Access

```cuda
__global__ void pattern2_strided(float *input, float *output, int n, int stride) {
    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * stride;
    if (idx < n) {
        output[idx] = input[idx] * 2.0f;  // Strided: reduced coalescing
    }
}
```

**Bandwidth**: Degrades with stride (450 GB/s at stride 2)

### Pattern 3: Shared Memory Tiling

```cuda
__global__ void pattern3_tiled(float *input, float *output, int n) {
    __shared__ float tile[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // Load tile from global memory (coalesced)
    if (idx < n) {
        tile[tid] = input[idx];
    }
    __syncthreads();
    
    // Process from shared memory (fast)
    float result = tile[tid] * 2.0f;
    
    // Write back (coalesced)
    if (idx < n) {
        output[idx] = result;
    }
}
```

**Bandwidth**: ~900 GB/s (same as pattern 1, but enables reuse)

## Texture Memory

Texture memory provides cached, read-only access:

```cuda
texture<float, 1, cudaReadModeElementType> tex_ref;

__global__ void texture_example(float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Texture fetch with caching
        float value = tex1Dfetch(tex_ref, idx);
        output[idx] = value * 2.0f;
    }
}

// Bind texture
cudaBindTexture(0, tex_ref, input_data, n * sizeof(float));
```

Benefits:
- **Automatic caching**: 2D spatial locality
- **Read-only**: No write conflicts
- **Good for random access**: Better than global memory

## Constant Memory

Constant memory is cached and read-only:

```cuda
__constant__ float constants[256];

__global__ void constant_example(float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Broadcast to all threads in warp
        float c = constants[0];
        output[idx] = c * idx;
    }
}
```

**Bandwidth**: Extremely high when all threads read same value (broadcast)

## Memory Optimization Example: Matrix Transpose

Naive transpose:

```cuda
__global__ void transpose_naive(float *input, float *output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x < width && y < height) {
        output[y * width + x] = input[x * height + y];  // Non-coalesced write
    }
}
```

Optimized with shared memory:

```cuda
__global__ void transpose_optimized(float *input, float *output, int width, int height) {
    __shared__ float tile[16][17];  // +1 to avoid bank conflicts
    
    int x = blockIdx.x * 16 + threadIdx.x;
    int y = blockIdx.y * 16 + threadIdx.y;
    
    // Load tile (coalesced read)
    if (x < width && y < height) {
        tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    }
    __syncthreads();
    
    // Transpose and write (coalesced write)
    x = blockIdx.y * 16 + threadIdx.x;
    y = blockIdx.x * 16 + threadIdx.y;
    if (x < height && y < width) {
        output[y * height + x] = tile[threadIdx.x][threadIdx.y];
    }
}
```

**Speedup**: 3.2x improvement

## Memory Bandwidth Measurement

```cuda
#include <cuda_runtime.h>
#include <cuda_profiler_api.h>

void measure_bandwidth(float *d_data, size_t size, int iterations) {
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    
    cudaEventRecord(start);
    for (int i = 0; i < iterations; i++) {
        // Memory operation
        cudaMemcpy(d_data, d_data, size, cudaMemcpyDeviceToDevice);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    
    double bandwidth = (size * iterations * 2) / (ms / 1000.0) / 1e9;  // GB/s
    printf("Bandwidth: %.2f GB/s\n", bandwidth);
}
```

## Conclusion

GPU memory optimization requires understanding the hierarchy:

1. **Registers**: Fastest, but limited - affects occupancy
2. **Shared Memory**: Fast, shared within block - watch for bank conflicts
3. **Global Memory**: Slow but large - coalescing is critical
4. **Texture/Constant**: Cached, good for read-only data

Key optimization strategies:
- **Coalesce global memory** accesses
- **Use shared memory** for data reuse
- **Avoid bank conflicts** in shared memory
- **Minimize register usage** for higher occupancy
- **Tile algorithms** to fit in shared memory

Master the memory hierarchy to unlock GPU performance.
