---
title: "CUDA Graphs for LLM Inference: Killing Startup Latency and Python Overhead"
author: "stanley-phoong"
description: "A systems-focused guide to CUDA Graphs for inference: what they actually capture, how much Python and launch overhead they remove, and when graphs fail to help because shapes or control flow keep changing."
publishDate: 2020-07-07
category: gpu-programming
tags: [cuda, graphs, inference, launch-overhead, optimization, performance]
difficulty: advanced
readingTime: 19
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

On modern GPUs, single-token LLM inference can become **launch- and framework-bound** instead of compute-bound: Python, dynamic shape checks, and dozens of tiny kernels inflate p99 latency.

CUDA Graphs let you capture a **static execution trace** once and replay it with almost zero launch overhead. This post focuses on when graphs help, how to wire them into an inference loop, and when they don’t.

## What CUDA Graphs actually capture

A graph records a DAG of:
- kernel launches (grid/block + params)
- memcpy operations
- dependencies between them

On replay:
- the CPU issues ONE graph launch instead of many
- driver avoids most per-launch overhead and some scheduling work

<Benchmark
  title="Where CUDA Graphs help"
  columns={["Workload", "Graph-friendly?", "Why / why not?"]}
  rows={[
    { values: ["Fixed-shape decode step", "Yes", "Same kernels, same shapes each token"], highlight: true },
    { values: ["Variable batch / seq len per token", "Partial", "Need padding or bucketing"], highlight: false },
    { values: ["Control-flow heavy kernels", "No", "Graph needs static structure"], highlight: false },
  ]}
/>

## Baseline: many launches per token

```python
def decode_step_naive(model, tokens, kv_cache):
    # Python + dispatcher per op
    hidden = model.embed(tokens)
    for layer in model.layers:
        hidden, kv_cache[layer] = layer(hidden, kv_cache[layer])
    logits = model.lm_head(hidden[:, -1, :])
    return logits
```

With 30–60 small kernels per step, launch overhead can be a few ms on its own.

## Capturing a graph for a fixed batch/sequence

```python
import torch

def capture_graph(model, batch_size, max_seq, device="cuda"):
    # Static input buffers
    static_tokens = torch.zeros(batch_size, max_seq, dtype=torch.long, device=device)
    static_logits = None

    stream = torch.cuda.Stream()
    g = torch.cuda.CUDAGraph()

    # Warm-up to allocate all buffers
    with torch.cuda.stream(stream):
        static_logits = model(static_tokens)
    torch.cuda.synchronize()

    # Capture
    with torch.cuda.graph(g):
        static_logits = model(static_tokens)

    return g, static_tokens, static_logits
```

Replay loop:

```python
def run_step_with_graph(g, static_tokens, static_logits, tokens):
    # Copy tokens into static buffer (no realloc)
    static_tokens[: tokens.shape[0], : tokens.shape[1]].copy_(tokens)
    g.replay()
    # static_logits now holds result
    return static_logits[: tokens.shape[0]]
```

## Performance impact (illustrative)

<Benchmark
  title="Per-token latency: before vs after graphs"
  columns={["Setup", "Batch", "ms/token", "Speedup"]}
  rows={[
    { values: ["Eager, Python", "1", "15.2", "1.0x"], highlight: false },
    { values: ["CUDA Graph", "1", "9.1", "1.67x"], highlight: true },
    { values: ["Eager, Python", "8", "7.8", "1.0x"], highlight: false },
    { values: ["CUDA Graph", "8", "5.0", "1.56x"], highlight: true },
  ]}
/>

<PerfChart
  title="Launch overhead fraction vs batch size"
  type="line"
  data={{
    labels: ["1", "2", "4", "8", "16"],
    datasets: [{
      label: "Launch + Python fraction of step time",
      data: [0.45, 0.32, 0.22, 0.16, 0.12],
      borderColor: "#ef4444",
    }]
  }}
/>

## When graphs don’t help (or hurt)

- **Changing shapes every step** (varying batch/sequence) → you must either:
  - pad to fixed shapes, or
  - maintain multiple graphs per bucket
- **Dynamic control flow** inside kernels → not capturable as-is
- **Frequent model changes** (finetuning, LoRA swapping) → frequent recapture costs

<Callout type="warning" title="Graphs and KV cache">
  Graphs assume static buffer layout. If your KV allocator moves buffers around or changes shapes per step, you must stabilize it (e.g., with a pool or paged allocator) before graphs pay off.
</Callout>

## Practical guidelines

- Start with **fixed batch/sequence** for hot path (e.g., batch=1 streaming)
- Capture separate graphs for a few **common shapes** (buckets)
- Use graphs for **decode loop**, not just prefill
- Couple graphs with **KV cache pooling** so addresses stay stable

## Conclusion

CUDA Graphs don’t make the model itself faster — they remove the tax you pay around it. In low-latency LLM serving, that tax is often the difference between “feels instant” and “feels laggy.”

Use graphs where control-flow and shapes are stable, and let eager execution handle the irregular tail of your workload.

