---
title: "GPU Tensor Core Optimization: Maximizing Performance with Mixed-Precision Operations"
author: "stanley-phoong"
description: "Advanced techniques for optimizing Tensor Core usage on NVIDIA GPUs, achieving peak performance with FP16 and INT8 matrix operations."
publishDate: 2019-12-03
category: gpu-programming
tags: [gpu, tensor-cores, cuda, optimization, performance, mixed-precision]
difficulty: expert
readingTime: 23
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Tensor Cores provide massive performance improvements for matrix operations. Optimizing Tensor Core usage is essential for achieving peak GPU performance.

## Tensor Core Architecture

Tensor Cores perform specialized matrix operations:

<Benchmark
  title="Tensor Core Specifications (NVIDIA V100)"
  columns={["Operation", "Precision", "Throughput", "Speedup"]}
  rows={[
    { values: ["FP32 GEMM", "FP32", "15.7 TFLOPS", "1.0x"], highlight: false },
    { values: ["Tensor Core", "FP16", "125 TFLOPS", "8x"], highlight: true },
    { values: ["Tensor Core", "INT8", "250 TOPS", "16x"], highlight: true },
    { values: ["Tensor Core", "INT4", "500 TOPS", "32x"], highlight: false },
  ]}
/>

## WMMA API Usage

Using WMMA (Warp Matrix Multiply Accumulate) API:

```cuda
#include <mma.h>

using namespace nvcuda;

__global__ void tensor_core_gemm(half *A, half *B, half *C, int M, int N, int K) {
    // Declare fragments
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, half> c_frag;
    
    // Initialize accumulator
    wmma::fill_fragment(c_frag, 0.0f);
    
    // Load matrix tiles
    wmma::load_matrix_sync(a_frag, A + threadIdx.y * 16, 16);
    wmma::load_matrix_sync(b_frag, B + threadIdx.x, 16);
    
    // Matrix multiply-accumulate
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    
    // Store result
    wmma::store_matrix_sync(C + threadIdx.y * 16 + threadIdx.x, c_frag, 16, wmma::mem_row_major);
}
```

## Performance Optimization

Tensor Core performance characteristics:

<PerfChart
  title="Tensor Core Performance vs Matrix Size"
  type="line"
  data={{
    labels: ["64x64", "128x128", "256x256", "512x512", "1024x1024"],
    datasets: [
      {
        label: "Tensor Core (TFLOPS)",
        data: [45, 95, 115, 122, 125],
        borderColor: "#10b981",
      },
      {
        label: "CUDA Core (TFLOPS)",
        data: [8, 12, 14, 15, 15.7],
        borderColor: "#3b82f6",
      }
    ]
  }}
/>

## Memory Alignment Requirements

Tensor Cores require specific memory alignment:

```cuda
__global__ void aligned_tensor_core_gemm(half *A, half *B, half *C, int M, int N, int K) {
    // Ensure 16-byte alignment for matrix A
    // Ensure 16-byte alignment for matrix B
    
    // Load with proper stride
    int stride_a = (M + 15) & ~15;  // Round up to multiple of 16
    int stride_b = (K + 15) & ~15;
    
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, half> c_frag;
    
    wmma::fill_fragment(c_frag, 0.0f);
    
    // Load with aligned stride
    wmma::load_matrix_sync(a_frag, A + blockIdx.y * 16 * stride_a + threadIdx.y * 16, stride_a);
    wmma::load_matrix_sync(b_frag, B + blockIdx.x * 16 + threadIdx.x * 16 * stride_b, stride_b);
    
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    
    wmma::store_matrix_sync(C + blockIdx.y * 16 * N + blockIdx.x * 16 + threadIdx.y * 16 + threadIdx.x, 
                            c_frag, N, wmma::mem_row_major);
}
```

<Callout type="tip" title="Memory Alignment">
  Tensor Cores require 16-byte alignment and matrix dimensions must be multiples of 16 for optimal performance.
</Callout>

## Mixed Precision Patterns

Optimal mixed precision usage:

```cuda
__global__ void mixed_precision_gemm(float *A_fp32, float *B_fp32, float *C_fp32, int M, int N, int K) {
    // Convert to FP16 for Tensor Cores
    __shared__ half A_fp16[16][16];
    __shared__ half B_fp16[16][16];
    
    // Load and convert
    if (threadIdx.x < 16 && threadIdx.y < 16) {
        A_fp16[threadIdx.y][threadIdx.x] = __float2half(A_fp32[blockIdx.y * 16 * K + threadIdx.y * K + threadIdx.x]);
        B_fp16[threadIdx.y][threadIdx.x] = __float2half(B_fp32[threadIdx.y * N + blockIdx.x * 16 + threadIdx.x]);
    }
    __syncthreads();
    
    // Tensor Core operation (FP16)
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;  // Accumulate in FP32
    
    wmma::fill_fragment(c_frag, 0.0f);
    wmma::load_matrix_sync(a_frag, (half*)A_fp16, 16);
    wmma::load_matrix_sync(b_frag, (half*)B_fp16, 16);
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    
    // Store FP32 result
    wmma::store_matrix_sync(C_fp32 + blockIdx.y * 16 * N + blockIdx.x * 16 + threadIdx.y * 16 + threadIdx.x,
                            c_frag, N, wmma::mem_row_major);
}
```

**Performance**: 8x speedup over FP32 CUDA cores

## INT8 Tensor Cores

Using INT8 for maximum throughput:

```cuda
__global__ void int8_tensor_core_gemm(int8_t *A, int8_t *B, int32_t *C, 
                                      float scale_A, float scale_B, float scale_C,
                                      int M, int N, int K) {
    wmma::fragment<wmma::matrix_a, 16, 16, 16, int8_t, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, int8_t, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, int32_t> c_frag;
    
    wmma::fill_fragment(c_frag, 0);
    
    // Load INT8 matrices
    wmma::load_matrix_sync(a_frag, A + threadIdx.y * 16, 16);
    wmma::load_matrix_sync(b_frag, B + threadIdx.x, 16);
    
    // INT8 matrix multiply (accumulates in INT32)
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    
    // Scale and convert to output format
    // Store with proper scaling
    wmma::store_matrix_sync(C + threadIdx.y * 16 + threadIdx.x, c_frag, 16, wmma::mem_row_major);
}
```

**Performance**: 16x speedup over FP32, 2x over FP16

<Benchmark
  title="Tensor Core Performance Comparison"
  columns={["Precision", "Throughput", "Speedup vs FP32", "Use Case"]}
  rows={[
    { values: ["FP32 (CUDA)", "15.7 TFLOPS", "1.0x", "Baseline"], highlight: false },
    { values: ["FP16 (Tensor)", "125 TFLOPS", "8.0x", "Training/Inference"], highlight: true },
    { values: ["INT8 (Tensor)", "250 TOPS", "16.0x", "Inference"], highlight: true },
  ]}
/>

## Optimization Strategies

1. **Align matrices**: 16-byte alignment, dimensions multiple of 16
2. **Use FP16**: 8x speedup with minimal quality loss
3. **Use INT8**: 16x speedup for inference
4. **Accumulate in FP32**: Better numerical stability
5. **Tile matrices**: Fit in shared memory

## Conclusion

Tensor Core optimization provides:

1. **Massive speedup**: 8-16x over FP32 CUDA cores
2. **Memory efficiency**: FP16/INT8 reduce memory bandwidth
3. **Power efficiency**: Lower power per operation
4. **Hardware acceleration**: Native support on modern GPUs

Key strategies:
- Use WMMA API for Tensor Core operations
- Ensure proper memory alignment
- Choose appropriate precision (FP16/INT8)
- Accumulate in FP32 for stability
- Optimize matrix dimensions (multiples of 16)

Master Tensor Cores to achieve peak GPU performance for matrix operations.
