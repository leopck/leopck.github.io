---
title: "Multi-GPU Performance: Data Parallel vs Model Parallel for Transformer Inference"
author: "stanley-phoong"
description: "A performance comparison of data parallel and model parallel strategies for transformer inference. We quantify communication costs (all-reduce, all-gather), memory savings, and when each strategy actually improves throughput."
publishDate: 2020-08-05
category: gpu-programming
tags: [multi-gpu, data-parallel, model-parallel, transformer, inference, allreduce, optimization]
difficulty: expert
readingTime: 21
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Throwing more GPUs at a transformer doesn’t automatically give you higher throughput. Whether **data parallel** or **model parallel** helps depends on:
- batch size
- sequence length
- interconnect bandwidth/latency
- memory headroom

This post models the main costs and shows when each strategy makes sense for *inference* (not training).

## Data parallel for inference: mostly useless unless you batch

Data parallel (DP) = each GPU has a full copy of the model, processes different batches.

For pure inference:
- there is **no gradient all-reduce**
- communication is limited to request routing and maybe periodic weight sync

If your batch size is already 1 per GPU (per request), DP only helps if:
- you have **many independent requests**
- your serving stack can **feed each GPU efficiently**

<Benchmark
  title="Data parallel inference scenarios"
  columns={["Scenario", "Batch", "DP helpful?", "Why?"]}
  rows={[
    { values: ["Single long-running request", "1", "No", "One GPU mostly busy, others idle"], highlight: false },
    { values: ["Many small requests", "1 per GPU", "Yes", "More aggregate capacity"], highlight: true },
    { values: ["Already batch=8 on 1 GPU", "8", "Sometimes", "If memory-bound, more GPUs help"], highlight: false },
  ]}
/>

## Model parallel: tensor / pipeline / sequence

For large models that **don’t fit** on one GPU, you split the model:
- **Tensor parallel**: split matrices across GPUs (all-reduce heavy)
- **Pipeline parallel**: split layers across GPUs (bubble / pipeline balance)
- **Sequence parallel**: split sequence dimension (all-gather heavy)

Each comes with a communication tax.

## Tensor parallel cost model

For linear layers of shape \([B, H] \times [H, 4H]\) (FFN), split H across \(N\) GPUs:

Each GPU holds a shard of weights and computes a partial output; you all-reduce to combine.

Let:
- \(C_{comp}\) = compute time per shard
- \(C_{comm}\) = all-reduce time (latency + bandwidth term)

Per step:
\[
T_{TP} \approx C_{comp} + C_{comm}
\]

If:
\[
C_{comp}/N \gg C_{comm}
\]
then you win (speedup close to N). If communication dominates, you lose.

<PerfChart
  title="Tensor parallel speedup vs interconnect"
  type="line"
  data={{
    labels: ["1", "2", "4", "8"],
    datasets: [
      { label: "NVLink (high BW)", data: [1.0, 1.8, 3.2, 5.5], borderColor: "#10b981" },
      { label: "PCIe only", data: [1.0, 1.5, 2.1, 2.4], borderColor: "#ef4444" },
    ]
  }}
/>

## Pipeline parallel cost model

Split layers into \(N\) stages:
- each stage processes part of the model
- you pass activations along the pipeline

Pipeline bubbles reduce efficiency for small numbers of microbatches.

Effective speedup:
\[
\text{Speedup} \approx \frac{N}{1 + \text{bubble\_fraction}}
\]

For inference with **batch=1**, bubble fraction is huge unless you accumulate multiple tokens in flight.

<Callout type="warning" title="Pipeline parallel is batch-hungry">
  For low-batch, pipeline parallel often hurts latency because bubbles dominate, and you still pay activation transfer costs.
</Callout>

## Sequence parallel cost model

Split sequence dimension across GPUs (e.g., attention heads see shard of sequence). Requires all-gather at some points.

Helps when:
- sequence length is very large
- single-GPU memory is the limiting factor

But all-gather latency can hurt single-token step time.

## Comparative summary (inference-focused)

<Benchmark
  title="Multi-GPU strategies for inference"
  columns={["Strategy", "When it helps", "Main cost", "Good for"]}
  rows={[
    { values: ["Data parallel", "Many independent requests", "None (if no sync)", "Throughput scaling"], highlight: true },
    { values: ["Tensor parallel", "Model too big, high-BW link", "All-reduces", "Large models with NVLink"], highlight: true },
    { values: ["Pipeline parallel", "High batch, long sequences", "Pipeline bubbles + activations", "Offline / batch inference"], highlight: false },
    { values: ["Sequence parallel", "Very long context", "All-gathers", "Long-context models"], highlight: false },
  ]}
/>

## Practical guidance

- If your model **fits on one GPU**:
  - scale **out** with data parallel for *throughput*
  - keep each GPU running as independently as possible
- If your model **does not fit**:
  - prefer **tensor parallel on NVLink-class** systems
  - combine with **paged KV cache** and efficient attention
- For low-latency single-request scenarios:
  - minimize cross-GPU traffic; a **single fast GPU** often beats many slow ones

## Conclusion

Multi-GPU inference is not a free lunch:
- data parallel helps when you have **concurrency**
- model parallel helps when you have **memory pressure** and **fast links**
- both can hurt p99 latency if communication is not carefully modeled

Before you shard the model, measure per-token compute and estimate communication time with realistic interconnect numbers — then choose the least-painful strategy for your latency/throughput budget.

