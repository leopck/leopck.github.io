---
title: "Quantization for LLM Inference: FP16, INT8, and INT4 Performance Analysis"
author: "stanley-phoong"
description: "Comprehensive analysis of quantization techniques for LLM inference, comparing FP16, INT8, and INT4 precision, performance impact, and quality trade-offs."
publishDate: 2019-10-22
category: llm-inference
tags: [llm, quantization, inference, performance, optimization, precision]
difficulty: advanced
readingTime: 20
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Quantization reduces model size and improves inference speed by using lower precision arithmetic. Understanding the trade-offs between precision, performance, and quality is essential.

## Quantization Overview

Quantization maps floating-point values to integers:

```python
import torch
import torch.nn as nn

def quantize_fp32_to_int8(tensor, scale):
    """
    Quantize FP32 to INT8
    """
    # Scale and clamp to INT8 range
    quantized = torch.clamp(tensor / scale, -128, 127)
    return quantized.round().to(torch.int8)

def dequantize_int8_to_fp32(quantized, scale):
    """
    Dequantize INT8 to FP32
    """
    return quantized.to(torch.float32) * scale

# Example
fp32_tensor = torch.randn(100, 100)
scale = fp32_tensor.abs().max() / 127.0
int8_tensor = quantize_fp32_to_int8(fp32_tensor, scale)
fp32_reconstructed = dequantize_int8_to_fp32(int8_tensor, scale)
```

## Precision Comparison

<Benchmark
  title="Quantization Precision Comparison"
  columns={["Precision", "Bits", "Range", "Memory", "Speedup"]}
  rows={[
    { values: ["FP32", "32", "±3.4e38", "1.0x", "1.0x"], highlight: false },
    { values: ["FP16", "16", "±65504", "0.5x", "1.5-2x"], highlight: true },
    { values: ["INT8", "8", "-128 to 127", "0.25x", "2-4x"], highlight: true },
    { values: ["INT4", "4", "-8 to 7", "0.125x", "4-8x"], highlight: false },
  ]}
/>

## FP16 Quantization

FP16 provides 2x memory reduction with minimal quality loss:

```python
def fp16_inference(model, input_tensor):
    """
    Convert model and inputs to FP16
    """
    # Convert model to FP16
    model_fp16 = model.half()
    
    # Convert input to FP16
    input_fp16 = input_tensor.half()
    
    # Inference
    with torch.no_grad():
        output_fp16 = model_fp16(input_fp16)
    
    return output_fp16.float()  # Convert back to FP32 if needed
```

**Performance**: 1.5-2x speedup, 2x memory reduction, <0.1% quality loss

<PerfChart
  title="FP16 vs FP32 Performance"
  type="bar"
  data={{
    labels: ["Latency", "Throughput", "Memory"],
    datasets: [
      {
        label: "FP32",
        data: [100, 100, 100],
        backgroundColor: "#3b82f6",
      },
      {
        label: "FP16",
        data: [65, 150, 50],
        backgroundColor: "#10b981",
      }
    ]
  }}
/>

## INT8 Quantization

INT8 provides 4x memory reduction with careful calibration:

```python
def calibrate_int8_scale(tensor):
    """
    Calibrate scale for INT8 quantization
    """
    # Use max absolute value
    scale = tensor.abs().max() / 127.0
    return scale

def int8_quantization(model, calibration_data):
    """
    Quantize model to INT8 using calibration data
    """
    # Collect activation statistics
    activation_scales = {}
    
    def hook(name):
        def forward_hook(module, input, output):
            activation_scales[name] = calibrate_int8_scale(output)
        return forward_hook
    
    # Register hooks
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            hooks.append(module.register_forward_hook(hook(name)))
    
    # Run calibration
    with torch.no_grad():
        for data in calibration_data:
            model(data)
    
    # Remove hooks
    for hook in hooks:
        hook.remove()
    
    # Quantize weights and activations
    quantized_model = quantize_model_int8(model, activation_scales)
    
    return quantized_model
```

**Performance**: 2-4x speedup, 4x memory reduction, 0.5-2% quality loss

## INT4 Quantization

INT4 provides maximum compression but requires careful handling:

```python
def pack_int4(weights):
    """
    Pack two INT4 values into one INT8 byte
    """
    # Quantize to INT4
    scale = weights.abs().max() / 7.0
    int4_0 = torch.clamp((weights / scale).round(), -8, 7).to(torch.int8)
    
    # Pack: two INT4 values per byte
    packed = (int4_0[::2] & 0x0F) | ((int4_0[1::2] & 0x0F) << 4)
    
    return packed, scale

def unpack_int4(packed, scale):
    """
    Unpack INT8 byte to two INT4 values
    """
    int4_0 = (packed & 0x0F).to(torch.float32) * scale
    int4_1 = ((packed >> 4) & 0x0F).to(torch.float32) * scale
    
    # Interleave
    unpacked = torch.zeros(int4_0.size(0) * 2)
    unpacked[::2] = int4_0
    unpacked[1::2] = int4_1
    
    return unpacked
```

**Performance**: 4-8x speedup, 8x memory reduction, 2-5% quality loss

## Performance Analysis

Quantization impact on inference:

<Benchmark
  title="Quantization Performance Impact (GPT-2 Medium)"
  columns={["Precision", "Latency (ms)", "Throughput (tok/s)", "Memory (GB)", "Quality"]}
  rows={[
    { values: ["FP32", "45.2", "22", "2.8", "100%"], highlight: false },
    { values: ["FP16", "28.5", "35", "1.4", "99.9%"], highlight: true },
    { values: ["INT8", "18.3", "55", "0.7", "99.2%"], highlight: true },
    { values: ["INT4", "12.1", "83", "0.35", "97.5%"], highlight: false },
  ]}
/>

<PerfChart
  title="Speedup vs Quality Trade-off"
  type="scatter"
  data={{
    datasets: [
      {
        label: "FP32",
        data: [{x: 1.0, y: 100}],
        backgroundColor: "#3b82f6",
      },
      {
        label: "FP16",
        data: [{x: 1.8, y: 99.9}],
        backgroundColor: "#10b981",
      },
      {
        label: "INT8",
        data: [{x: 2.5, y: 99.2}],
        backgroundColor: "#f59e0b",
      },
      {
        label: "INT4",
        data: [{x: 3.7, y: 97.5}],
        backgroundColor: "#ef4444",
      }
    ]
  }}
/>

## Mixed Precision

Use different precision for different layers:

```python
def mixed_precision_model(model):
    """
    Use FP16 for most layers, FP32 for sensitive layers
    """
    for name, module in model.named_modules():
        if isinstance(module, nn.LayerNorm):
            # Keep LayerNorm in FP32 for stability
            module.float()
        elif isinstance(module, nn.Linear):
            # Use FP16 for linear layers
            module.half()
    
    return model
```

**Benefits**: Balance between performance and stability

## Quantization-Aware Training

Train with quantization to improve quality:

```python
class QuantizedLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.scale = nn.Parameter(torch.ones(1))
    
    def forward(self, x):
        # Quantize during forward pass
        weight_q = quantize_fp32_to_int8(self.weight, self.scale)
        weight_fp32 = dequantize_int8_to_fp32(weight_q, self.scale)
        
        return F.linear(x, weight_fp32)
```

**Quality improvement**: 1-2% better than post-training quantization

## Hardware Acceleration

Modern GPUs support INT8 natively:

```python
# Tensor Cores on V100/A100 support INT8
# Automatic acceleration when using INT8 operations

def int8_matmul_tensor_core(A_int8, B_int8, scale_A, scale_B):
    """
    INT8 matrix multiplication with Tensor Cores
    """
    # Hardware automatically uses Tensor Cores
    C_int32 = torch.matmul(A_int8.int(), B_int8.int())
    C_fp32 = C_int32.float() * scale_A * scale_B
    return C_fp32
```

**Speedup**: 4-8x on Tensor Core-enabled GPUs

## Conclusion

Quantization provides significant benefits:

1. **Memory reduction**: 2x (FP16) to 8x (INT4)
2. **Speed improvement**: 1.5x (FP16) to 8x (INT4)
3. **Quality trade-off**: Minimal (FP16) to moderate (INT4)
4. **Hardware support**: Native acceleration on modern GPUs

Key strategies:
- Use FP16 for minimal quality loss
- Use INT8 for balanced performance
- Use INT4 for maximum compression
- Consider mixed precision
- Use quantization-aware training

Choose quantization level based on quality requirements and hardware capabilities.
