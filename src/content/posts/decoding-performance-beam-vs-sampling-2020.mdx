---
title: "Decoding Performance: Beam Search vs Sampling (Latency, Throughput, and Memory)"
author: "stanley-phoong"
description: "A performance-focused comparison of beam search and sampling for LLM inference. We quantify compute and memory costs per token, identify bottlenecks (top-k, softmax, KV cache duplication), and provide practical optimization guidelines."
publishDate: 2020-05-06
category: llm-inference
tags: [llm, decoding, beam-search, sampling, latency, throughput, optimization, performance]
difficulty: advanced
readingTime: 18
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

In production inference, “decoding strategy” is not just a quality knob — it’s a **systems cost multiplier**. Beam search can multiply your per-token compute and memory traffic, while sampling tends to keep the model on the fast path.

This post compares beam search vs sampling purely through the lens of **performance** (latency/throughput/memory), and gives concrete ways to keep decode overhead from dominating.

## What changes between strategies?

Both strategies still run the transformer forward pass. The difference is what you do with logits and how many hypotheses you keep.

<Benchmark
  title="Decode strategy impact surface"
  columns={["Strategy", "Hypotheses per step", "Extra ops on logits", "KV cache impact"]}
  rows={[
    { values: ["Sampling (top-k / top-p)", "1", "partial sort + RNG", "1× cache"], highlight: true },
    { values: ["Greedy", "1", "argmax", "1× cache"], highlight: false },
    { values: ["Beam search (B beams)", "B", "top-k + beam scoring", "≈B× cache (if duplicated)"], highlight: true },
  ]}
/>

## The two big costs: (1) beams, (2) logits post-processing

### Cost 1: B beams amplify compute

Naively, beam search runs the model forward **B times per step** (or one batched forward of size B).

If your single-token forward is \(T_{model}\), then:
\[
T_{beam} \approx B \cdot T_{model} + T_{select}
\]
while sampling is:
\[
T_{sample} \approx T_{model} + T_{select}
\]

### Cost 2: selection overhead grows with vocabulary

For vocab size \(V\), naive softmax is \(O(V)\). Top-k selection is roughly \(O(V)\) to \(O(V \log k)\) depending on implementation.

For modern \(V\sim 50k-250k\), **selection can be non-trivial** if the model forward is optimized (e.g., small model or fast GPU).

## Memory: KV cache is the silent killer for beams

If you duplicate the KV cache per beam, memory grows ~B×:

```python
def kv_cache_bytes(batch, layers, heads, head_dim, seq_len, dtype_bytes=2):
    # K + V
    return 2 * batch * layers * heads * seq_len * head_dim * dtype_bytes

# Beam search: batch == B beams
```

<Callout type="warning" title="Beam search can become memory-bound">
  For long contexts, beam search often hits memory limits (KV cache) before it hits compute limits. That forces smaller batch sizes and destroys throughput.
</Callout>

## A practical performance model

Let:
- \(B\) = number of beams
- \(S\) = generated tokens
- \(T_{1}\) = time per token for batch=1
- \(E(B)\) = batching efficiency (sublinear scaling)

Then:
\[
T_{total}^{beam} \approx S \cdot \frac{B \cdot T_1}{E(B)} + S \cdot T_{select}(B)
\]

Sampling:
\[
T_{total}^{sample} \approx S \cdot T_1 + S \cdot T_{select}(1)
\]

The key: even if you batch beams, **you still pay the KV cache bandwidth**, and you still pay selection overhead.

## Benchmark-style comparison (illustrative)

Assume a mid-size model on a GPU where single-token forward is ~12 ms at batch=1.

<Benchmark
  title="Decode strategy performance (example)"
  columns={["Strategy", "Beams", "p99 latency (ms/token)", "Throughput (tok/s)", "Peak memory"]}
  rows={[
    { values: ["Sampling (top-p)", "1", "12.5", "80", "1.0×"], highlight: true },
    { values: ["Beam search", "4", "18.0", "55", "3.2×"], highlight: false },
    { values: ["Beam search", "8", "26.0", "38", "6.1×"], highlight: false },
  ]}
/>

<PerfChart
  title="Latency vs beams (example)"
  type="line"
  data={{
    labels: ["1", "2", "4", "8", "16"],
    datasets: [{
      label: "ms/token",
      data: [12.5, 14.0, 18.0, 26.0, 45.0],
      borderColor: "#ef4444",
    }]
  }}
/>

## Optimization techniques (high-signal)

### 1) Prefer sampling for interactive paths

If you need low latency (chat), sampling almost always wins. Beam search is typically reserved for offline generation or strict quality constraints.

### 2) Use small beams (B=2..4) and early stop

Beam search often sees diminishing returns beyond 4 beams. Performance cost keeps climbing.

### 3) Don’t duplicate KV caches if you can avoid it

If you can represent beams as **shared prefixes** (common early in generation), you can share KV cache pages and only fork when needed.

### 4) Optimize logits post-processing

- Fuse bias + softmax + top-k where possible
- Use approximate top-k for large vocab
- Run top-k on GPU (avoid device↔host sync)

### 5) Batch beams (but watch memory)

Batching beams improves compute utilization but increases KV cache footprint and can push you into a memory-bound regime.

## Rule-of-thumb decision table

<Benchmark
  title="What to use when"
  columns={["Goal", "Recommended", "Why"]}
  rows={[
    { values: ["Lowest latency", "Sampling (top-p/top-k)", "Single hypothesis, smaller KV cache"], highlight: true },
    { values: ["Highest throughput", "Sampling + batching", "Best GPU utilization per memory"], highlight: true },
    { values: ["Quality constrained", "Small beam (2–4)", "Incremental quality gain vs cost"], highlight: false },
  ]}
/>

## Conclusion

Beam search is a **systems multiplier**: it amplifies KV cache, memory bandwidth, and decode overhead. Sampling keeps the inference path lean and predictable.

If you must do beam search, treat it like a performance feature: budget memory, batch carefully, and aggressively optimize selection and cache sharing.

