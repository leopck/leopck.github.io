---
title: "GPU Memory Bandwidth Optimization: Maximizing DRAM Throughput in CUDA Kernels"
author: "stanley-phoong"
description: "Advanced techniques for optimizing GPU memory bandwidth, analyzing access patterns, coalescing strategies, and achieving peak DRAM throughput."
publishDate: 2020-02-25
category: gpu-programming
tags: [gpu, cuda, memory-bandwidth, optimization, performance, dram]
difficulty: expert
readingTime: 22
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

GPU memory bandwidth is often the bottleneck. Optimizing memory access patterns is essential for achieving peak performance.

## Memory Bandwidth Fundamentals

GPU memory hierarchy bandwidth:

<Benchmark
  title="GPU Memory Bandwidth (NVIDIA V100)"
  columns={["Memory Type", "Bandwidth", "Latency", "Use Case"]}
  rows={[
    { values: ["Registers", "~10 TB/s", "1 cycle", "Fastest"], highlight: true },
    { values: ["Shared Memory", "~3 TB/s", "20 cycles", "On-chip"], highlight: true },
    { values: ["L2 Cache", "~2.5 TB/s", "200 cycles", "Cached"], highlight: false },
    { values: ["Global Memory", "~900 GB/s", "400 cycles", "DRAM"], highlight: false },
  ]}
/>

## Coalescing Analysis

Memory coalescing is critical for bandwidth:

```cuda
// Perfect coalescing: 128-byte transaction per warp
__global__ void coalesced_copy(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        output[idx] = input[idx];  // Sequential access
    }
}

// Non-coalesced: multiple transactions
__global__ void strided_copy(float *input, float *output, int n, int stride) {
    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * stride;
    if (idx < n) {
        output[idx] = input[idx];  // Strided access
    }
}
```

<Benchmark
  title="Memory Bandwidth: Coalescing Impact"
  columns={["Access Pattern", "Bandwidth", "Transactions", "Efficiency"]}
  rows={[
    { values: ["Coalesced", "900 GB/s", "1 per warp", "100%"], highlight: true },
    { values: ["Stride 2", "450 GB/s", "2 per warp", "50%"], highlight: false },
    { values: ["Stride 4", "225 GB/s", "4 per warp", "25%"], highlight: false },
    { values: ["Random", "45 GB/s", "32 per warp", "5%"], highlight: false },
  ]}
/>

<PerfChart
  title="Bandwidth vs Stride"
  type="bar"
  data={{
    labels: ["1", "2", "4", "8", "16", "32"],
    datasets: [{
      label: "Bandwidth (GB/s)",
      data: [900, 450, 225, 112, 56, 28],
      backgroundColor: ["#10b981", "#3b82f6", "#f59e0b", "#ef4444", "#ef4444", "#6b7280"],
    }]
  }}
/>

## Memory Access Patterns

Optimize access patterns:

```cuda
// Pattern 1: Sequential (optimal)
__global__ void pattern1(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        data[idx] = data[idx] * 2.0f;  // Perfect coalescing
    }
}

// Pattern 2: Tiled with shared memory
__global__ void pattern2(float *input, float *output, int n) {
    __shared__ float tile[256];
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // Coalesced load
    tile[tid] = (idx < n) ? input[idx] : 0.0f;
    __syncthreads();
    
    // Process from shared memory
    float result = tile[tid] * 2.0f;
    __syncthreads();
    
    // Coalesced store
    if (idx < n) {
        output[idx] = result;
    }
}
```

## Prefetching Strategies

Prefetch data to hide latency:

```cuda
__global__ void prefetch_example(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Prefetch next iteration
    if (idx + blockDim.x < n) {
        __builtin_prefetch(&data[idx + blockDim.x], 1, 3);
    }
    
    // Process current
    data[idx] = data[idx] * 2.0f;
}
```

## Memory Alignment

Align data for optimal access:

```cuda
// Align to 128 bytes (coalescing boundary)
__global__ void aligned_access(float *data, int n) {
    // Ensure data is 128-byte aligned
    // Use cudaMalloc or aligned_alloc
    
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        data[idx] = data[idx] * 2.0f;
    }
}

// Allocate aligned memory
void allocate_aligned_memory(float **data, size_t size) {
    cudaMalloc((void**)data, size);
    // Or use aligned allocation
    *data = (float*)aligned_alloc(128, size);
}
```

## Bandwidth Measurement

Measure achieved bandwidth:

```cuda
#include <cuda_runtime.h>

void measure_bandwidth(float *d_data, size_t size, int iterations) {
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    
    cudaEventRecord(start);
    for (int i = 0; i < iterations; i++) {
        // Memory operation
        cudaMemcpy(d_data, d_data, size, cudaMemcpyDeviceToDevice);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    
    double bandwidth = (size * iterations * 2) / (ms / 1000.0) / 1e9;
    printf("Bandwidth: %.2f GB/s (%.1f%% of peak)\n", 
           bandwidth, bandwidth / 900.0 * 100.0);
}
```

## Optimization Example: Matrix Transpose

Optimize memory access:

```cuda
// Naive: non-coalesced writes
__global__ void transpose_naive(float *input, float *output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x < width && y < height) {
        output[y * width + x] = input[x * height + y];  // Strided write
    }
}

// Optimized: coalesced both directions
__global__ void transpose_optimized(float *input, float *output, int width, int height) {
    __shared__ float tile[16][17];  // Padding avoids bank conflicts
    
    int x = blockIdx.x * 16 + threadIdx.x;
    int y = blockIdx.y * 16 + threadIdx.y;
    
    // Coalesced read
    if (x < width && y < height) {
        tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    }
    __syncthreads();
    
    // Coalesced write (transposed)
    x = blockIdx.y * 16 + threadIdx.x;
    y = blockIdx.x * 16 + threadIdx.y;
    if (x < height && y < width) {
        output[y * height + x] = tile[threadIdx.x][threadIdx.y];
    }
}
```

**Speedup**: 6x improvement (112 GB/s â†’ 680 GB/s)

## Memory Pool Optimization

Reuse memory allocations:

```cuda
class MemoryPool {
    void* pool;
    size_t pool_size;
    size_t allocated;
    
public:
    MemoryPool(size_t size) {
        cudaMalloc(&pool, size);
        pool_size = size;
        allocated = 0;
    }
    
    void* allocate(size_t size) {
        if (allocated + size <= pool_size) {
            void* ptr = (char*)pool + allocated;
            allocated += size;
            return ptr;
        }
        return nullptr;
    }
    
    void reset() {
        allocated = 0;
    }
};
```

## Conclusion

Memory bandwidth optimization requires:

1. **Coalescing**: Sequential, aligned access
2. **Shared memory**: Reduce global memory access
3. **Prefetching**: Hide memory latency
4. **Alignment**: 128-byte boundaries
5. **Access patterns**: Optimize for coalescing

Key strategies:
- Ensure coalesced memory access
- Use shared memory for data reuse
- Prefetch data to hide latency
- Align data structures
- Profile bandwidth utilization

Master memory bandwidth optimization to achieve peak GPU performance.
