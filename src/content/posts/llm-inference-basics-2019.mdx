---
title: "LLM Inference Basics: Understanding Latency, Throughput, and Memory Requirements"
author: "stanley-phoong"
description: "Introduction to large language model inference performance, analyzing latency vs throughput trade-offs, memory requirements, and optimization opportunities."
publishDate: 2019-04-22
category: llm-inference
tags: [llm, inference, performance, latency, throughput, memory]
difficulty: intermediate
readingTime: 18
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Large language model inference presents unique performance challenges. Understanding the fundamental trade-offs between latency, throughput, and memory is essential for building efficient inference systems.

## Inference Performance Metrics

Two primary metrics characterize inference performance:

<Benchmark
  title="Inference Performance Metrics"
  columns={["Metric", "Definition", "Use Case", "Optimization Target"]}
  rows={[
    { values: ["Latency", "Time per token", "Interactive applications", "Minimize"], highlight: true },
    { values: ["Throughput", "Tokens per second", "Batch processing", "Maximize"], highlight: true },
    { values: ["Memory", "Model + KV cache", "Resource constraints", "Minimize"], highlight: false },
  ]}
/>

## Autoregressive Generation

LLMs generate text token-by-token:

```python
def generate_autoregressive(model, prompt, max_tokens=100):
    tokens = tokenize(prompt)
    generated = []
    
    for _ in range(max_tokens):
        # Forward pass: [batch, seq_len] -> [batch, seq_len, vocab_size]
        logits = model(tokens)
        
        # Sample next token
        next_token = sample(logits[:, -1, :])
        generated.append(next_token)
        
        # Append to sequence
        tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)
    
    return detokenize(generated)
```

Each iteration requires:
- **Forward pass**: O(n × d) computation
- **Memory**: KV cache grows linearly

## Latency Analysis

Latency components:

```python
def analyze_latency(model, prompt_length, generation_length):
    """
    Break down inference latency
    """
    # Prefill phase: process prompt
    prefill_time = model.forward_time(prompt_length)
    
    # Generation phase: generate tokens
    generation_time_per_token = model.forward_time(1)  # Single token
    total_generation_time = generation_time_per_token * generation_length
    
    total_latency = prefill_time + total_generation_time
    
    print(f"Prefill ({prompt_length} tokens): {prefill_time:.2f} ms")
    print(f"Generation ({generation_length} tokens): {total_generation_time:.2f} ms")
    print(f"Total latency: {total_latency:.2f} ms")
    print(f"Time per token: {total_latency / generation_length:.2f} ms")
    
    return total_latency
```

<PerfChart
  title="Latency Breakdown: Prefill vs Generation"
  type="bar"
  data={{
    labels: ["Prefill", "Token 1", "Token 2", "Token 3", "Token 4+"],
    datasets: [{
      label: "Time (ms)",
      data: [45.2, 12.3, 12.1, 12.0, 11.9],
      backgroundColor: ["#ef4444", "#3b82f6", "#3b82f6", "#3b82f6", "#3b82f6"],
    }]
  }}
/>

## Throughput vs Latency Trade-off

Batch processing improves throughput but increases latency:

```python
def batch_inference(model, prompts, batch_size):
    """
    Process multiple prompts in batch
    """
    batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]
    
    total_tokens = 0
    total_time = 0
    
    for batch in batches:
        start = time.time()
        outputs = model.generate_batch(batch)
        elapsed = time.time() - start
        
        total_tokens += sum(len(out) for out in outputs)
        total_time += elapsed
    
    throughput = total_tokens / total_time
    avg_latency = total_time / len(prompts)
    
    return throughput, avg_latency
```

<Benchmark
  title="Batch Size Impact on Performance"
  columns={["Batch Size", "Throughput (tok/s)", "Latency (ms)", "GPU Utilization"]}
  rows={[
    { values: ["1", "45", "22", "15%"], highlight: false },
    { values: ["4", "128", "31", "42%"], highlight: false },
    { values: ["8", "210", "38", "68%"], highlight: true },
    { values: ["16", "285", "56", "85%"], highlight: true },
    { values: ["32", "320", "100", "92%"], highlight: false },
  ]}
/>

<Callout type="info" title="Latency vs Throughput">
  Larger batches improve GPU utilization and throughput but increase per-request latency. Choose based on application requirements.
</Callout>

## Memory Requirements

Memory usage breakdown:

```python
def calculate_memory_usage(model_size_gb, seq_len, batch_size, d_model=768, num_layers=12):
    """
    Calculate total memory requirements
    """
    # Model weights
    model_memory = model_size_gb
    
    # KV cache per layer: 2 × batch × seq_len × d_model × 2 bytes (FP16)
    kv_cache_per_layer = 2 * batch_size * seq_len * d_model * 2 / 1e9  # GB
    kv_cache_total = kv_cache_per_layer * num_layers
    
    # Activation memory (approximate)
    activation_memory = batch_size * seq_len * d_model * 4 / 1e9  # FP32
    
    total_memory = model_memory + kv_cache_total + activation_memory
    
    print(f"Model weights: {model_memory:.2f} GB")
    print(f"KV cache: {kv_cache_total:.2f} GB")
    print(f"Activations: {activation_memory:.2f} GB")
    print(f"Total: {total_memory:.2f} GB")
    
    return total_memory

# Example: GPT-2 medium, batch_size=8, seq_len=1024
calculate_memory_usage(1.4, 1024, 8)
```

Output:
```
Model weights: 1.40 GB
KV cache: 0.15 GB
Activations: 0.02 GB
Total: 1.57 GB
```

<PerfChart
  title="Memory Usage vs Sequence Length"
  type="line"
  data={{
    labels: ["256", "512", "1024", "2048", "4096"],
    datasets: [
      {
        label: "KV Cache (GB)",
        data: [0.04, 0.08, 0.15, 0.30, 0.60],
        borderColor: "#3b82f6",
      },
      {
        label: "Total Memory (GB)",
        data: [1.44, 1.48, 1.57, 1.72, 2.02],
        borderColor: "#ef4444",
      }
    ]
  }}
/>

## KV Cache Optimization

KV cache grows linearly with sequence length:

```python
class OptimizedInference:
    def __init__(self, model):
        self.model = model
        self.kv_cache = {}  # Store KV cache per request
    
    def generate_with_cache(self, prompt, max_tokens):
        tokens = tokenize(prompt)
        generated = []
        
        # Prefill: compute KV cache
        kv_cache = self.model.prefill(tokens)
        
        for _ in range(max_tokens):
            # Use cached K, V for previous tokens
            logits = self.model.forward_single_token(
                tokens[:, -1:],  # Only last token
                kv_cache
            )
            
            next_token = sample(logits)
            generated.append(next_token)
            
            # Update KV cache (append new K, V)
            kv_cache = self.model.update_kv_cache(kv_cache, next_token)
            tokens = torch.cat([tokens, next_token.unsqueeze(1)], dim=1)
        
        return detokenize(generated)
```

**Memory savings**: O(n²) → O(n) per token during generation

## Quantization Impact

Quantization reduces memory and can improve latency:

<Benchmark
  title="Quantization Impact on Performance"
  columns={["Precision", "Model Size", "Latency", "Memory", "Quality"]}
  rows={[
    { values: ["FP32", "1.4 GB", "22 ms", "2.1 GB", "100%"], highlight: false },
    { values: ["FP16", "0.7 GB", "12 ms", "1.2 GB", "100%"], highlight: true },
    { values: ["INT8", "0.35 GB", "8 ms", "0.8 GB", "99.5%"], highlight: true },
    { values: ["INT4", "0.18 GB", "6 ms", "0.6 GB", "98%"], highlight: false },
  ]}
/>

## Optimization Strategies

### 1. Batch Processing

```python
# Process requests in batches
def batch_requests(requests, batch_size=8):
    batches = []
    for i in range(0, len(requests), batch_size):
        batches.append(requests[i:i+batch_size])
    return batches
```

### 2. KV Cache Management

```python
# Limit KV cache size
def limit_kv_cache(kv_cache, max_length=2048):
    if kv_cache['length'] > max_length:
        # Keep only recent tokens
        kv_cache['K'] = kv_cache['K'][:, -max_length:, :]
        kv_cache['V'] = kv_cache['V'][:, -max_length:, :]
    return kv_cache
```

### 3. Continuous Batching

```python
# Add new requests as others complete
def continuous_batch(active_requests, new_requests):
    # Remove completed requests
    active_requests = [r for r in active_requests if not r.complete]
    
    # Add new requests
    while len(active_requests) < max_batch_size and new_requests:
        active_requests.append(new_requests.pop(0))
    
    return active_requests
```

## Conclusion

LLM inference optimization requires balancing:

1. **Latency**: Critical for interactive applications
2. **Throughput**: Important for batch processing
3. **Memory**: Constrains batch size and sequence length

Key strategies:
- **KV caching**: Reduces computation during generation
- **Batch processing**: Improves GPU utilization
- **Quantization**: Reduces memory and latency
- **Continuous batching**: Maximizes throughput

Choose optimizations based on your application's requirements.
