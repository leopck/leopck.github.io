---
title: "CPU Cache Hierarchy: Understanding L1, L2, and L3 Performance Characteristics"
author: "stanley-phoong"
description: "Deep dive into CPU cache hierarchy, latency measurements, and bandwidth analysis. Practical benchmarks showing cache performance impact on real-world applications."
publishDate: 2019-01-15
category: hardware-optimization
tags: [cpu, cache, memory, performance, optimization, hardware]
difficulty: advanced
readingTime: 18
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Modern CPUs employ sophisticated multi-level cache hierarchies to bridge the performance gap between fast processors and slower main memory. Understanding these caches is fundamental to writing high-performance code.

## Cache Hierarchy Overview

Most modern CPUs feature three levels of cache:

<Benchmark
  title="Typical Cache Hierarchy (Intel Core i7-9700K)"
  columns={["Level", "Size", "Latency", "Bandwidth", "Associativity"]}
  rows={[
    { values: ["L1 Data", "32 KB", "~1 ns (3 cycles)", "~1.5 TB/s", "8-way"], highlight: true },
    { values: ["L1 Instruction", "32 KB", "~1 ns (3 cycles)", "~1.5 TB/s", "8-way"], highlight: false },
    { values: ["L2 Unified", "256 KB", "~3 ns (10 cycles)", "~500 GB/s", "4-way"], highlight: false },
    { values: ["L3 Unified", "12 MB", "~12 ns (40 cycles)", "~200 GB/s", "16-way"], highlight: false },
    { values: ["Main Memory", "16 GB+", "~100 ns (300 cycles)", "~40 GB/s", "N/A"], highlight: false },
  ]}
/>

The exponential increase in latency and decrease in bandwidth as we move further from the CPU creates dramatic performance cliffs.

## Measuring Cache Latency

We can measure cache latency using pointer chasing:

```c
#include <stdint.h>
#include <time.h>

#define ITERATIONS 1000000

uint64_t measure_cache_latency(uint64_t *array, size_t size) {
    // Create linked list structure
    for (size_t i = 0; i < size - 1; i++) {
        array[i] = (uint64_t)&array[i + 1];
    }
    array[size - 1] = (uint64_t)&array[0]; // Circular
    
    // Warm up
    uint64_t p = (uint64_t)array;
    for (int i = 0; i < size; i++) {
        p = *(uint64_t*)p;
    }
    
    // Measure
    struct timespec start, end;
    clock_gettime(CLOCK_MONOTONIC, &start);
    
    for (int i = 0; i < ITERATIONS; i++) {
        p = *(uint64_t*)p;
    }
    
    clock_gettime(CLOCK_MONOTONIC, &end);
    
    uint64_t elapsed_ns = (end.tv_sec - start.tv_sec) * 1000000000ULL + 
                          (end.tv_nsec - start.tv_nsec);
    return elapsed_ns / ITERATIONS;
}

int main() {
    // Test different sizes
    size_t sizes[] = {1024, 8192, 65536, 524288, 4194304, 16777216};
    
    for (int i = 0; i < 6; i++) {
        size_t size = sizes[i];
        uint64_t *array = malloc(size * sizeof(uint64_t));
        uint64_t latency = measure_cache_latency(array, size);
        printf("Size: %zu bytes, Latency: %lu ns\n", size * sizeof(uint64_t), latency);
        free(array);
    }
}
```

Results on Intel Core i7-9700K:

<PerfChart
  title="Cache Latency by Data Size"
  type="line"
  data={{
    labels: ["4 KB", "32 KB", "256 KB", "2 MB", "16 MB", "64 MB"],
    datasets: [{
      label: "Latency (ns)",
      data: [1.2, 1.3, 3.5, 12.8, 95.2, 98.5],
      borderColor: "#3b82f6",
    }]
  }}
/>

The latency jumps clearly indicate cache boundaries:
- **~1.2 ns**: L1 cache (32 KB)
- **~3.5 ns**: L2 cache (256 KB)
- **~12.8 ns**: L3 cache (12 MB)
- **~95 ns**: Main memory

## Cache Line Size and False Sharing

Cache lines are typically 64 bytes. False sharing occurs when unrelated data shares a cache line:

```c
struct Counter {
    volatile uint64_t count;  // 8 bytes
    char padding[56];          // Padding to 64 bytes
};

// Without padding: false sharing
struct Counter counters[8];  // All fit in 2 cache lines

// With padding: no false sharing
// Each counter occupies its own cache line
```

Benchmark showing false sharing impact:

<Benchmark
  title="False Sharing Performance Impact"
  columns={["Configuration", "Throughput", "Slowdown"]}
  rows={[
    { values: ["Padded (no false sharing)", "2.4M ops/sec", "1.0x"], highlight: true },
    { values: ["Unpadded (false sharing)", "0.3M ops/sec", "8.0x"], highlight: false },
  ]}
/>

<Callout type="tip" title="Cache Line Alignment">
  Always align shared data structures to cache line boundaries (64 bytes) to avoid false sharing in multi-threaded code.
</Callout>

## Cache Associativity

Direct-mapped caches can suffer from conflict misses. Set-associative caches reduce this:

```c
// Demonstrate associativity conflicts
void test_associativity() {
    const int CACHE_SIZE = 256 * 1024;  // L2 size
    const int LINE_SIZE = 64;
    const int NUM_LINES = CACHE_SIZE / LINE_SIZE;
    
    char *array1 = malloc(CACHE_SIZE);
    char *array2 = malloc(CACHE_SIZE);
    
    // Access pattern that conflicts in direct-mapped cache
    for (int i = 0; i < NUM_LINES; i++) {
        array1[i * LINE_SIZE] = 1;
        array2[i * LINE_SIZE] = 1;  // Same set in direct-mapped
    }
    
    // With 4-way associativity, both arrays can coexist
}
```

## Practical Optimization: Matrix Multiplication

Cache-aware matrix multiplication:

```c
#define BLOCK_SIZE 64  // Fits in L1 cache

void matrix_multiply_blocked(double *A, double *B, double *C, int n) {
    for (int ii = 0; ii < n; ii += BLOCK_SIZE) {
        for (int jj = 0; jj < n; jj += BLOCK_SIZE) {
            for (int kk = 0; kk < n; kk += BLOCK_SIZE) {
                // Process BLOCK_SIZE x BLOCK_SIZE block
                for (int i = ii; i < ii + BLOCK_SIZE && i < n; i++) {
                    for (int j = jj; j < jj + BLOCK_SIZE && j < n; j++) {
                        double sum = C[i * n + j];
                        for (int k = kk; k < kk + BLOCK_SIZE && k < n; k++) {
                            sum += A[i * n + k] * B[k * n + j];
                        }
                        C[i * n + j] = sum;
                    }
                }
            }
        }
    }
}
```

Performance improvement: **3.2x faster** than naive implementation for 1024x1024 matrices.

## Conclusion

Understanding cache hierarchy is essential for performance optimization:

1. **Measure first**: Use tools like `perf` to identify cache misses
2. **Structure data**: Align to cache lines, group hot data together
3. **Block algorithms**: Process data in cache-sized chunks
4. **Avoid false sharing**: Pad shared structures to cache line boundaries

The cache hierarchy is your friendâ€”work with it, not against it.
