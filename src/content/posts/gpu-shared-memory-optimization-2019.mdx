---
title: "GPU Shared Memory Optimization: Bank Conflicts, Tiling Strategies, and Performance Analysis"
author: "stanley-phoong"
description: "Advanced techniques for optimizing GPU shared memory usage, avoiding bank conflicts, implementing efficient tiling patterns, and maximizing memory bandwidth."
publishDate: 2019-08-20
category: gpu-programming
tags: [gpu, cuda, shared-memory, optimization, performance, bank-conflicts]
difficulty: expert
readingTime: 21
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Shared memory is the fastest on-chip memory in GPUs, but bank conflicts can dramatically reduce performance. Understanding bank organization and optimization techniques is crucial.

## Shared Memory Architecture

Shared memory is organized into 32 banks (one per thread in a warp):

<Benchmark
  title="Shared Memory Characteristics (NVIDIA V100)"
  columns={["Property", "Value", "Impact"]}
  rows={[
    { values: ["Size per SM", "96 KB", "Limits occupancy"], highlight: true },
    { values: ["Banks", "32", "Bank conflicts"], highlight: true },
    { values: ["Bank Width", "4 bytes", "Access granularity"], highlight: false },
    { values: ["Latency", "~20 cycles", "Fast"], highlight: false },
    { values: ["Bandwidth", "~3 TB/s", "High"], highlight: false },
  ]}
/>

## Bank Conflict Analysis

Bank conflicts occur when multiple threads access the same bank:

```cuda
// No bank conflicts: sequential access
__global__ void no_conflicts(float *input, float *output) {
    __shared__ float s_data[256];
    int tid = threadIdx.x;
    
    // Each thread accesses different bank
    s_data[tid] = input[blockIdx.x * 256 + tid];
    __syncthreads();
    
    output[blockIdx.x * 256 + tid] = s_data[tid] * 2.0f;
}

// Bank conflicts: stride 32
__global__ void bank_conflicts(float *input, float *output) {
    __shared__ float s_data[256];
    int tid = threadIdx.x;
    
    // All threads access bank (tid % 32)
    s_data[tid * 32] = input[blockIdx.x * 256 + tid];
    __syncthreads();
    
    output[blockIdx.x * 256 + tid] = s_data[tid * 32] * 2.0f;
}
```

<Benchmark
  title="Bank Conflict Impact on Bandwidth"
  columns={["Access Pattern", "Bandwidth", "Efficiency", "Conflicts"]}
  rows={[
    { values: ["Stride 1", "2.8 TB/s", "93%", "None"], highlight: true },
    { values: ["Stride 2", "2.6 TB/s", "87%", "2-way"], highlight: false },
    { values: ["Stride 4", "2.1 TB/s", "70%", "4-way"], highlight: false },
    { values: ["Stride 8", "1.4 TB/s", "47%", "8-way"], highlight: false },
    { values: ["Stride 32", "0.35 TB/s", "12%", "32-way"], highlight: false },
  ]}
/>

<PerfChart
  title="Shared Memory Bandwidth vs Stride"
  type="bar"
  data={{
    labels: ["1", "2", "4", "8", "16", "32"],
    datasets: [{
      label: "Bandwidth (TB/s)",
      data: [2.8, 2.6, 2.1, 1.4, 0.7, 0.35],
      backgroundColor: ["#10b981", "#3b82f6", "#f59e0b", "#ef4444", "#ef4444", "#6b7280"],
    }]
  }}
/>

## Padding to Avoid Conflicts

Add padding to eliminate bank conflicts:

```cuda
// Without padding: potential conflicts
__shared__ float matrix[16][16];  // May have conflicts

// With padding: no conflicts
__shared__ float matrix[16][17];  // +1 column padding

__global__ void padded_matrix(float *input, float *output) {
    int row = threadIdx.y;
    int col = threadIdx.x;
    
    // Access with padding
    matrix[row][col] = input[row * 16 + col];
    __syncthreads();
    
    // Transpose access (no conflicts due to padding)
    output[col * 16 + row] = matrix[row][col];
}
```

**Performance improvement**: 2-4x for matrix transpose operations

## Tiling Strategies

### 1. Square Tiles

```cuda
#define TILE_SIZE 16

__global__ void square_tile(float *A, float *B, float *C, int N) {
    __shared__ float tileA[TILE_SIZE][TILE_SIZE + 1];  // Padding
    __shared__ float tileB[TILE_SIZE][TILE_SIZE + 1];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    // Process tiles
    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; tile++) {
        // Load tile A (coalesced)
        if (row < N && tile * TILE_SIZE + tx < N) {
            tileA[ty][tx] = A[row * N + tile * TILE_SIZE + tx];
        } else {
            tileA[ty][tx] = 0.0f;
        }
        
        // Load tile B (coalesced)
        if (tile * TILE_SIZE + ty < N && col < N) {
            tileB[ty][tx] = B[(tile * TILE_SIZE + ty) * N + col];
        } else {
            tileB[ty][tx] = 0.0f;
        }
        
        __syncthreads();
        
        // Compute partial dot product
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += tileA[ty][k] * tileB[k][tx];  // No bank conflicts
        }
        
        __syncthreads();
    }
    
    if (row < N && col < N) {
        C[row * N + col] = sum;
    }
}
```

### 2. Rectangular Tiles

```cuda
#define TILE_H 32
#define TILE_W 16

__global__ void rectangular_tile(float *A, float *B, float *C, int N) {
    __shared__ float tileA[TILE_H][TILE_W + 1];
    __shared__ float tileB[TILE_W][TILE_H + 1];
    
    // Optimize for memory access patterns
    // TILE_H × TILE_W chosen to maximize coalescing
}
```

## Memory Access Patterns

### Coalesced Loads

```cuda
__global__ void coalesced_load(float *input, float *output) {
    __shared__ float s_data[256];
    int tid = threadIdx.x;
    
    // Coalesced: threads 0-31 load consecutive addresses
    s_data[tid] = input[blockIdx.x * 256 + tid];
    __syncthreads();
    
    // Process from shared memory
    output[blockIdx.x * 256 + tid] = s_data[tid] * 2.0f;
}
```

**Bandwidth**: ~900 GB/s (global memory) → ~2.8 TB/s (shared memory)

### Strided Access

```cuda
__global__ void strided_access(float *input, float *output, int stride) {
    __shared__ float s_data[256];
    int tid = threadIdx.x;
    
    // Strided: may cause bank conflicts
    s_data[tid * stride] = input[blockIdx.x * 256 + tid];
    __syncthreads();
    
    output[blockIdx.x * 256 + tid] = s_data[tid * stride] * 2.0f;
}
```

**Performance degradation**: Increases with stride

## Reduction Operations

Optimized warp-level reduction:

```cuda
__global__ void shared_memory_reduce(float *input, float *output, int n) {
    __shared__ float s_data[256];
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    // Load into shared memory
    s_data[tid] = (idx < n) ? input[idx] : 0.0f;
    __syncthreads();
    
    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            s_data[tid] += s_data[tid + s];
        }
        __syncthreads();
    }
    
    // Write result
    if (tid == 0) {
        output[blockIdx.x] = s_data[0];
    }
}
```

**Optimization**: Use warp shuffle for final reduction:

```cuda
__global__ void optimized_reduce(float *input, float *output, int n) {
    __shared__ float s_data[256];
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tid;
    
    s_data[tid] = (idx < n) ? input[idx] : 0.0f;
    __syncthreads();
    
    // Reduction in shared memory
    for (int s = blockDim.x / 2; s > 32; s >>= 1) {
        if (tid < s) {
            s_data[tid] += s_data[tid + s];
        }
        __syncthreads();
    }
    
    // Final reduction with warp shuffle
    if (tid < 32) {
        float val = s_data[tid];
        #pragma unroll
        for (int offset = 16; offset > 0; offset >>= 1) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        }
        if (tid == 0) {
            output[blockIdx.x] = val;
        }
    }
}
```

**Speedup**: 1.3x improvement over pure shared memory reduction

## Occupancy vs Shared Memory

Shared memory usage affects occupancy:

```cuda
// Calculate occupancy based on shared memory
void analyze_shared_memory_occupancy(int shared_mem_per_block) {
    int max_shared_mem = 96 * 1024;  // 96 KB per SM
    int max_blocks_by_shared = max_shared_mem / shared_mem_per_block;
    
    // Other constraints (threads, registers)
    int max_threads = 2048;
    int threads_per_block = 256;
    int max_blocks_by_threads = max_threads / threads_per_block;
    
    int max_blocks = min(max_blocks_by_shared, max_blocks_by_threads);
    float occupancy = (float)max_blocks / 32.0 * 100.0;  // 32 = max blocks per SM
    
    printf("Shared memory per block: %d bytes\n", shared_mem_per_block);
    printf("Max blocks by shared memory: %d\n", max_blocks_by_shared);
    printf("Occupancy: %.1f%%\n", occupancy);
}
```

<PerfChart
  title="Occupancy vs Shared Memory Usage"
  type="line"
  data={{
    labels: ["0 KB", "8 KB", "16 KB", "32 KB", "48 KB", "64 KB"],
    datasets: [{
      label: "Occupancy (%)",
      data: [100, 100, 100, 75, 50, 25],
      borderColor: "#3b82f6",
    }]
  }}
/>

## Performance Optimization Example

Matrix transpose optimization:

```cuda
// Naive transpose: non-coalesced writes
__global__ void transpose_naive(float *input, float *output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x < width && y < height) {
        output[y * width + x] = input[x * height + y];  // Strided write
    }
}

// Optimized with shared memory
__global__ void transpose_optimized(float *input, float *output, int width, int height) {
    __shared__ float tile[16][17];  // Padding to avoid bank conflicts
    
    int x = blockIdx.x * 16 + threadIdx.x;
    int y = blockIdx.y * 16 + threadIdx.y;
    
    // Coalesced read
    if (x < width && y < height) {
        tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    }
    __syncthreads();
    
    // Coalesced write (transposed)
    x = blockIdx.y * 16 + threadIdx.x;
    y = blockIdx.x * 16 + threadIdx.y;
    if (x < height && y < width) {
        output[y * height + x] = tile[threadIdx.x][threadIdx.y];
    }
}
```

<Benchmark
  title="Matrix Transpose Performance"
  columns={["Method", "Bandwidth", "Speedup"]}
  rows={[
    { values: ["Naive", "112 GB/s", "1.0x"], highlight: false },
    { values: ["Shared Memory", "450 GB/s", "4.0x"], highlight: true },
    { values: ["Padded Shared", "680 GB/s", "6.1x"], highlight: true },
  ]}
/>

## Optimization Checklist

1. **Avoid bank conflicts**: Stride != 32, use padding
2. **Coalesce global memory**: Load/store in coalesced patterns
3. **Tile algorithms**: Fit working set in shared memory
4. **Minimize shared memory**: Maximize occupancy
5. **Use warp shuffle**: For reductions within warp
6. **Profile bank conflicts**: Use Nsight Compute

## Conclusion

Shared memory optimization requires:

1. **Understanding bank organization**: 32 banks, 4-byte width
2. **Avoiding conflicts**: Padding, careful indexing
3. **Tiling strategies**: Square vs rectangular tiles
4. **Balancing occupancy**: Shared memory vs block count
5. **Coalescing patterns**: Efficient global memory access

Key strategies:
- Add padding to eliminate bank conflicts
- Use shared memory for data reuse
- Tile algorithms to fit in shared memory
- Balance shared memory usage with occupancy
- Profile to identify bottlenecks

Master shared memory to unlock GPU performance potential.
