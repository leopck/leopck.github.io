---
title: "Roofline for GPU Kernels: Turning Profiler Counters into Optimization Decisions"
author: "stanley-phoong"
description: "A practical roofline workflow: compute arithmetic intensity, measure achieved bandwidth/FLOPs, classify kernels as memory- or compute-bound, then pick optimizations that actually move the dot."
publishDate: 2020-06-03
category: profiling
tags: [gpu, roofline, profiling, optimization, performance, memory-bandwidth, compute]
difficulty: expert
readingTime: 20
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Most “GPU optimization” advice is generic. Roofline is the antidote: it tells you **what kind of optimization can help** before you touch code.

This post gives a roofline workflow you can run on any CUDA kernel using profiler counters.

## Roofline basics (minimal math, maximum utility)

Define arithmetic intensity (AI):
\[
AI = \frac{\text{FLOPs}}{\text{Bytes moved from DRAM}}
\]

Hardware provides:
- peak compute \( \pi \) (FLOP/s)
- peak bandwidth \( \beta \) (Bytes/s)

Performance upper bound:
\[
P \le \min(\pi,\ AI \cdot \beta)
\]

Ridge point:
\[
AI_{ridge} = \frac{\pi}{\beta}
\]

If your kernel’s AI is below \(AI_{ridge}\): **memory-bound**.  
Above: **compute-bound**.

## Step 1: compute AI from counters (not guesses)

You need:
- achieved FLOPs (or instructions)
- DRAM bytes read/written

Profiler (Nsight Compute) can provide:
- `dram__bytes_read.sum`
- `dram__bytes_write.sum`
- `smsp__sass_thread_inst_executed_op_fadd_pred_on.sum` (etc) or high-level FLOP metrics

AI:

```text
AI = FLOPs / (dram_read_bytes + dram_write_bytes)
```

## Step 2: compute achieved performance

If you have kernel time \(t\):
\[
P_{achieved} = \frac{FLOPs}{t}
\quad,\quad
BW_{achieved} = \frac{Bytes}{t}
\]

## Step 3: classify

<Benchmark
  title="Roofline classification"
  columns={["Symptom", "AI", "Likely bound", "Optimization direction"]}
  rows={[
    { values: ["High DRAM BW %, low SM %", "Low", "Memory-bound", "Reduce bytes: reuse, coalesce, compression"], highlight: true },
    { values: ["High SM %, low DRAM BW %", "High", "Compute-bound", "Increase math efficiency: vectorize, tensor cores, fuse"], highlight: true },
    { values: ["Both low", "Any", "Under-occupied / stalled", "Fix occupancy, divergence, launch config"], highlight: false },
  ]}
/>

## A concrete example (illustrative numbers)

Assume V100-like GPU:
- peak compute \(\pi = 125\) TFLOP/s (FP16 tensor core path)
- peak BW \(\beta = 900\) GB/s

Ridge:
\[
AI_{ridge} = 125e12 / 900e9 \approx 139\ \text{FLOP/Byte}
\]

If your kernel has AI=5 FLOP/Byte, it is strongly memory-bound.

<PerfChart
  title="Conceptual roofline regions"
  type="line"
  data={{
    labels: ["1", "5", "20", "80", "160"],
    datasets: [
      { label: "Memory سقف (AI·β)", data: [0.9, 4.5, 18, 72, 144], borderColor: "#3b82f6" },
      { label: "Compute سقف (π)", data: [125, 125, 125, 125, 125], borderColor: "#ef4444" }
    ]
  }}
/>

## Step 4: pick optimizations that move the dot

### If memory-bound (low AI)

Goal: reduce DRAM bytes per output.

High-yield tactics:
- **Coalesce** global loads/stores
- Use **shared memory tiling** (reuse)
- Reduce precision (FP16/INT8) if acceptable
- Fuse kernels to avoid round-trips to DRAM
- Avoid redundant reads (hoist invariants)

### If compute-bound (high AI)

Goal: increase effective FLOP/s.

High-yield tactics:
- Increase occupancy only if you’re latency-limited
- Use vectorized math / tensor cores
- Fuse pointwise ops into matmul epilogue
- Reduce divergence and instruction overhead

### If under-occupied / stalled

Goal: keep more warps runnable and reduce stalls.

Tactics:
- Reduce register pressure (avoid spills)
- Improve memory locality (L2 hit rate)
- Choose launch config that avoids tiny blocks

## A “move the dot” checklist

<Benchmark
  title="Optimization → roofline effect"
  columns={["Optimization", "What it changes", "Roofline movement"]}
  rows={[
    { values: ["Tiling in shared memory", "Bytes ↓", "AI ↑ (right)"], highlight: true },
    { values: ["Kernel fusion", "Bytes ↓", "AI ↑ (right)"], highlight: true },
    { values: ["Tensor cores / MMA", "FLOPs/s ↑", "Ceiling ↑ (up)"], highlight: false },
    { values: ["Quantization", "Bytes ↓, FLOPs/s ↑ (sometimes)", "Right + up"], highlight: true },
  ]}
/>

## Practical workflow

1. Profile kernel, capture bytes + FLOPs + time  
2. Compute AI, achieved BW, achieved FLOP/s  
3. Compare to ridge point → classify bound  
4. Apply 1–2 targeted optimizations  
5. Re-profile and ensure the dot moved in the intended direction

<Callout type="tip" title="Don’t optimize blind">
  If your kernel is memory-bound, shaving instruction count rarely helps. If it’s compute-bound, micro-optimizing loads rarely helps. Roofline prevents wasted effort.
</Callout>

## Conclusion

Roofline turns “optimization” into a decision procedure:
- measure → compute AI/BW/FLOP → classify → choose optimizations → verify movement

Once you’re fluent with it, you stop arguing about what to try and start shipping the changes that matter.

