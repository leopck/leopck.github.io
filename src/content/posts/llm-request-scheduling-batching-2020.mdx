---
title: "LLM Request Scheduling: Batching, Fairness, and p99 Latency in Shared Clusters"
author: "stanley-phoong"
description: "A systems-engineering view of LLM request scheduling: how batch size, queueing, and fairness policies interact with KV cache and GPU utilization to determine both throughput and tail latency."
publishDate: 2020-10-09
category: llm-inference
tags: [llm, scheduling, batching, queueing, latency, throughput, optimization]
difficulty: expert
readingTime: 21
---

import Callout from '@/components/mdx/Callout.astro';
import PerfChart from '@/components/mdx/PerfChart.astro';
import Benchmark from '@/components/mdx/Benchmark.astro';

Given a fixed model and GPU, **request scheduling** is often the biggest lever you have over:
- throughput (tokens/s)
- p99 latency
- perceived fairness between tenants

This post looks at the scheduling problem as a queueing system with **batching** and **continuous decoding** constraints.

## Basic model: arrivals, service, and batching

Simplify:
- requests arrive with some rate \(\lambda\)
- each token decode step costs \(T\) on average if run alone
- batching B requests into one step costs \(T_B \le B \cdot T\)

Throughput (tokens/s) with batch size B:
\[
R_B \approx \frac{B}{T_B}
\]

Latency has two parts:
- **queueing delay** waiting to be batched
- **service time** once in a batch

<Benchmark
  title="Batching trade-offs"
  columns={["Batch size", "Throughput", "Queueing delay", "Tail latency risk"]}
  rows={[
    { values: ["1", "Low", "None", "Low"], highlight: false },
    { values: ["4", "Medium", "Small", "Moderate"], highlight: true },
    { values: ["16", "High", "Can be large", "High if arrivals bursty"], highlight: false },
  ]}
/>

## Simple schedulers

### FIFO with fixed batch size

```python
class FifoBatchScheduler:
    def __init__(self, max_batch_size):
        self.queue = []
        self.max_batch = max_batch_size

    def enqueue(self, req):
        self.queue.append(req)

    def form_batch(self):
        if not self.queue:
            return []
        # take up to max_batch requests
        batch = self.queue[: self.max_batch]
        self.queue = self.queue[self.max_batch :]
        return batch
```

Pros:
- simple
- high utilization at high load

Cons:
- at low load, you delay small batches waiting to fill

### Timeout-based batching

```python
class TimeoutBatchScheduler:
    def __init__(self, max_batch_size, max_wait_ms):
        self.queue = []
        self.max_batch = max_batch_size
        self.max_wait = max_wait_ms / 1000.0

    def enqueue(self, req):
        req.arrival = time.time()
        self.queue.append(req)

    def form_batch(self):
        if not self.queue:
            return []
        now = time.time()
        # If oldest request waited long enough, form whatever batch we have
        if now - self.queue[0].arrival >= self.max_wait or len(self.queue) >= self.max_batch:
            batch = self.queue[: self.max_batch]
            self.queue = self.queue[self.max_batch :]
            return batch
        return []
```

This caps queueing delay and stabilizes p99.

## Continuous decoding and iteration-level batches

Unlike prefill, decode steps repeat for each token. With continuous batching, at each **decode iteration** you:
- drop finished requests
- add new arrivals
- form a batch across all active requests

<Callout type="tip" title="Think in iterations, not requests">
  The GPU runs per-iteration batches of active sequences. Scheduling is about which sequences make it into each iteration and in what groupings.
</Callout>

## Fairness vs throughput

Greedy batching (always fill largest possible batch) can starve small, latency-sensitive jobs behind a stream of long, high-throughput streams.

Simple mitigation:
- **age-based priority**: weight requests by waiting time
- **tenant-aware limits**: cap concurrent tokens per tenant

<Benchmark
  title="Fairness policy examples"
  columns={["Policy", "Pros", "Cons"]}
  rows={[
    { values: ["Pure FIFO", "Simple, fair by arrival", "May waste batching opportunities"], highlight: false },
    { values: ["Greedy size-based", "High throughput", "Can hurt small requests"], highlight: true },
    { values: ["Age-weighted", "Balances both", "Slightly more complex"], highlight: true },
  ]}
/>

## Metrics you should track

- **tokens/sec** per GPU (throughput)
- **queueing delay** distribution
- **time-in-system** distribution (end-to-end latency)
- **utilization** of GPU (SM active %, memory BW)

<PerfChart
  title="Example: latency vs batch size"
  type="line"
  data={{
    labels: ["1", "2", "4", "8", "16"],
    datasets: [
      { label: "Median latency (ms)", data: [80, 85, 95, 120, 180], borderColor: "#3b82f6" },
      { label: "p99 latency (ms)", data: [120, 135, 170, 250, 420], borderColor: "#ef4444" },
    ]
  }}
/>

## Practical guidance

- In low-traffic environments:
  - prioritize latency over throughput
  - small batches, tight timeouts
- In high-traffic environments:
  - prioritize throughput, but cap max wait
  - use continuous batching with age-based fairness
- For multi-tenant setups:
  - enforce per-tenant token budgets
  - monitor per-tenant p99 separately

## Conclusion

Scheduling is where your **performance SLOs** meet your **quality SLOs**:
- batching boosts throughput but hurts tail latency if unmanaged
- age- and tenant-aware policies prevent starvation
- continuous batching makes the most of active sequences

You don’t control arrivals, but you do control how you group and order work on the GPU — treat that as a first-class optimization problem.

